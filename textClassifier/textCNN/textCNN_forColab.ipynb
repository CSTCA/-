{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "textCNN.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7D4inSYSKnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!git clone https://github.com/ljyslyc/comment-analysis.git\n",
        "from pathlib import Path\n",
        "mypath = Path(\"/content/comment-analysis/textClassifier\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxjMae2xR2oK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import json\n",
        "\n",
        "from collections import Counter\n",
        "from math import sqrt\n",
        "\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_Y3koleR2oP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 配置参数\n",
        "\n",
        "class TrainingConfig(object):\n",
        "    epoches = 5\n",
        "    evaluateEvery = 100\n",
        "    checkpointEvery = 100\n",
        "    learningRate = 0.001\n",
        "    \n",
        "class ModelConfig(object):\n",
        "    embeddingSize = 200\n",
        "    numFilters = 128\n",
        "\n",
        "    filterSizes = [2, 3, 4, 5]\n",
        "    dropoutKeepProb = 0.5\n",
        "    l2RegLambda = 0.0\n",
        "    \n",
        "class Config(object):\n",
        "    sequenceLength = 200  # 取了所有序列长度的均值\n",
        "    batchSize = 128\n",
        "    \n",
        "    dataSource = mypath/\"data/preProcess/labeledTrain.csv\"\n",
        "    \n",
        "    stopWordSource = mypath/\"data/english\"\n",
        "    \n",
        "    numClasses = 1  # 二分类设置为1，多分类设置为类别的数目\n",
        "    \n",
        "    rate = 0.8  # 训练集的比例\n",
        "    \n",
        "    training = TrainingConfig()\n",
        "    \n",
        "    model = ModelConfig()\n",
        "\n",
        "    \n",
        "# 实例化配置参数对象\n",
        "config = Config()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT1w2c8FR2oQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "dbcd0af2-80f5-4a0c-b079-0acc638c612e"
      },
      "source": [
        "# 数据预处理的类，生成训练集和测试集\n",
        "\n",
        "class Dataset(object):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self._dataSource = config.dataSource\n",
        "        self._stopWordSource = config.stopWordSource  \n",
        "        \n",
        "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
        "        self._embeddingSize = config.model.embeddingSize\n",
        "        self._batchSize = config.batchSize\n",
        "        self._rate = config.rate\n",
        "        \n",
        "        self._stopWordDict = {}\n",
        "        \n",
        "        self.trainReviews = []\n",
        "        self.trainLabels = []\n",
        "        \n",
        "        self.evalReviews = []\n",
        "        self.evalLabels = []\n",
        "        \n",
        "        self.wordEmbedding =None\n",
        "        \n",
        "        self.labelList = []\n",
        "        \n",
        "    def _readData(self, filePath):\n",
        "        \"\"\"\n",
        "        从csv文件中读取数据集\n",
        "        \"\"\"\n",
        "        \n",
        "        df = pd.read_csv(filePath)\n",
        "        \n",
        "        if self.config.numClasses == 1:\n",
        "            labels = df[\"sentiment\"].tolist()\n",
        "        elif self.config.numClasses > 1:\n",
        "            labels = df[\"rate\"].tolist()\n",
        "            \n",
        "        review = df[\"review\"].tolist()\n",
        "        reviews = [line.strip().split() for line in review]\n",
        "\n",
        "        return reviews, labels\n",
        "    \n",
        "    def _labelToIndex(self, labels, label2idx):\n",
        "        \"\"\"\n",
        "        将标签转换成索引表示\n",
        "        \"\"\"\n",
        "        labelIds = [label2idx[label] for label in labels]\n",
        "        return labelIds\n",
        "    \n",
        "    def _wordToIndex(self, reviews, word2idx):\n",
        "        \"\"\"\n",
        "        将词转换成索引\n",
        "        \"\"\"\n",
        "        reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
        "        return reviewIds\n",
        "        \n",
        "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
        "        \"\"\"\n",
        "        生成训练集和验证集\n",
        "        \"\"\"\n",
        "        reviews = []\n",
        "        for review in x:\n",
        "            if len(review) >= self._sequenceLength:\n",
        "                reviews.append(review[:self._sequenceLength])\n",
        "            else:\n",
        "                reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
        "            \n",
        "        trainIndex = int(len(x) * rate)\n",
        "        \n",
        "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
        "        trainLabels = np.array(y[:trainIndex], dtype=\"float32\")\n",
        "        \n",
        "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
        "        evalLabels = np.array(y[trainIndex:], dtype=\"float32\")\n",
        "\n",
        "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
        "        \n",
        "    def _genVocabulary(self, reviews, labels):\n",
        "        \"\"\"\n",
        "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
        "        \"\"\"\n",
        "        \n",
        "        allWords = [word for review in reviews for word in review]\n",
        "        \n",
        "        # 去掉停用词\n",
        "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
        "        \n",
        "        wordCount = Counter(subWords)  # 统计词频\n",
        "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        # 去除低频词\n",
        "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
        "        \n",
        "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
        "        self.wordEmbedding = wordEmbedding\n",
        "        \n",
        "        word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
        "        \n",
        "        uniqueLabel = list(set(labels))\n",
        "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
        "        self.labelList = list(range(len(uniqueLabel)))\n",
        "        \n",
        "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
        "        with open(mypath/\"data/wordJson/word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(word2idx, f)\n",
        "        \n",
        "        with open(mypath/\"data/wordJson/label2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(label2idx, f)\n",
        "        \n",
        "        return word2idx, label2idx\n",
        "            \n",
        "    def _getWordEmbedding(self, words):\n",
        "        \"\"\"\n",
        "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
        "        \"\"\"\n",
        "        \n",
        "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(mypath/\"word2vec/word2Vec.bin\", binary=True)\n",
        "        vocab = []\n",
        "        wordEmbedding = []\n",
        "        \n",
        "        # 添加 \"pad\" 和 \"UNK\", \n",
        "        vocab.append(\"PAD\")\n",
        "        vocab.append(\"UNK\")\n",
        "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
        "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
        "        \n",
        "        for word in words:\n",
        "            try:\n",
        "                vector = wordVec.wv[word]\n",
        "                vocab.append(word)\n",
        "                wordEmbedding.append(vector)\n",
        "            except:\n",
        "                print(word + \"不存在于词向量中\")\n",
        "                \n",
        "        return vocab, np.array(wordEmbedding)\n",
        "    \n",
        "    def _readStopWord(self, stopWordPath):\n",
        "        \"\"\"\n",
        "        读取停用词\n",
        "        \"\"\"\n",
        "        \n",
        "        with open(stopWordPath, \"r\") as f:\n",
        "            stopWords = f.read()\n",
        "            stopWordList = stopWords.splitlines()\n",
        "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
        "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
        "            \n",
        "    def dataGen(self):\n",
        "        \"\"\"\n",
        "        初始化训练集和验证集\n",
        "        \"\"\"\n",
        "        \n",
        "        # 初始化停用词\n",
        "        self._readStopWord(self._stopWordSource)\n",
        "        \n",
        "        # 初始化数据集\n",
        "        reviews, labels = self._readData(self._dataSource)\n",
        "        \n",
        "        # 初始化词汇-索引映射表和词向量矩阵\n",
        "        word2idx, label2idx = self._genVocabulary(reviews, labels)\n",
        "        \n",
        "        # 将标签和句子数值化\n",
        "        labelIds = self._labelToIndex(labels, label2idx)\n",
        "        reviewIds = self._wordToIndex(reviews, word2idx)\n",
        "        \n",
        "        # 初始化训练集和测试集\n",
        "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviewIds, labelIds, word2idx, self._rate)\n",
        "        self.trainReviews = trainReviews\n",
        "        self.trainLabels = trainLabels\n",
        "        \n",
        "        self.evalReviews = evalReviews\n",
        "        self.evalLabels = evalLabels\n",
        "        \n",
        "        \n",
        "data = Dataset(config)\n",
        "data.dataGen()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuzKTPpUR2oT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1f9da13e-4bfb-4169-f567-7eacfa8b839f"
      },
      "source": [
        "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
        "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
        "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data shape: (20000, 200)\n",
            "train label shape: (20000,)\n",
            "eval data shape: (5000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt7PfFQxR2oW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 输出batch数据集\n",
        "\n",
        "def nextBatch(x, y, batchSize):\n",
        "        \"\"\"\n",
        "        生成batch数据集，用生成器的方式输出\n",
        "        \"\"\"\n",
        "    \n",
        "        perm = np.arange(len(x))\n",
        "        np.random.shuffle(perm)\n",
        "        x = x[perm]\n",
        "        y = y[perm]\n",
        "        \n",
        "        numBatches = len(x) // batchSize\n",
        "\n",
        "        for i in range(numBatches):\n",
        "            start = i * batchSize\n",
        "            end = start + batchSize\n",
        "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
        "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
        "            \n",
        "            yield batchX, batchY"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-KQIVJqR2oY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 构建模型\n",
        "class TextCNN(object):\n",
        "    \"\"\"\n",
        "    Text CNN 用于文本分类\n",
        "    \"\"\"\n",
        "    def __init__(self, config, wordEmbedding):\n",
        "\n",
        "        # 定义模型的输入\n",
        "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
        "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
        "        \n",
        "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
        "        \n",
        "        # 定义l2损失\n",
        "        l2Loss = tf.constant(0.0)\n",
        "        \n",
        "        # 词嵌入层\n",
        "        with tf.name_scope(\"embedding\"):\n",
        "\n",
        "            # 利用预训练的词向量初始化词嵌入矩阵\n",
        "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
        "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
        "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
        "            # 卷积的输入是思维[batch_size, width, height, channel]，因此需要增加维度，用tf.expand_dims来增大维度\n",
        "            self.embeddedWordsExpanded = tf.expand_dims(self.embeddedWords, -1)\n",
        "\n",
        "        # 创建卷积和池化层\n",
        "        pooledOutputs = []\n",
        "        # 有三种size的filter，3， 4， 5，textCNN是个多通道单层卷积的模型，可以看作三个单层的卷积模型的融合\n",
        "        for i, filterSize in enumerate(config.model.filterSizes):\n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filterSize):\n",
        "                # 卷积层，卷积核尺寸为filterSize * embeddingSize，卷积核的个数为numFilters\n",
        "                # 初始化权重矩阵和偏置\n",
        "                filterShape = [filterSize, config.model.embeddingSize, 1, config.model.numFilters]\n",
        "                W = tf.Variable(tf.truncated_normal(filterShape, stddev=0.1), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[config.model.numFilters]), name=\"b\")\n",
        "                conv = tf.nn.conv2d(\n",
        "                    self.embeddedWordsExpanded,\n",
        "                    W,\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv\")\n",
        "                \n",
        "                # relu函数的非线性映射\n",
        "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
        "                \n",
        "                # 池化层，最大池化，池化是对卷积后的序列取一个最大值\n",
        "                pooled = tf.nn.max_pool(\n",
        "                    h,\n",
        "                    ksize=[1, config.sequenceLength - filterSize + 1, 1, 1],  # ksize shape: [batch, height, width, channels]\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding='VALID',\n",
        "                    name=\"pool\")\n",
        "                pooledOutputs.append(pooled)  # 将三种size的filter的输出一起加入到列表中\n",
        "\n",
        "        # 得到CNN网络的输出长度\n",
        "        numFiltersTotal = config.model.numFilters * len(config.model.filterSizes)\n",
        "        \n",
        "        # 池化后的维度不变，按照最后的维度channel来concat\n",
        "        self.hPool = tf.concat(pooledOutputs, 3)\n",
        "        \n",
        "        # 摊平成二维的数据输入到全连接层\n",
        "        self.hPoolFlat = tf.reshape(self.hPool, [-1, numFiltersTotal])\n",
        "\n",
        "        # dropout\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "            self.hDrop = tf.nn.dropout(self.hPoolFlat, self.dropoutKeepProb)\n",
        "       \n",
        "        # 全连接层的输出\n",
        "        with tf.name_scope(\"output\"):\n",
        "            outputW = tf.get_variable(\n",
        "                \"outputW\",\n",
        "                shape=[numFiltersTotal, config.numClasses],\n",
        "                initializer=tf.contrib.layers.xavier_initializer())\n",
        "            outputB= tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
        "            l2Loss += tf.nn.l2_loss(outputW)\n",
        "            l2Loss += tf.nn.l2_loss(outputB)\n",
        "            self.logits = tf.nn.xw_plus_b(self.hDrop, outputW, outputB, name=\"logits\")\n",
        "            if config.numClasses == 1:\n",
        "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.int32, name=\"predictions\")\n",
        "            elif config.numClasses > 1:\n",
        "                self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
        "            \n",
        "            print(self.predictions)\n",
        "        \n",
        "        # 计算二元交叉熵损失\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            \n",
        "            if config.numClasses == 1:\n",
        "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
        "                                                                                                    dtype=tf.float32))\n",
        "            elif config.numClasses > 1:\n",
        "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY)\n",
        "                \n",
        "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVMdkgEOR2oa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "定义各类性能指标\n",
        "\"\"\"\n",
        "\n",
        "def mean(item: list) -> float:\n",
        "    \"\"\"\n",
        "    计算列表中元素的平均值\n",
        "    :param item: 列表对象\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    res = sum(item) / len(item) if len(item) > 0 else 0\n",
        "    return res\n",
        "\n",
        "\n",
        "def accuracy(pred_y, true_y):\n",
        "    \"\"\"\n",
        "    计算二类和多类的准确率\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if isinstance(pred_y[0], list):\n",
        "        pred_y = [item[0] for item in pred_y]\n",
        "    corr = 0\n",
        "    for i in range(len(pred_y)):\n",
        "        if pred_y[i] == true_y[i]:\n",
        "            corr += 1\n",
        "    acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
        "    return acc\n",
        "\n",
        "\n",
        "def binary_precision(pred_y, true_y, positive=1):\n",
        "    \"\"\"\n",
        "    二类的精确率计算\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param positive: 正例的索引表示\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    corr = 0\n",
        "    pred_corr = 0\n",
        "    for i in range(len(pred_y)):\n",
        "        if pred_y[i] == positive:\n",
        "            pred_corr += 1\n",
        "            if pred_y[i] == true_y[i]:\n",
        "                corr += 1\n",
        "\n",
        "    prec = corr / pred_corr if pred_corr > 0 else 0\n",
        "    return prec\n",
        "\n",
        "\n",
        "def binary_recall(pred_y, true_y, positive=1):\n",
        "    \"\"\"\n",
        "    二类的召回率\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param positive: 正例的索引表示\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    corr = 0\n",
        "    true_corr = 0\n",
        "    for i in range(len(pred_y)):\n",
        "        if true_y[i] == positive:\n",
        "            true_corr += 1\n",
        "            if pred_y[i] == true_y[i]:\n",
        "                corr += 1\n",
        "\n",
        "    rec = corr / true_corr if true_corr > 0 else 0\n",
        "    return rec\n",
        "\n",
        "\n",
        "def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
        "    \"\"\"\n",
        "    二类的f beta值\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param beta: beta值\n",
        "    :param positive: 正例的索引表示\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    precision = binary_precision(pred_y, true_y, positive)\n",
        "    recall = binary_recall(pred_y, true_y, positive)\n",
        "    try:\n",
        "        f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
        "    except:\n",
        "        f_b = 0\n",
        "    return f_b\n",
        "\n",
        "\n",
        "def multi_precision(pred_y, true_y, labels):\n",
        "    \"\"\"\n",
        "    多类的精确率\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param labels: 标签列表\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if isinstance(pred_y[0], list):\n",
        "        pred_y = [item[0] for item in pred_y]\n",
        "\n",
        "    precisions = [binary_precision(pred_y, true_y, label) for label in labels]\n",
        "    prec = mean(precisions)\n",
        "    return prec\n",
        "\n",
        "\n",
        "def multi_recall(pred_y, true_y, labels):\n",
        "    \"\"\"\n",
        "    多类的召回率\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param labels: 标签列表\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if isinstance(pred_y[0], list):\n",
        "        pred_y = [item[0] for item in pred_y]\n",
        "\n",
        "    recalls = [binary_recall(pred_y, true_y, label) for label in labels]\n",
        "    rec = mean(recalls)\n",
        "    return rec\n",
        "\n",
        "\n",
        "def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
        "    \"\"\"\n",
        "    多类的f beta值\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param labels: 标签列表\n",
        "    :param beta: beta值\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if isinstance(pred_y[0], list):\n",
        "        pred_y = [item[0] for item in pred_y]\n",
        "\n",
        "    f_betas = [binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
        "    f_beta = mean(f_betas)\n",
        "    return f_beta\n",
        "\n",
        "\n",
        "def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
        "    \"\"\"\n",
        "    得到二分类的性能指标\n",
        "    :param pred_y:\n",
        "    :param true_y:\n",
        "    :param f_beta:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    acc = accuracy(pred_y, true_y)\n",
        "    recall = binary_recall(pred_y, true_y)\n",
        "    precision = binary_precision(pred_y, true_y)\n",
        "    f_beta = binary_f_beta(pred_y, true_y, f_beta)\n",
        "    return acc, recall, precision, f_beta\n",
        "\n",
        "\n",
        "def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
        "    \"\"\"\n",
        "    得到多分类的性能指标\n",
        "    :param pred_y:\n",
        "    :param true_y:\n",
        "    :param labels:\n",
        "    :param f_beta:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    acc = accuracy(pred_y, true_y)\n",
        "    recall = multi_recall(pred_y, true_y, labels)\n",
        "    precision = multi_precision(pred_y, true_y, labels)\n",
        "    f_beta = multi_f_beta(pred_y, true_y, labels, f_beta)\n",
        "    return acc, recall, precision, f_beta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_viziDyhR2ob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "edb142d5-a92d-4fd6-f4fc-6d8c77134dd8"
      },
      "source": [
        "# 训练模型\n",
        "\n",
        "# 生成训练集和验证集\n",
        "trainReviews = data.trainReviews\n",
        "trainLabels = data.trainLabels\n",
        "evalReviews = data.evalReviews\n",
        "evalLabels = data.evalLabels\n",
        "\n",
        "wordEmbedding = data.wordEmbedding\n",
        "labelList = data.labelList\n",
        "\n",
        "# 定义计算图\n",
        "with tf.Graph().as_default():\n",
        "\n",
        "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
        "    session_conf.gpu_options.allow_growth=True\n",
        "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
        "\n",
        "    sess = tf.Session(config=session_conf)\n",
        "    \n",
        "    # 定义会话\n",
        "    with sess.as_default():\n",
        "        cnn = TextCNN(config, wordEmbedding)\n",
        "        \n",
        "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
        "        # 定义优化函数，传入学习速率参数\n",
        "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
        "        # 计算梯度,得到梯度和变量\n",
        "        gradsAndVars = optimizer.compute_gradients(cnn.loss)\n",
        "        # 将梯度应用到变量下，生成训练器\n",
        "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
        "        \n",
        "        # 用summary绘制tensorBoard\n",
        "        gradSummaries = []\n",
        "        for g, v in gradsAndVars:\n",
        "            if g is not None:\n",
        "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
        "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
        "        \n",
        "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
        "        print(\"Writing to {}\\n\".format(outDir))\n",
        "        \n",
        "        lossSummary = tf.summary.scalar(\"loss\", cnn.loss)\n",
        "        summaryOp = tf.summary.merge_all()\n",
        "        \n",
        "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
        "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
        "        \n",
        "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
        "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
        "        \n",
        "        \n",
        "        # 初始化所有变量\n",
        "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
        "        \n",
        "        # 保存模型的一种方式，保存为pb文件\n",
        "        savedModelPath = \"../model/textCNN/savedModel\"\n",
        "        if os.path.exists(savedModelPath):\n",
        "            os.rmdir(savedModelPath)\n",
        "        builder = tf.saved_model.builder.SavedModelBuilder(savedModelPath)\n",
        "            \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        def trainStep(batchX, batchY):\n",
        "            \"\"\"\n",
        "            训练函数\n",
        "            \"\"\"   \n",
        "            feed_dict = {\n",
        "              cnn.inputX: batchX,\n",
        "              cnn.inputY: batchY,\n",
        "              cnn.dropoutKeepProb: config.model.dropoutKeepProb\n",
        "            }\n",
        "            _, summary, step, loss, predictions = sess.run(\n",
        "                [trainOp, summaryOp, globalStep, cnn.loss, cnn.predictions],\n",
        "                feed_dict)\n",
        "            timeStr = datetime.datetime.now().isoformat()\n",
        "            \n",
        "            if config.numClasses == 1:\n",
        "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
        "\n",
        "                \n",
        "            elif config.numClasses > 1:\n",
        "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY,\n",
        "                                                              labels=labelList)\n",
        "                \n",
        "            trainSummaryWriter.add_summary(summary, step)\n",
        "            \n",
        "            return loss, acc, prec, recall, f_beta\n",
        "\n",
        "        def devStep(batchX, batchY):\n",
        "            \"\"\"\n",
        "            验证函数\n",
        "            \"\"\"\n",
        "            feed_dict = {\n",
        "              cnn.inputX: batchX,\n",
        "              cnn.inputY: batchY,\n",
        "              cnn.dropoutKeepProb: 1.0\n",
        "            }\n",
        "            summary, step, loss, predictions = sess.run(\n",
        "                [summaryOp, globalStep, cnn.loss, cnn.predictions],\n",
        "                feed_dict)\n",
        "            \n",
        "            if config.numClasses == 1:\n",
        "            \n",
        "                acc, precision, recall, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
        "            elif config.numClasses > 1:\n",
        "                acc, precision, recall, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
        "            \n",
        "            evalSummaryWriter.add_summary(summary, step)\n",
        "            \n",
        "            return loss, acc, precision, recall, f_beta\n",
        "        \n",
        "        for i in range(config.training.epoches):\n",
        "            # 训练模型\n",
        "            print(\"start training model\")\n",
        "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
        "                loss, acc, prec, recall, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
        "                \n",
        "                currentStep = tf.train.global_step(sess, globalStep) \n",
        "                print(\"train: step: {}, loss: {}, acc: {}, recall: {}, precision: {}, f_beta: {}\".format(\n",
        "                    currentStep, loss, acc, recall, prec, f_beta))\n",
        "                if currentStep % config.training.evaluateEvery == 0:\n",
        "                    print(\"\\nEvaluation:\")\n",
        "                    \n",
        "                    losses = []\n",
        "                    accs = []\n",
        "                    f_betas = []\n",
        "                    precisions = []\n",
        "                    recalls = []\n",
        "                    \n",
        "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
        "                        loss, acc, precision, recall, f_beta = devStep(batchEval[0], batchEval[1])\n",
        "                        losses.append(loss)\n",
        "                        accs.append(acc)\n",
        "                        f_betas.append(f_beta)\n",
        "                        precisions.append(precision)\n",
        "                        recalls.append(recall)\n",
        "                        \n",
        "                    time_str = datetime.datetime.now().isoformat()\n",
        "                    print(\"{}, step: {}, loss: {}, acc: {},precision: {}, recall: {}, f_beta: {}\".format(time_str, currentStep, mean(losses), \n",
        "                                                                                                       mean(accs), mean(precisions),\n",
        "                                                                                                       mean(recalls), mean(f_betas)))\n",
        "                    \n",
        "                if currentStep % config.training.checkpointEvery == 0:\n",
        "                    # 保存模型的另一种方法，保存checkpoint文件\n",
        "                    path = saver.save(sess, mypath/\"model/textCNN/model/my-model\", global_step=currentStep)\n",
        "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
        "                    \n",
        "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(cnn.inputX),\n",
        "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(cnn.dropoutKeepProb)}\n",
        "\n",
        "        outputs = {\"predictions\": tf.saved_model.utils.build_tensor_info(cnn.predictions)}\n",
        "\n",
        "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
        "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
        "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
        "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
        "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
        "\n",
        "        builder.save()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-23-32c693efecf2>:66: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Tensor(\"output/predictions:0\", shape=(?, 1), dtype=int32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/hist is illegal; using conv-maxpool-2/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-2/W:0/grad/sparsity is illegal; using conv-maxpool-2/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/hist is illegal; using conv-maxpool-2/b_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-2/b:0/grad/sparsity is illegal; using conv-maxpool-2/b_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
            "Writing to /content/summarys\n",
            "\n",
            "start training model\n",
            "train: step: 1, loss: 1.6024754047393799, acc: 0.53125, recall: 0.463768115942029, precision: 0.5818181818181818, f_beta: 0.5161290322580645\n",
            "train: step: 2, loss: 2.9574737548828125, acc: 0.546875, recall: 0.9436619718309859, precision: 0.5537190082644629, f_beta: 0.6979166666666666\n",
            "train: step: 3, loss: 1.7187327146530151, acc: 0.46875, recall: 0.6417910447761194, precision: 0.4942528735632184, f_beta: 0.5584415584415584\n",
            "train: step: 4, loss: 1.8464152812957764, acc: 0.515625, recall: 0.1346153846153846, precision: 0.2916666666666667, f_beta: 0.18421052631578946\n",
            "train: step: 5, loss: 2.13045072555542, acc: 0.546875, recall: 0.10909090909090909, precision: 0.4, f_beta: 0.17142857142857143\n",
            "train: step: 6, loss: 2.391529083251953, acc: 0.484375, recall: 0.18461538461538463, precision: 0.48, f_beta: 0.2666666666666667\n",
            "train: step: 7, loss: 1.6981631517410278, acc: 0.4921875, recall: 0.39705882352941174, precision: 0.5294117647058824, f_beta: 0.453781512605042\n",
            "train: step: 8, loss: 1.6935993432998657, acc: 0.5703125, recall: 0.8059701492537313, precision: 0.5625, f_beta: 0.6625766871165644\n",
            "train: step: 9, loss: 2.2893857955932617, acc: 0.5234375, recall: 0.8769230769230769, precision: 0.5181818181818182, f_beta: 0.6514285714285715\n",
            "train: step: 10, loss: 1.90037202835083, acc: 0.5, recall: 0.7894736842105263, precision: 0.4639175257731959, f_beta: 0.5844155844155845\n",
            "train: step: 11, loss: 1.4364829063415527, acc: 0.5546875, recall: 0.8363636363636363, precision: 0.48936170212765956, f_beta: 0.6174496644295302\n",
            "train: step: 12, loss: 1.3881672620773315, acc: 0.5546875, recall: 0.38461538461538464, precision: 0.4444444444444444, f_beta: 0.4123711340206186\n",
            "train: step: 13, loss: 2.154405117034912, acc: 0.4609375, recall: 0.16, precision: 0.6666666666666666, f_beta: 0.25806451612903225\n",
            "train: step: 14, loss: 2.0715951919555664, acc: 0.5078125, recall: 0.17647058823529413, precision: 0.631578947368421, f_beta: 0.27586206896551724\n",
            "train: step: 15, loss: 1.7880176305770874, acc: 0.515625, recall: 0.2903225806451613, precision: 0.5, f_beta: 0.3673469387755102\n",
            "train: step: 16, loss: 1.5788540840148926, acc: 0.4921875, recall: 0.3125, precision: 0.4878048780487805, f_beta: 0.38095238095238093\n",
            "train: step: 17, loss: 1.3583581447601318, acc: 0.4921875, recall: 0.6792452830188679, precision: 0.42857142857142855, f_beta: 0.5255474452554745\n",
            "train: step: 18, loss: 1.4369914531707764, acc: 0.5, recall: 0.6052631578947368, precision: 0.575, f_beta: 0.5897435897435896\n",
            "train: step: 19, loss: 1.467965006828308, acc: 0.546875, recall: 0.8153846153846154, precision: 0.5353535353535354, f_beta: 0.6463414634146342\n",
            "train: step: 20, loss: 1.6645610332489014, acc: 0.5390625, recall: 0.7611940298507462, precision: 0.5425531914893617, f_beta: 0.6335403726708074\n",
            "train: step: 21, loss: 1.458083152770996, acc: 0.546875, recall: 0.71875, precision: 0.5348837209302325, f_beta: 0.6133333333333333\n",
            "train: step: 22, loss: 1.2968617677688599, acc: 0.515625, recall: 0.7142857142857143, precision: 0.46511627906976744, f_beta: 0.5633802816901409\n",
            "train: step: 23, loss: 1.1647793054580688, acc: 0.5859375, recall: 0.5, precision: 0.5660377358490566, f_beta: 0.5309734513274337\n",
            "train: step: 24, loss: 1.5286993980407715, acc: 0.5078125, recall: 0.29850746268656714, precision: 0.5555555555555556, f_beta: 0.3883495145631068\n",
            "train: step: 25, loss: 1.5908823013305664, acc: 0.53125, recall: 0.1864406779661017, precision: 0.4782608695652174, f_beta: 0.26829268292682923\n",
            "train: step: 26, loss: 1.3634124994277954, acc: 0.515625, recall: 0.26229508196721313, precision: 0.48484848484848486, f_beta: 0.34042553191489355\n",
            "train: step: 27, loss: 1.2060301303863525, acc: 0.4765625, recall: 0.4166666666666667, precision: 0.5454545454545454, f_beta: 0.47244094488188976\n",
            "train: step: 28, loss: 1.1233758926391602, acc: 0.5546875, recall: 0.5666666666666667, precision: 0.5230769230769231, f_beta: 0.5439999999999999\n",
            "train: step: 29, loss: 1.277202844619751, acc: 0.4921875, recall: 0.6774193548387096, precision: 0.4827586206896552, f_beta: 0.5637583892617449\n",
            "train: step: 30, loss: 1.2496708631515503, acc: 0.5859375, recall: 0.8428571428571429, precision: 0.5841584158415841, f_beta: 0.6900584795321637\n",
            "train: step: 31, loss: 1.567960500717163, acc: 0.453125, recall: 0.6153846153846154, precision: 0.47058823529411764, f_beta: 0.5333333333333333\n",
            "train: step: 32, loss: 1.4256174564361572, acc: 0.5078125, recall: 0.5901639344262295, precision: 0.4864864864864865, f_beta: 0.5333333333333332\n",
            "train: step: 33, loss: 1.1236774921417236, acc: 0.5390625, recall: 0.6470588235294118, precision: 0.5569620253164557, f_beta: 0.5986394557823129\n",
            "train: step: 34, loss: 1.1318001747131348, acc: 0.546875, recall: 0.463768115942029, precision: 0.6037735849056604, f_beta: 0.5245901639344263\n",
            "train: step: 35, loss: 1.1373629570007324, acc: 0.546875, recall: 0.3548387096774194, precision: 0.55, f_beta: 0.43137254901960786\n",
            "train: step: 36, loss: 1.3152518272399902, acc: 0.5, recall: 0.3, precision: 0.45, f_beta: 0.36000000000000004\n",
            "train: step: 37, loss: 1.2942466735839844, acc: 0.484375, recall: 0.4067796610169492, precision: 0.43636363636363634, f_beta: 0.4210526315789474\n",
            "train: step: 38, loss: 1.1539257764816284, acc: 0.59375, recall: 0.5087719298245614, precision: 0.5471698113207547, f_beta: 0.5272727272727273\n",
            "train: step: 39, loss: 1.071628451347351, acc: 0.578125, recall: 0.5428571428571428, precision: 0.6333333333333333, f_beta: 0.5846153846153846\n",
            "train: step: 40, loss: 1.1967592239379883, acc: 0.5234375, recall: 0.6551724137931034, precision: 0.4810126582278481, f_beta: 0.5547445255474452\n",
            "train: step: 41, loss: 1.0964443683624268, acc: 0.578125, recall: 0.6349206349206349, precision: 0.5633802816901409, f_beta: 0.5970149253731343\n",
            "train: step: 42, loss: 0.8466184139251709, acc: 0.671875, recall: 0.7605633802816901, precision: 0.6835443037974683, f_beta: 0.72\n",
            "train: step: 43, loss: 1.090428113937378, acc: 0.5703125, recall: 0.6515151515151515, precision: 0.5733333333333334, f_beta: 0.6099290780141844\n",
            "train: step: 44, loss: 0.9648450613021851, acc: 0.5546875, recall: 0.6440677966101694, precision: 0.5135135135135135, f_beta: 0.5714285714285714\n",
            "train: step: 45, loss: 0.9462889432907104, acc: 0.6171875, recall: 0.6666666666666666, precision: 0.6, f_beta: 0.631578947368421\n",
            "train: step: 46, loss: 1.1708931922912598, acc: 0.53125, recall: 0.3684210526315789, precision: 0.4666666666666667, f_beta: 0.4117647058823529\n",
            "train: step: 47, loss: 1.0314981937408447, acc: 0.6015625, recall: 0.4897959183673469, precision: 0.48, f_beta: 0.48484848484848486\n",
            "train: step: 48, loss: 1.1068236827850342, acc: 0.5078125, recall: 0.37662337662337664, precision: 0.6590909090909091, f_beta: 0.4793388429752067\n",
            "train: step: 49, loss: 1.058163046836853, acc: 0.546875, recall: 0.4375, precision: 0.56, f_beta: 0.4912280701754386\n",
            "train: step: 50, loss: 1.3147141933441162, acc: 0.53125, recall: 0.5294117647058824, precision: 0.5625, f_beta: 0.5454545454545455\n",
            "train: step: 51, loss: 1.0042203664779663, acc: 0.6171875, recall: 0.8166666666666667, precision: 0.5632183908045977, f_beta: 0.6666666666666665\n",
            "train: step: 52, loss: 1.053128957748413, acc: 0.578125, recall: 0.7121212121212122, precision: 0.573170731707317, f_beta: 0.6351351351351352\n",
            "train: step: 53, loss: 0.9690817594528198, acc: 0.6328125, recall: 0.8545454545454545, precision: 0.5465116279069767, f_beta: 0.6666666666666665\n",
            "train: step: 54, loss: 0.898775577545166, acc: 0.5703125, recall: 0.5555555555555556, precision: 0.6349206349206349, f_beta: 0.5925925925925926\n",
            "train: step: 55, loss: 0.824835479259491, acc: 0.640625, recall: 0.55, precision: 0.6346153846153846, f_beta: 0.5892857142857143\n",
            "train: step: 56, loss: 0.8892414569854736, acc: 0.609375, recall: 0.4918032786885246, precision: 0.6122448979591837, f_beta: 0.5454545454545455\n",
            "train: step: 57, loss: 0.7722786664962769, acc: 0.5859375, recall: 0.42857142857142855, precision: 0.5333333333333333, f_beta: 0.4752475247524753\n",
            "train: step: 58, loss: 0.9431082010269165, acc: 0.609375, recall: 0.5076923076923077, precision: 0.6470588235294118, f_beta: 0.5689655172413793\n",
            "train: step: 59, loss: 0.8839092254638672, acc: 0.6640625, recall: 0.5, precision: 0.627906976744186, f_beta: 0.5567010309278351\n",
            "train: step: 60, loss: 1.0575275421142578, acc: 0.484375, recall: 0.44285714285714284, precision: 0.5344827586206896, f_beta: 0.484375\n",
            "train: step: 61, loss: 0.8367052674293518, acc: 0.6015625, recall: 0.6891891891891891, precision: 0.6455696202531646, f_beta: 0.6666666666666666\n",
            "train: step: 62, loss: 0.7948136329650879, acc: 0.59375, recall: 0.6944444444444444, precision: 0.625, f_beta: 0.6578947368421053\n",
            "train: step: 63, loss: 1.0145833492279053, acc: 0.578125, recall: 0.8088235294117647, precision: 0.5729166666666666, f_beta: 0.6707317073170731\n",
            "train: step: 64, loss: 0.6485534906387329, acc: 0.703125, recall: 0.8714285714285714, precision: 0.6777777777777778, f_beta: 0.7625\n",
            "train: step: 65, loss: 0.8240758776664734, acc: 0.609375, recall: 0.7391304347826086, precision: 0.6144578313253012, f_beta: 0.6710526315789473\n",
            "train: step: 66, loss: 0.731576681137085, acc: 0.609375, recall: 0.6896551724137931, precision: 0.5555555555555556, f_beta: 0.6153846153846154\n",
            "train: step: 67, loss: 0.7348504662513733, acc: 0.65625, recall: 0.5806451612903226, precision: 0.6666666666666666, f_beta: 0.6206896551724138\n",
            "train: step: 68, loss: 0.8136121034622192, acc: 0.5546875, recall: 0.3387096774193548, precision: 0.5675675675675675, f_beta: 0.42424242424242425\n",
            "train: step: 69, loss: 0.763405978679657, acc: 0.6875, recall: 0.4827586206896552, precision: 0.7368421052631579, f_beta: 0.5833333333333334\n",
            "train: step: 70, loss: 0.6578238606452942, acc: 0.6328125, recall: 0.453125, precision: 0.7073170731707317, f_beta: 0.5523809523809524\n",
            "train: step: 71, loss: 0.7332464456558228, acc: 0.671875, recall: 0.5245901639344263, precision: 0.7111111111111111, f_beta: 0.6037735849056605\n",
            "train: step: 72, loss: 0.7952038049697876, acc: 0.625, recall: 0.6615384615384615, precision: 0.6231884057971014, f_beta: 0.6417910447761194\n",
            "train: step: 73, loss: 0.7130774259567261, acc: 0.6484375, recall: 0.8333333333333334, precision: 0.6179775280898876, f_beta: 0.7096774193548387\n",
            "train: step: 74, loss: 0.817663311958313, acc: 0.5703125, recall: 0.6923076923076923, precision: 0.5625, f_beta: 0.6206896551724138\n",
            "train: step: 75, loss: 0.7954233884811401, acc: 0.6171875, recall: 0.746031746031746, precision: 0.5875, f_beta: 0.6573426573426574\n",
            "train: step: 76, loss: 0.7615305781364441, acc: 0.65625, recall: 0.6617647058823529, precision: 0.6818181818181818, f_beta: 0.6716417910447761\n",
            "train: step: 77, loss: 0.6816141605377197, acc: 0.6640625, recall: 0.6911764705882353, precision: 0.6811594202898551, f_beta: 0.6861313868613138\n",
            "train: step: 78, loss: 0.7137383222579956, acc: 0.6328125, recall: 0.5606060606060606, precision: 0.6727272727272727, f_beta: 0.6115702479338843\n",
            "train: step: 79, loss: 0.7682133913040161, acc: 0.6640625, recall: 0.5396825396825397, precision: 0.7083333333333334, f_beta: 0.6126126126126126\n",
            "train: step: 80, loss: 0.7966890931129456, acc: 0.6015625, recall: 0.5714285714285714, precision: 0.6, f_beta: 0.5853658536585366\n",
            "train: step: 81, loss: 0.6209496259689331, acc: 0.7109375, recall: 0.7014925373134329, precision: 0.734375, f_beta: 0.7175572519083969\n",
            "train: step: 82, loss: 0.6826651096343994, acc: 0.6640625, recall: 0.6805555555555556, precision: 0.7101449275362319, f_beta: 0.6950354609929078\n",
            "train: step: 83, loss: 0.7967439889907837, acc: 0.6328125, recall: 0.7162162162162162, precision: 0.6708860759493671, f_beta: 0.6928104575163399\n",
            "train: step: 84, loss: 0.7241517901420593, acc: 0.65625, recall: 0.7794117647058824, precision: 0.6463414634146342, f_beta: 0.7066666666666667\n",
            "train: step: 85, loss: 0.7160075902938843, acc: 0.703125, recall: 0.8169014084507042, precision: 0.6987951807228916, f_beta: 0.7532467532467533\n",
            "train: step: 86, loss: 0.7208576202392578, acc: 0.6484375, recall: 0.71875, precision: 0.6301369863013698, f_beta: 0.6715328467153284\n",
            "train: step: 87, loss: 0.7486571073532104, acc: 0.609375, recall: 0.6428571428571429, precision: 0.6428571428571429, f_beta: 0.6428571428571429\n",
            "train: step: 88, loss: 0.6718162298202515, acc: 0.6875, recall: 0.6521739130434783, precision: 0.7377049180327869, f_beta: 0.6923076923076924\n",
            "train: step: 89, loss: 0.6862618923187256, acc: 0.625, recall: 0.5285714285714286, precision: 0.7115384615384616, f_beta: 0.6065573770491803\n",
            "train: step: 90, loss: 0.6319835186004639, acc: 0.6953125, recall: 0.6825396825396826, precision: 0.6935483870967742, f_beta: 0.6880000000000001\n",
            "train: step: 91, loss: 0.6280557513237, acc: 0.640625, recall: 0.6774193548387096, precision: 0.6176470588235294, f_beta: 0.6461538461538462\n",
            "train: step: 92, loss: 0.49483591318130493, acc: 0.7578125, recall: 0.7454545454545455, precision: 0.7068965517241379, f_beta: 0.7256637168141592\n",
            "train: step: 93, loss: 0.7347779273986816, acc: 0.6171875, recall: 0.6323529411764706, precision: 0.6417910447761194, f_beta: 0.6370370370370371\n",
            "train: step: 94, loss: 0.6525267362594604, acc: 0.703125, recall: 0.7012987012987013, precision: 0.782608695652174, f_beta: 0.7397260273972603\n",
            "train: step: 95, loss: 0.6528512239456177, acc: 0.6640625, recall: 0.7377049180327869, precision: 0.625, f_beta: 0.6766917293233082\n",
            "train: step: 96, loss: 0.5767915844917297, acc: 0.7109375, recall: 0.7575757575757576, precision: 0.704225352112676, f_beta: 0.7299270072992701\n",
            "train: step: 97, loss: 0.7474297285079956, acc: 0.671875, recall: 0.6229508196721312, precision: 0.6666666666666666, f_beta: 0.6440677966101694\n",
            "train: step: 98, loss: 0.6441162824630737, acc: 0.671875, recall: 0.6721311475409836, precision: 0.6507936507936508, f_beta: 0.6612903225806451\n",
            "train: step: 99, loss: 0.6404815912246704, acc: 0.71875, recall: 0.6515151515151515, precision: 0.7678571428571429, f_beta: 0.7049180327868853\n",
            "train: step: 100, loss: 0.46911337971687317, acc: 0.78125, recall: 0.7971014492753623, precision: 0.7971014492753623, f_beta: 0.7971014492753623\n",
            "\n",
            "Evaluation:\n",
            "2019-10-10T08:20:23.038846, step: 100, loss: 0.44722104989565337, acc: 0.8004807692307693,precision: 0.8103165029403585, recall: 0.7984252110394614, f_beta: 0.8033600404249956\n",
            "Saved model checkpoint to ../model/textCNN/model/my-model-100\n",
            "\n",
            "train: step: 101, loss: 0.5759264230728149, acc: 0.78125, recall: 0.7419354838709677, precision: 0.7931034482758621, f_beta: 0.7666666666666667\n",
            "train: step: 102, loss: 0.6454115509986877, acc: 0.65625, recall: 0.671875, precision: 0.6515151515151515, f_beta: 0.6615384615384616\n",
            "train: step: 103, loss: 0.661170482635498, acc: 0.6484375, recall: 0.671875, precision: 0.6417910447761194, f_beta: 0.6564885496183207\n",
            "train: step: 104, loss: 0.48705917596817017, acc: 0.78125, recall: 0.78125, precision: 0.78125, f_beta: 0.78125\n",
            "train: step: 105, loss: 0.5397855043411255, acc: 0.7109375, recall: 0.8125, precision: 0.6753246753246753, f_beta: 0.7375886524822695\n",
            "train: step: 106, loss: 0.649905800819397, acc: 0.671875, recall: 0.7, precision: 0.6363636363636364, f_beta: 0.6666666666666666\n",
            "train: step: 107, loss: 0.5794893503189087, acc: 0.6875, recall: 0.6875, precision: 0.6875, f_beta: 0.6875\n",
            "train: step: 108, loss: 0.5703030228614807, acc: 0.7109375, recall: 0.7230769230769231, precision: 0.7121212121212122, f_beta: 0.717557251908397\n",
            "train: step: 109, loss: 0.4371955692768097, acc: 0.7890625, recall: 0.6721311475409836, precision: 0.8541666666666666, f_beta: 0.7522935779816513\n",
            "train: step: 110, loss: 0.4174453020095825, acc: 0.8203125, recall: 0.7936507936507936, precision: 0.8333333333333334, f_beta: 0.8130081300813008\n",
            "train: step: 111, loss: 0.5271381139755249, acc: 0.734375, recall: 0.7301587301587301, precision: 0.7301587301587301, f_beta: 0.7301587301587301\n",
            "train: step: 112, loss: 0.4397638440132141, acc: 0.7890625, recall: 0.7205882352941176, precision: 0.8596491228070176, f_beta: 0.7839999999999999\n",
            "train: step: 113, loss: 0.6158714294433594, acc: 0.6953125, recall: 0.7681159420289855, precision: 0.6973684210526315, f_beta: 0.7310344827586206\n",
            "train: step: 114, loss: 0.5470080375671387, acc: 0.7890625, recall: 0.9, precision: 0.7590361445783133, f_beta: 0.8235294117647058\n",
            "train: step: 115, loss: 0.588324785232544, acc: 0.703125, recall: 0.8135593220338984, precision: 0.64, f_beta: 0.7164179104477612\n",
            "train: step: 116, loss: 0.5516801476478577, acc: 0.75, recall: 0.8166666666666667, precision: 0.7, f_beta: 0.7538461538461538\n",
            "train: step: 117, loss: 0.587643027305603, acc: 0.7109375, recall: 0.6865671641791045, precision: 0.7419354838709677, f_beta: 0.7131782945736433\n",
            "train: step: 118, loss: 0.4814363718032837, acc: 0.78125, recall: 0.782608695652174, precision: 0.8059701492537313, f_beta: 0.7941176470588236\n",
            "train: step: 119, loss: 0.4719482958316803, acc: 0.796875, recall: 0.803030303030303, precision: 0.803030303030303, f_beta: 0.803030303030303\n",
            "train: step: 120, loss: 0.5949481725692749, acc: 0.671875, recall: 0.5483870967741935, precision: 0.7083333333333334, f_beta: 0.6181818181818182\n",
            "train: step: 121, loss: 0.5309869647026062, acc: 0.71875, recall: 0.639344262295082, precision: 0.7358490566037735, f_beta: 0.6842105263157895\n",
            "train: step: 122, loss: 0.6277672648429871, acc: 0.6640625, recall: 0.6724137931034483, precision: 0.6190476190476191, f_beta: 0.6446280991735537\n",
            "train: step: 123, loss: 0.5660854578018188, acc: 0.6796875, recall: 0.6774193548387096, precision: 0.6666666666666666, f_beta: 0.6719999999999999\n",
            "train: step: 124, loss: 0.6307592391967773, acc: 0.640625, recall: 0.7258064516129032, precision: 0.6081081081081081, f_beta: 0.6617647058823529\n",
            "train: step: 125, loss: 0.4946657419204712, acc: 0.734375, recall: 0.7678571428571429, precision: 0.671875, f_beta: 0.7166666666666668\n",
            "train: step: 126, loss: 0.585029125213623, acc: 0.7109375, recall: 0.7384615384615385, precision: 0.7058823529411765, f_beta: 0.7218045112781954\n",
            "train: step: 127, loss: 0.7429149746894836, acc: 0.6171875, recall: 0.5428571428571428, precision: 0.6909090909090909, f_beta: 0.608\n",
            "train: step: 128, loss: 0.5734277963638306, acc: 0.6953125, recall: 0.6911764705882353, precision: 0.7230769230769231, f_beta: 0.7067669172932332\n",
            "train: step: 129, loss: 0.6349564790725708, acc: 0.6484375, recall: 0.7123287671232876, precision: 0.6842105263157895, f_beta: 0.6979865771812079\n",
            "train: step: 130, loss: 0.4795083999633789, acc: 0.7578125, recall: 0.7837837837837838, precision: 0.7945205479452054, f_beta: 0.7891156462585033\n",
            "train: step: 131, loss: 0.5807867050170898, acc: 0.703125, recall: 0.7580645161290323, precision: 0.6714285714285714, f_beta: 0.7121212121212119\n",
            "train: step: 132, loss: 0.5717123746871948, acc: 0.7578125, recall: 0.8985507246376812, precision: 0.7209302325581395, f_beta: 0.7999999999999999\n",
            "train: step: 133, loss: 0.5125292539596558, acc: 0.7578125, recall: 0.9076923076923077, precision: 0.7023809523809523, f_beta: 0.7919463087248322\n",
            "train: step: 134, loss: 0.5553442239761353, acc: 0.7109375, recall: 0.7121212121212122, precision: 0.7230769230769231, f_beta: 0.717557251908397\n",
            "train: step: 135, loss: 0.4262954592704773, acc: 0.8515625, recall: 0.86, precision: 0.7818181818181819, f_beta: 0.819047619047619\n",
            "train: step: 136, loss: 0.6031452417373657, acc: 0.671875, recall: 0.5901639344262295, precision: 0.6792452830188679, f_beta: 0.631578947368421\n",
            "train: step: 137, loss: 0.5839306116104126, acc: 0.71875, recall: 0.6, precision: 0.84, f_beta: 0.7000000000000001\n",
            "train: step: 138, loss: 0.4286201000213623, acc: 0.8125, recall: 0.7413793103448276, precision: 0.8269230769230769, f_beta: 0.7818181818181817\n",
            "train: step: 139, loss: 0.5712808966636658, acc: 0.734375, recall: 0.7258064516129032, precision: 0.7258064516129032, f_beta: 0.7258064516129032\n",
            "train: step: 140, loss: 0.44621893763542175, acc: 0.7578125, recall: 0.6388888888888888, precision: 0.9019607843137255, f_beta: 0.7479674796747967\n",
            "train: step: 141, loss: 0.5279005765914917, acc: 0.6875, recall: 0.72, precision: 0.5806451612903226, f_beta: 0.6428571428571428\n",
            "train: step: 142, loss: 0.5420205593109131, acc: 0.6953125, recall: 0.8070175438596491, precision: 0.6216216216216216, f_beta: 0.7022900763358778\n",
            "train: step: 143, loss: 0.5563936233520508, acc: 0.7578125, recall: 0.859375, precision: 0.7142857142857143, f_beta: 0.7801418439716311\n",
            "train: step: 144, loss: 0.4919118285179138, acc: 0.765625, recall: 0.847457627118644, precision: 0.704225352112676, f_beta: 0.7692307692307692\n",
            "train: step: 145, loss: 0.4856986403465271, acc: 0.7734375, recall: 0.7666666666666667, precision: 0.7540983606557377, f_beta: 0.7603305785123967\n",
            "train: step: 146, loss: 0.43279844522476196, acc: 0.7734375, recall: 0.7407407407407407, precision: 0.7272727272727273, f_beta: 0.7339449541284404\n",
            "train: step: 147, loss: 0.4119168817996979, acc: 0.8203125, recall: 0.7592592592592593, precision: 0.803921568627451, f_beta: 0.780952380952381\n",
            "train: step: 148, loss: 0.6895373463630676, acc: 0.7109375, recall: 0.6025641025641025, precision: 0.8867924528301887, f_beta: 0.7175572519083969\n",
            "train: step: 149, loss: 0.5485225915908813, acc: 0.71875, recall: 0.6666666666666666, precision: 0.8, f_beta: 0.7272727272727272\n",
            "train: step: 150, loss: 0.5460354089736938, acc: 0.7109375, recall: 0.6551724137931034, precision: 0.6909090909090909, f_beta: 0.6725663716814159\n",
            "train: step: 151, loss: 0.4655420184135437, acc: 0.78125, recall: 0.8363636363636363, precision: 0.7076923076923077, f_beta: 0.7666666666666666\n",
            "train: step: 152, loss: 0.49465465545654297, acc: 0.8125, recall: 0.9253731343283582, precision: 0.7654320987654321, f_beta: 0.8378378378378379\n",
            "train: step: 153, loss: 0.5399836897850037, acc: 0.7265625, recall: 0.8870967741935484, precision: 0.6626506024096386, f_beta: 0.7586206896551725\n",
            "train: step: 154, loss: 0.6247878074645996, acc: 0.703125, recall: 0.7833333333333333, precision: 0.6527777777777778, f_beta: 0.7121212121212122\n",
            "train: step: 155, loss: 0.440712034702301, acc: 0.7890625, recall: 0.8392857142857143, precision: 0.7230769230769231, f_beta: 0.7768595041322314\n",
            "train: step: 156, loss: 0.4434541165828705, acc: 0.765625, recall: 0.7321428571428571, precision: 0.7321428571428571, f_beta: 0.7321428571428571\n",
            "start training model\n",
            "train: step: 157, loss: 0.4919378459453583, acc: 0.78125, recall: 0.6666666666666666, precision: 0.8085106382978723, f_beta: 0.7307692307692306\n",
            "train: step: 158, loss: 0.5055369138717651, acc: 0.7578125, recall: 0.609375, precision: 0.8666666666666667, f_beta: 0.7155963302752294\n",
            "train: step: 159, loss: 0.4598841071128845, acc: 0.8046875, recall: 0.6666666666666666, precision: 0.8636363636363636, f_beta: 0.7524752475247524\n",
            "train: step: 160, loss: 0.4083690643310547, acc: 0.8203125, recall: 0.7288135593220338, precision: 0.86, f_beta: 0.7889908256880733\n",
            "train: step: 161, loss: 0.4156595468521118, acc: 0.8203125, recall: 0.7575757575757576, precision: 0.8771929824561403, f_beta: 0.8130081300813008\n",
            "train: step: 162, loss: 0.49128806591033936, acc: 0.734375, recall: 0.8245614035087719, precision: 0.6619718309859155, f_beta: 0.734375\n",
            "train: step: 163, loss: 0.40044689178466797, acc: 0.828125, recall: 0.863013698630137, precision: 0.84, f_beta: 0.8513513513513513\n",
            "train: step: 164, loss: 0.47707292437553406, acc: 0.7734375, recall: 0.8888888888888888, precision: 0.717948717948718, f_beta: 0.7943262411347517\n",
            "train: step: 165, loss: 0.44649800658226013, acc: 0.796875, recall: 0.8923076923076924, precision: 0.7532467532467533, f_beta: 0.8169014084507042\n",
            "train: step: 166, loss: 0.4700084328651428, acc: 0.7578125, recall: 0.9180327868852459, precision: 0.6829268292682927, f_beta: 0.7832167832167832\n",
            "train: step: 167, loss: 0.3957386612892151, acc: 0.7890625, recall: 0.896551724137931, precision: 0.7123287671232876, f_beta: 0.7938931297709924\n",
            "train: step: 168, loss: 0.3636115789413452, acc: 0.84375, recall: 0.8088235294117647, precision: 0.8870967741935484, f_beta: 0.8461538461538463\n",
            "train: step: 169, loss: 0.4294447898864746, acc: 0.78125, recall: 0.6956521739130435, precision: 0.8727272727272727, f_beta: 0.7741935483870968\n",
            "train: step: 170, loss: 0.48023808002471924, acc: 0.7890625, recall: 0.7285714285714285, precision: 0.864406779661017, f_beta: 0.7906976744186046\n",
            "train: step: 171, loss: 0.49681609869003296, acc: 0.8203125, recall: 0.7037037037037037, precision: 0.8444444444444444, f_beta: 0.7676767676767676\n",
            "train: step: 172, loss: 0.4421907961368561, acc: 0.828125, recall: 0.7457627118644068, precision: 0.8627450980392157, f_beta: 0.8\n",
            "train: step: 173, loss: 0.3786168098449707, acc: 0.8359375, recall: 0.8507462686567164, precision: 0.8382352941176471, f_beta: 0.8444444444444444\n",
            "train: step: 174, loss: 0.41015613079071045, acc: 0.8359375, recall: 0.875, precision: 0.8115942028985508, f_beta: 0.8421052631578948\n",
            "train: step: 175, loss: 0.4310073256492615, acc: 0.78125, recall: 0.8870967741935484, precision: 0.7236842105263158, f_beta: 0.7971014492753624\n",
            "train: step: 176, loss: 0.40281030535697937, acc: 0.7890625, recall: 0.8857142857142857, precision: 0.7654320987654321, f_beta: 0.8211920529801325\n",
            "train: step: 177, loss: 0.32656538486480713, acc: 0.875, recall: 0.9571428571428572, precision: 0.8375, f_beta: 0.8933333333333334\n",
            "train: step: 178, loss: 0.3920707702636719, acc: 0.7734375, recall: 0.8970588235294118, precision: 0.7349397590361446, f_beta: 0.8079470198675497\n",
            "train: step: 179, loss: 0.42498600482940674, acc: 0.8125, recall: 0.8548387096774194, precision: 0.7794117647058824, f_beta: 0.8153846153846154\n",
            "train: step: 180, loss: 0.4337887167930603, acc: 0.8203125, recall: 0.8285714285714286, precision: 0.8405797101449275, f_beta: 0.8345323741007195\n",
            "train: step: 181, loss: 0.3598228991031647, acc: 0.8515625, recall: 0.9107142857142857, precision: 0.7846153846153846, f_beta: 0.8429752066115703\n",
            "train: step: 182, loss: 0.3700249195098877, acc: 0.828125, recall: 0.7894736842105263, precision: 0.8181818181818182, f_beta: 0.8035714285714286\n",
            "train: step: 183, loss: 0.5767689943313599, acc: 0.703125, recall: 0.5538461538461539, precision: 0.8, f_beta: 0.6545454545454547\n",
            "train: step: 184, loss: 0.44201624393463135, acc: 0.8203125, recall: 0.75, precision: 0.8490566037735849, f_beta: 0.7964601769911505\n",
            "train: step: 185, loss: 0.3331945240497589, acc: 0.859375, recall: 0.8688524590163934, precision: 0.8412698412698413, f_beta: 0.8548387096774194\n",
            "train: step: 186, loss: 0.37395405769348145, acc: 0.8671875, recall: 0.8253968253968254, precision: 0.896551724137931, f_beta: 0.8595041322314049\n",
            "train: step: 187, loss: 0.35537922382354736, acc: 0.84375, recall: 0.8392857142857143, precision: 0.8103448275862069, f_beta: 0.8245614035087718\n",
            "train: step: 188, loss: 0.4317016899585724, acc: 0.7890625, recall: 0.8055555555555556, precision: 0.8169014084507042, f_beta: 0.8111888111888113\n",
            "train: step: 189, loss: 0.4491490125656128, acc: 0.8125, recall: 0.8333333333333334, precision: 0.8333333333333334, f_beta: 0.8333333333333334\n",
            "train: step: 190, loss: 0.455244243144989, acc: 0.8203125, recall: 0.855072463768116, precision: 0.8194444444444444, f_beta: 0.8368794326241135\n",
            "train: step: 191, loss: 0.5227493643760681, acc: 0.75, recall: 0.90625, precision: 0.6904761904761905, f_beta: 0.7837837837837837\n",
            "train: step: 192, loss: 0.3979209065437317, acc: 0.7890625, recall: 0.8412698412698413, precision: 0.7571428571428571, f_beta: 0.7969924812030075\n",
            "train: step: 193, loss: 0.4781372547149658, acc: 0.7734375, recall: 0.8181818181818182, precision: 0.7605633802816901, f_beta: 0.7883211678832117\n",
            "train: step: 194, loss: 0.3412967026233673, acc: 0.8515625, recall: 0.9104477611940298, precision: 0.8243243243243243, f_beta: 0.8652482269503546\n",
            "train: step: 195, loss: 0.4478754699230194, acc: 0.7734375, recall: 0.7727272727272727, precision: 0.7846153846153846, f_beta: 0.7786259541984732\n",
            "train: step: 196, loss: 0.4120602309703827, acc: 0.8203125, recall: 0.75, precision: 0.8490566037735849, f_beta: 0.7964601769911505\n",
            "train: step: 197, loss: 0.5907267332077026, acc: 0.71875, recall: 0.6666666666666666, precision: 0.7368421052631579, f_beta: 0.7\n",
            "train: step: 198, loss: 0.3566461205482483, acc: 0.84375, recall: 0.796875, precision: 0.8793103448275862, f_beta: 0.8360655737704917\n",
            "train: step: 199, loss: 0.3801742494106293, acc: 0.8046875, recall: 0.8225806451612904, precision: 0.7846153846153846, f_beta: 0.8031496062992126\n",
            "train: step: 200, loss: 0.4258832335472107, acc: 0.84375, recall: 0.8412698412698413, precision: 0.8412698412698413, f_beta: 0.8412698412698413\n",
            "\n",
            "Evaluation:\n",
            "2019-10-10T08:25:18.993540, step: 200, loss: 0.3724209414078639, acc: 0.8381410256410257,precision: 0.8763803290385953, recall: 0.8171885698396141, f_beta: 0.8446227586283707\n",
            "Saved model checkpoint to ../model/textCNN/model/my-model-200\n",
            "\n",
            "train: step: 201, loss: 0.3390645682811737, acc: 0.84375, recall: 0.8769230769230769, precision: 0.8260869565217391, f_beta: 0.8507462686567164\n",
            "train: step: 202, loss: 0.3721781373023987, acc: 0.8359375, recall: 0.9253731343283582, precision: 0.7948717948717948, f_beta: 0.8551724137931035\n",
            "train: step: 203, loss: 0.4100312888622284, acc: 0.8515625, recall: 0.8985507246376812, precision: 0.8378378378378378, f_beta: 0.8671328671328672\n",
            "train: step: 204, loss: 0.3468446731567383, acc: 0.8515625, recall: 0.8484848484848485, precision: 0.8615384615384616, f_beta: 0.8549618320610687\n",
            "train: step: 205, loss: 0.37597835063934326, acc: 0.84375, recall: 0.8285714285714286, precision: 0.8787878787878788, f_beta: 0.8529411764705883\n",
            "train: step: 206, loss: 0.4025519788265228, acc: 0.8515625, recall: 0.8983050847457628, precision: 0.803030303030303, f_beta: 0.8480000000000001\n",
            "train: step: 207, loss: 0.4418279528617859, acc: 0.8203125, recall: 0.7971014492753623, precision: 0.859375, f_beta: 0.8270676691729324\n",
            "train: step: 208, loss: 0.43554747104644775, acc: 0.8046875, recall: 0.7241379310344828, precision: 0.8235294117647058, f_beta: 0.7706422018348623\n",
            "train: step: 209, loss: 0.3317105770111084, acc: 0.8359375, recall: 0.8253968253968254, precision: 0.8387096774193549, f_beta: 0.832\n",
            "train: step: 210, loss: 0.39935919642448425, acc: 0.8203125, recall: 0.8484848484848485, precision: 0.8115942028985508, f_beta: 0.8296296296296296\n",
            "train: step: 211, loss: 0.3245593011379242, acc: 0.8671875, recall: 0.875, precision: 0.8615384615384616, f_beta: 0.8682170542635659\n",
            "train: step: 212, loss: 0.37052491307258606, acc: 0.78125, recall: 0.8387096774193549, precision: 0.7428571428571429, f_beta: 0.787878787878788\n",
            "train: step: 213, loss: 0.44977453351020813, acc: 0.78125, recall: 0.7868852459016393, precision: 0.7619047619047619, f_beta: 0.7741935483870968\n",
            "train: step: 214, loss: 0.32031989097595215, acc: 0.8359375, recall: 0.8666666666666667, precision: 0.8, f_beta: 0.832\n",
            "train: step: 215, loss: 0.3799394369125366, acc: 0.828125, recall: 0.78125, precision: 0.8620689655172413, f_beta: 0.8196721311475409\n",
            "train: step: 216, loss: 0.4475141167640686, acc: 0.7890625, recall: 0.782608695652174, precision: 0.8181818181818182, f_beta: 0.8\n",
            "train: step: 217, loss: 0.35400038957595825, acc: 0.8203125, recall: 0.8181818181818182, precision: 0.8307692307692308, f_beta: 0.8244274809160306\n",
            "train: step: 218, loss: 0.40643587708473206, acc: 0.84375, recall: 0.8636363636363636, precision: 0.8382352941176471, f_beta: 0.8507462686567164\n",
            "train: step: 219, loss: 0.31424352526664734, acc: 0.8515625, recall: 0.835820895522388, precision: 0.875, f_beta: 0.8549618320610687\n",
            "train: step: 220, loss: 0.33808785676956177, acc: 0.875, recall: 0.8771929824561403, precision: 0.847457627118644, f_beta: 0.8620689655172413\n",
            "train: step: 221, loss: 0.37077611684799194, acc: 0.796875, recall: 0.9137931034482759, precision: 0.7162162162162162, f_beta: 0.8030303030303031\n",
            "train: step: 222, loss: 0.38452208042144775, acc: 0.796875, recall: 0.8225806451612904, precision: 0.7727272727272727, f_beta: 0.796875\n",
            "train: step: 223, loss: 0.37363380193710327, acc: 0.8203125, recall: 0.8333333333333334, precision: 0.8208955223880597, f_beta: 0.8270676691729324\n",
            "train: step: 224, loss: 0.35843780636787415, acc: 0.828125, recall: 0.8051948051948052, precision: 0.8985507246376812, f_beta: 0.8493150684931507\n",
            "train: step: 225, loss: 0.27963727712631226, acc: 0.875, recall: 0.8421052631578947, precision: 0.8727272727272727, f_beta: 0.8571428571428571\n",
            "train: step: 226, loss: 0.3521043062210083, acc: 0.8203125, recall: 0.7794117647058824, precision: 0.8688524590163934, f_beta: 0.8217054263565892\n",
            "train: step: 227, loss: 0.46244901418685913, acc: 0.796875, recall: 0.7142857142857143, precision: 0.8, f_beta: 0.7547169811320756\n",
            "train: step: 228, loss: 0.33491915464401245, acc: 0.8671875, recall: 0.8870967741935484, precision: 0.8461538461538461, f_beta: 0.8661417322834646\n",
            "train: step: 229, loss: 0.43252161145210266, acc: 0.8125, recall: 0.8636363636363636, precision: 0.7916666666666666, f_beta: 0.8260869565217391\n",
            "train: step: 230, loss: 0.3798312544822693, acc: 0.8125, recall: 0.8596491228070176, precision: 0.7538461538461538, f_beta: 0.8032786885245902\n",
            "train: step: 231, loss: 0.3631187379360199, acc: 0.84375, recall: 0.8307692307692308, precision: 0.8571428571428571, f_beta: 0.84375\n",
            "train: step: 232, loss: 0.3704896569252014, acc: 0.84375, recall: 0.855072463768116, precision: 0.855072463768116, f_beta: 0.855072463768116\n",
            "train: step: 233, loss: 0.36725956201553345, acc: 0.8203125, recall: 0.819672131147541, precision: 0.8064516129032258, f_beta: 0.8130081300813008\n",
            "train: step: 234, loss: 0.34015461802482605, acc: 0.859375, recall: 0.8181818181818182, precision: 0.9, f_beta: 0.8571428571428572\n",
            "train: step: 235, loss: 0.32603949308395386, acc: 0.875, recall: 0.9090909090909091, precision: 0.8571428571428571, f_beta: 0.8823529411764706\n",
            "train: step: 236, loss: 0.44941580295562744, acc: 0.8125, recall: 0.8166666666666667, precision: 0.7903225806451613, f_beta: 0.8032786885245902\n",
            "train: step: 237, loss: 0.38885045051574707, acc: 0.8046875, recall: 0.8, precision: 0.7868852459016393, f_beta: 0.7933884297520661\n",
            "train: step: 238, loss: 0.3448522686958313, acc: 0.8203125, recall: 0.835820895522388, precision: 0.8235294117647058, f_beta: 0.8296296296296296\n",
            "train: step: 239, loss: 0.3904179334640503, acc: 0.796875, recall: 0.7878787878787878, precision: 0.8125, f_beta: 0.8\n",
            "train: step: 240, loss: 0.42608410120010376, acc: 0.78125, recall: 0.7910447761194029, precision: 0.7910447761194029, f_beta: 0.7910447761194029\n",
            "train: step: 241, loss: 0.2807731628417969, acc: 0.859375, recall: 0.8307692307692308, precision: 0.8852459016393442, f_beta: 0.8571428571428572\n",
            "train: step: 242, loss: 0.37417271733283997, acc: 0.8203125, recall: 0.8636363636363636, precision: 0.8028169014084507, f_beta: 0.832116788321168\n",
            "train: step: 243, loss: 0.34893032908439636, acc: 0.84375, recall: 0.8529411764705882, precision: 0.8529411764705882, f_beta: 0.8529411764705882\n",
            "train: step: 244, loss: 0.36949974298477173, acc: 0.8203125, recall: 0.8387096774193549, precision: 0.8, f_beta: 0.8188976377952757\n",
            "train: step: 245, loss: 0.3952612280845642, acc: 0.828125, recall: 0.896551724137931, precision: 0.7647058823529411, f_beta: 0.8253968253968255\n",
            "train: step: 246, loss: 0.37008506059646606, acc: 0.8203125, recall: 0.8571428571428571, precision: 0.7941176470588235, f_beta: 0.8244274809160305\n",
            "train: step: 247, loss: 0.27508676052093506, acc: 0.8984375, recall: 0.8805970149253731, precision: 0.921875, f_beta: 0.9007633587786259\n",
            "train: step: 248, loss: 0.3602284789085388, acc: 0.8125, recall: 0.8070175438596491, precision: 0.7796610169491526, f_beta: 0.7931034482758621\n",
            "train: step: 249, loss: 0.3352222442626953, acc: 0.890625, recall: 0.8611111111111112, precision: 0.9393939393939394, f_beta: 0.8985507246376813\n",
            "train: step: 250, loss: 0.2844913601875305, acc: 0.8515625, recall: 0.8032786885245902, precision: 0.875, f_beta: 0.8376068376068376\n",
            "train: step: 251, loss: 0.3489941358566284, acc: 0.8828125, recall: 0.855072463768116, precision: 0.921875, f_beta: 0.887218045112782\n",
            "train: step: 252, loss: 0.3470190465450287, acc: 0.84375, recall: 0.8095238095238095, precision: 0.864406779661017, f_beta: 0.8360655737704918\n",
            "train: step: 253, loss: 0.3895387053489685, acc: 0.8359375, recall: 0.8461538461538461, precision: 0.8333333333333334, f_beta: 0.8396946564885497\n",
            "train: step: 254, loss: 0.3429252505302429, acc: 0.8203125, recall: 0.9180327868852459, precision: 0.7567567567567568, f_beta: 0.8296296296296297\n",
            "train: step: 255, loss: 0.40580999851226807, acc: 0.8203125, recall: 0.896551724137931, precision: 0.7536231884057971, f_beta: 0.8188976377952756\n",
            "train: step: 256, loss: 0.49285268783569336, acc: 0.8046875, recall: 0.8412698412698413, precision: 0.7794117647058824, f_beta: 0.8091603053435115\n",
            "train: step: 257, loss: 0.3381836414337158, acc: 0.8359375, recall: 0.859375, precision: 0.8208955223880597, f_beta: 0.8396946564885497\n",
            "train: step: 258, loss: 0.4120575785636902, acc: 0.8125, recall: 0.7575757575757576, precision: 0.8620689655172413, f_beta: 0.8064516129032258\n",
            "train: step: 259, loss: 0.2547687888145447, acc: 0.9140625, recall: 0.9090909090909091, precision: 0.8928571428571429, f_beta: 0.9009009009009009\n",
            "train: step: 260, loss: 0.3262288272380829, acc: 0.8515625, recall: 0.8533333333333334, precision: 0.8888888888888888, f_beta: 0.8707482993197277\n",
            "train: step: 261, loss: 0.33955684304237366, acc: 0.875, recall: 0.8103448275862069, precision: 0.9038461538461539, f_beta: 0.8545454545454546\n",
            "train: step: 262, loss: 0.37575095891952515, acc: 0.84375, recall: 0.8596491228070176, precision: 0.8032786885245902, f_beta: 0.8305084745762712\n",
            "train: step: 263, loss: 0.40216800570487976, acc: 0.84375, recall: 0.9016393442622951, precision: 0.7971014492753623, f_beta: 0.8461538461538461\n",
            "train: step: 264, loss: 0.3244231939315796, acc: 0.859375, recall: 0.8125, precision: 0.896551724137931, f_beta: 0.8524590163934426\n",
            "train: step: 265, loss: 0.4324533939361572, acc: 0.8125, recall: 0.782608695652174, precision: 0.8571428571428571, f_beta: 0.8181818181818182\n",
            "train: step: 266, loss: 0.45995235443115234, acc: 0.765625, recall: 0.803030303030303, precision: 0.7571428571428571, f_beta: 0.7794117647058824\n",
            "train: step: 267, loss: 0.4296633005142212, acc: 0.8359375, recall: 0.8571428571428571, precision: 0.7868852459016393, f_beta: 0.8205128205128205\n",
            "train: step: 268, loss: 0.3783422112464905, acc: 0.8515625, recall: 0.8428571428571429, precision: 0.8805970149253731, f_beta: 0.8613138686131387\n",
            "train: step: 269, loss: 0.39564216136932373, acc: 0.828125, recall: 0.8275862068965517, precision: 0.8, f_beta: 0.8135593220338982\n",
            "train: step: 270, loss: 0.3733486235141754, acc: 0.828125, recall: 0.8709677419354839, precision: 0.7941176470588235, f_beta: 0.8307692307692308\n",
            "train: step: 271, loss: 0.4042501449584961, acc: 0.8125, recall: 0.7352941176470589, precision: 0.8928571428571429, f_beta: 0.806451612903226\n",
            "train: step: 272, loss: 0.3384411334991455, acc: 0.859375, recall: 0.875, precision: 0.8166666666666667, f_beta: 0.8448275862068966\n",
            "train: step: 273, loss: 0.3774775266647339, acc: 0.84375, recall: 0.8805970149253731, precision: 0.8309859154929577, f_beta: 0.8550724637681161\n",
            "train: step: 274, loss: 0.38916465640068054, acc: 0.7890625, recall: 0.8656716417910447, precision: 0.7631578947368421, f_beta: 0.8111888111888113\n",
            "train: step: 275, loss: 0.34151187539100647, acc: 0.84375, recall: 0.8125, precision: 0.8666666666666667, f_beta: 0.8387096774193549\n",
            "train: step: 276, loss: 0.31428593397140503, acc: 0.875, recall: 0.8714285714285714, precision: 0.8970588235294118, f_beta: 0.8840579710144928\n",
            "train: step: 277, loss: 0.42827877402305603, acc: 0.8046875, recall: 0.8615384615384616, precision: 0.7777777777777778, f_beta: 0.8175182481751826\n",
            "train: step: 278, loss: 0.301036536693573, acc: 0.8671875, recall: 0.8245614035087719, precision: 0.8703703703703703, f_beta: 0.8468468468468469\n",
            "train: step: 279, loss: 0.4015369415283203, acc: 0.8203125, recall: 0.8507462686567164, precision: 0.8142857142857143, f_beta: 0.832116788321168\n",
            "train: step: 280, loss: 0.3134189546108246, acc: 0.875, recall: 0.9452054794520548, precision: 0.8518518518518519, f_beta: 0.8961038961038961\n",
            "train: step: 281, loss: 0.35856980085372925, acc: 0.8359375, recall: 0.8253968253968254, precision: 0.8387096774193549, f_beta: 0.832\n",
            "train: step: 282, loss: 0.31642961502075195, acc: 0.890625, recall: 0.8363636363636363, precision: 0.9019607843137255, f_beta: 0.8679245283018867\n",
            "train: step: 283, loss: 0.5147735476493835, acc: 0.78125, recall: 0.7833333333333333, precision: 0.7580645161290323, f_beta: 0.7704918032786884\n",
            "train: step: 284, loss: 0.37458711862564087, acc: 0.84375, recall: 0.7916666666666666, precision: 0.7916666666666666, f_beta: 0.7916666666666666\n",
            "train: step: 285, loss: 0.302345871925354, acc: 0.8984375, recall: 0.8524590163934426, precision: 0.9285714285714286, f_beta: 0.888888888888889\n",
            "train: step: 286, loss: 0.3337923288345337, acc: 0.8515625, recall: 0.8461538461538461, precision: 0.859375, f_beta: 0.8527131782945736\n",
            "train: step: 287, loss: 0.271906316280365, acc: 0.875, recall: 0.8461538461538461, precision: 0.9016393442622951, f_beta: 0.873015873015873\n",
            "train: step: 288, loss: 0.5221351981163025, acc: 0.78125, recall: 0.71875, precision: 0.8214285714285714, f_beta: 0.7666666666666666\n",
            "train: step: 289, loss: 0.3381252586841583, acc: 0.8359375, recall: 0.8484848484848485, precision: 0.835820895522388, f_beta: 0.8421052631578948\n",
            "train: step: 290, loss: 0.31964239478111267, acc: 0.875, recall: 0.9, precision: 0.875, f_beta: 0.8873239436619719\n",
            "train: step: 291, loss: 0.39605873823165894, acc: 0.828125, recall: 0.921875, precision: 0.7763157894736842, f_beta: 0.842857142857143\n",
            "train: step: 292, loss: 0.33899253606796265, acc: 0.8671875, recall: 0.88, precision: 0.8918918918918919, f_beta: 0.8859060402684563\n",
            "train: step: 293, loss: 0.48046761751174927, acc: 0.7890625, recall: 0.803921568627451, precision: 0.7068965517241379, f_beta: 0.7522935779816514\n",
            "train: step: 294, loss: 0.3739163875579834, acc: 0.859375, recall: 0.890625, precision: 0.8382352941176471, f_beta: 0.8636363636363636\n",
            "train: step: 295, loss: 0.3264167904853821, acc: 0.84375, recall: 0.8513513513513513, precision: 0.875, f_beta: 0.863013698630137\n",
            "train: step: 296, loss: 0.2947522699832916, acc: 0.875, recall: 0.9104477611940298, precision: 0.8591549295774648, f_beta: 0.8840579710144928\n",
            "train: step: 297, loss: 0.3444420397281647, acc: 0.859375, recall: 0.8636363636363636, precision: 0.8636363636363636, f_beta: 0.8636363636363636\n",
            "train: step: 298, loss: 0.34665316343307495, acc: 0.875, recall: 0.8636363636363636, precision: 0.890625, f_beta: 0.8769230769230768\n",
            "train: step: 299, loss: 0.30921849608421326, acc: 0.890625, recall: 0.8939393939393939, precision: 0.8939393939393939, f_beta: 0.8939393939393939\n",
            "train: step: 300, loss: 0.2870839238166809, acc: 0.84375, recall: 0.8125, precision: 0.8666666666666667, f_beta: 0.8387096774193549\n",
            "\n",
            "Evaluation:\n",
            "2019-10-10T08:30:14.381674, step: 300, loss: 0.3404175367874977, acc: 0.8531650641025641,precision: 0.8709591149288968, recall: 0.8448602782162226, f_beta: 0.8565336915596609\n",
            "Saved model checkpoint to ../model/textCNN/model/my-model-300\n",
            "\n",
            "train: step: 301, loss: 0.24680493772029877, acc: 0.8984375, recall: 0.9375, precision: 0.8695652173913043, f_beta: 0.9022556390977444\n",
            "train: step: 302, loss: 0.39438602328300476, acc: 0.828125, recall: 0.8769230769230769, precision: 0.8028169014084507, f_beta: 0.8382352941176471\n",
            "train: step: 303, loss: 0.3383675217628479, acc: 0.8671875, recall: 0.9076923076923077, precision: 0.8428571428571429, f_beta: 0.8740740740740741\n",
            "train: step: 304, loss: 0.2707517445087433, acc: 0.890625, recall: 0.9032258064516129, precision: 0.875, f_beta: 0.8888888888888888\n",
            "train: step: 305, loss: 0.4024692177772522, acc: 0.84375, recall: 0.8461538461538461, precision: 0.8461538461538461, f_beta: 0.8461538461538461\n",
            "train: step: 306, loss: 0.3615248501300812, acc: 0.84375, recall: 0.8676470588235294, precision: 0.8428571428571429, f_beta: 0.855072463768116\n",
            "train: step: 307, loss: 0.3774958848953247, acc: 0.828125, recall: 0.8309859154929577, precision: 0.855072463768116, f_beta: 0.8428571428571429\n",
            "train: step: 308, loss: 0.3486378788948059, acc: 0.8671875, recall: 0.9090909090909091, precision: 0.8450704225352113, f_beta: 0.8759124087591241\n",
            "train: step: 309, loss: 0.3696480393409729, acc: 0.8671875, recall: 0.9032258064516129, precision: 0.835820895522388, f_beta: 0.8682170542635659\n",
            "train: step: 310, loss: 0.48368459939956665, acc: 0.7578125, recall: 0.8775510204081632, precision: 0.6323529411764706, f_beta: 0.7350427350427351\n",
            "train: step: 311, loss: 0.3552234172821045, acc: 0.8671875, recall: 0.873015873015873, precision: 0.859375, f_beta: 0.8661417322834646\n",
            "train: step: 312, loss: 0.3527246117591858, acc: 0.84375, recall: 0.765625, precision: 0.9074074074074074, f_beta: 0.8305084745762712\n",
            "start training model\n",
            "train: step: 313, loss: 0.2664405107498169, acc: 0.875, recall: 0.84375, precision: 0.9, f_beta: 0.870967741935484\n",
            "train: step: 314, loss: 0.38955211639404297, acc: 0.8515625, recall: 0.8, precision: 0.9180327868852459, f_beta: 0.8549618320610688\n",
            "train: step: 315, loss: 0.32300448417663574, acc: 0.859375, recall: 0.8309859154929577, precision: 0.9076923076923077, f_beta: 0.8676470588235293\n",
            "train: step: 316, loss: 0.20782728493213654, acc: 0.9296875, recall: 0.9516129032258065, precision: 0.9076923076923077, f_beta: 0.9291338582677167\n",
            "train: step: 317, loss: 0.2594594359397888, acc: 0.8671875, recall: 0.9014084507042254, precision: 0.8648648648648649, f_beta: 0.8827586206896552\n",
            "train: step: 318, loss: 0.31554436683654785, acc: 0.8828125, recall: 0.984375, precision: 0.8181818181818182, f_beta: 0.8936170212765957\n",
            "train: step: 319, loss: 0.3905429542064667, acc: 0.8125, recall: 0.9206349206349206, precision: 0.7532467532467533, f_beta: 0.8285714285714286\n",
            "train: step: 320, loss: 0.25796931982040405, acc: 0.8828125, recall: 0.9180327868852459, precision: 0.8484848484848485, f_beta: 0.8818897637795275\n",
            "train: step: 321, loss: 0.2828432321548462, acc: 0.8671875, recall: 0.8823529411764706, precision: 0.8695652173913043, f_beta: 0.8759124087591241\n",
            "train: step: 322, loss: 0.33592909574508667, acc: 0.84375, recall: 0.8088235294117647, precision: 0.8870967741935484, f_beta: 0.8461538461538463\n",
            "train: step: 323, loss: 0.18135790526866913, acc: 0.953125, recall: 0.9666666666666667, precision: 0.9354838709677419, f_beta: 0.9508196721311476\n",
            "train: step: 324, loss: 0.22529175877571106, acc: 0.921875, recall: 0.8840579710144928, precision: 0.9682539682539683, f_beta: 0.9242424242424243\n",
            "train: step: 325, loss: 0.34170985221862793, acc: 0.859375, recall: 0.855072463768116, precision: 0.8805970149253731, f_beta: 0.8676470588235295\n",
            "train: step: 326, loss: 0.28010866045951843, acc: 0.8984375, recall: 0.8813559322033898, precision: 0.896551724137931, f_beta: 0.888888888888889\n",
            "train: step: 327, loss: 0.3587040305137634, acc: 0.84375, recall: 0.8333333333333334, precision: 0.8823529411764706, f_beta: 0.8571428571428571\n",
            "train: step: 328, loss: 0.3352832794189453, acc: 0.8671875, recall: 0.9, precision: 0.8307692307692308, f_beta: 0.8640000000000001\n",
            "train: step: 329, loss: 0.28507283329963684, acc: 0.875, recall: 0.9473684210526315, precision: 0.8059701492537313, f_beta: 0.8709677419354839\n",
            "train: step: 330, loss: 0.33207207918167114, acc: 0.8515625, recall: 0.8596491228070176, precision: 0.8166666666666667, f_beta: 0.8376068376068376\n",
            "train: step: 331, loss: 0.2720121443271637, acc: 0.890625, recall: 0.9565217391304348, precision: 0.8571428571428571, f_beta: 0.904109589041096\n",
            "train: step: 332, loss: 0.2770398259162903, acc: 0.8828125, recall: 0.8591549295774648, precision: 0.9242424242424242, f_beta: 0.8905109489051095\n",
            "train: step: 333, loss: 0.2743443250656128, acc: 0.8984375, recall: 0.8688524590163934, precision: 0.9137931034482759, f_beta: 0.8907563025210085\n",
            "train: step: 334, loss: 0.27717262506484985, acc: 0.8515625, recall: 0.8636363636363636, precision: 0.8507462686567164, f_beta: 0.8571428571428571\n",
            "train: step: 335, loss: 0.28406578302383423, acc: 0.875, recall: 0.8833333333333333, precision: 0.8548387096774194, f_beta: 0.8688524590163934\n",
            "train: step: 336, loss: 0.22845131158828735, acc: 0.9140625, recall: 0.9090909090909091, precision: 0.9230769230769231, f_beta: 0.9160305343511451\n",
            "train: step: 337, loss: 0.25583797693252563, acc: 0.921875, recall: 0.896551724137931, precision: 0.9285714285714286, f_beta: 0.912280701754386\n",
            "train: step: 338, loss: 0.226598858833313, acc: 0.8984375, recall: 0.8888888888888888, precision: 0.9032258064516129, f_beta: 0.8959999999999999\n",
            "train: step: 339, loss: 0.2908484935760498, acc: 0.875, recall: 0.9107142857142857, precision: 0.8225806451612904, f_beta: 0.864406779661017\n",
            "train: step: 340, loss: 0.28759682178497314, acc: 0.921875, recall: 0.8955223880597015, precision: 0.9523809523809523, f_beta: 0.923076923076923\n",
            "train: step: 341, loss: 0.27009469270706177, acc: 0.8984375, recall: 0.8867924528301887, precision: 0.8703703703703703, f_beta: 0.8785046728971964\n",
            "train: step: 342, loss: 0.3964479863643646, acc: 0.828125, recall: 0.8909090909090909, precision: 0.7538461538461538, f_beta: 0.8166666666666667\n",
            "train: step: 343, loss: 0.30594533681869507, acc: 0.8359375, recall: 0.8591549295774648, precision: 0.8472222222222222, f_beta: 0.8531468531468532\n",
            "train: step: 344, loss: 0.2523948550224304, acc: 0.8828125, recall: 0.9264705882352942, precision: 0.863013698630137, f_beta: 0.8936170212765958\n",
            "train: step: 345, loss: 0.30463576316833496, acc: 0.8984375, recall: 0.8615384615384616, precision: 0.9333333333333333, f_beta: 0.8960000000000001\n",
            "train: step: 346, loss: 0.19402918219566345, acc: 0.9296875, recall: 0.92, precision: 0.9583333333333334, f_beta: 0.9387755102040817\n",
            "train: step: 347, loss: 0.3125227987766266, acc: 0.8515625, recall: 0.8070175438596491, precision: 0.8518518518518519, f_beta: 0.8288288288288288\n",
            "train: step: 348, loss: 0.19175729155540466, acc: 0.90625, recall: 0.9047619047619048, precision: 0.9047619047619048, f_beta: 0.9047619047619048\n",
            "train: step: 349, loss: 0.2711041569709778, acc: 0.859375, recall: 0.8596491228070176, precision: 0.8305084745762712, f_beta: 0.8448275862068966\n",
            "train: step: 350, loss: 0.26400062441825867, acc: 0.8984375, recall: 0.8783783783783784, precision: 0.9420289855072463, f_beta: 0.9090909090909092\n",
            "train: step: 351, loss: 0.23904924094676971, acc: 0.90625, recall: 0.9508196721311475, precision: 0.8656716417910447, f_beta: 0.9062499999999999\n",
            "train: step: 352, loss: 0.3279186487197876, acc: 0.875, recall: 0.9315068493150684, precision: 0.8607594936708861, f_beta: 0.8947368421052632\n",
            "train: step: 353, loss: 0.31968924403190613, acc: 0.8515625, recall: 0.9636363636363636, precision: 0.7571428571428571, f_beta: 0.848\n",
            "train: step: 354, loss: 0.22876644134521484, acc: 0.90625, recall: 0.9354838709677419, precision: 0.8787878787878788, f_beta: 0.90625\n",
            "train: step: 355, loss: 0.2804299592971802, acc: 0.890625, recall: 0.9393939393939394, precision: 0.8611111111111112, f_beta: 0.8985507246376813\n",
            "train: step: 356, loss: 0.3582494258880615, acc: 0.8515625, recall: 0.8333333333333334, precision: 0.873015873015873, f_beta: 0.8527131782945736\n",
            "train: step: 357, loss: 0.3233356177806854, acc: 0.8515625, recall: 0.8153846153846154, precision: 0.8833333333333333, f_beta: 0.848\n",
            "train: step: 358, loss: 0.27492743730545044, acc: 0.859375, recall: 0.7910447761194029, precision: 0.9298245614035088, f_beta: 0.8548387096774193\n",
            "train: step: 359, loss: 0.23354560136795044, acc: 0.921875, recall: 0.8524590163934426, precision: 0.9811320754716981, f_beta: 0.912280701754386\n",
            "train: step: 360, loss: 0.31683868169784546, acc: 0.890625, recall: 0.8524590163934426, precision: 0.9122807017543859, f_beta: 0.8813559322033898\n",
            "train: step: 361, loss: 0.25285542011260986, acc: 0.9140625, recall: 0.8928571428571429, precision: 0.9090909090909091, f_beta: 0.9009009009009009\n",
            "train: step: 362, loss: 0.23530712723731995, acc: 0.890625, recall: 0.9636363636363636, precision: 0.8153846153846154, f_beta: 0.8833333333333333\n",
            "train: step: 363, loss: 0.2624695599079132, acc: 0.8984375, recall: 0.9552238805970149, precision: 0.8648648648648649, f_beta: 0.9078014184397162\n",
            "train: step: 364, loss: 0.22211189568042755, acc: 0.9453125, recall: 0.9552238805970149, precision: 0.9411764705882353, f_beta: 0.9481481481481482\n",
            "train: step: 365, loss: 0.2571650445461273, acc: 0.8984375, recall: 0.9117647058823529, precision: 0.8985507246376812, f_beta: 0.9051094890510949\n",
            "train: step: 366, loss: 0.27493494749069214, acc: 0.859375, recall: 0.9661016949152542, precision: 0.7808219178082192, f_beta: 0.8636363636363635\n",
            "train: step: 367, loss: 0.25135159492492676, acc: 0.8828125, recall: 0.7962962962962963, precision: 0.9148936170212766, f_beta: 0.8514851485148516\n",
            "train: step: 368, loss: 0.21339531242847443, acc: 0.9296875, recall: 0.9, precision: 0.9473684210526315, f_beta: 0.9230769230769231\n",
            "train: step: 369, loss: 0.2541764974594116, acc: 0.921875, recall: 0.9014084507042254, precision: 0.9552238805970149, f_beta: 0.927536231884058\n",
            "train: step: 370, loss: 0.2836034297943115, acc: 0.8984375, recall: 0.8125, precision: 0.9069767441860465, f_beta: 0.8571428571428572\n",
            "train: step: 371, loss: 0.19458946585655212, acc: 0.9296875, recall: 0.8939393939393939, precision: 0.9672131147540983, f_beta: 0.9291338582677166\n",
            "train: step: 372, loss: 0.35598495602607727, acc: 0.859375, recall: 0.8548387096774194, precision: 0.8548387096774194, f_beta: 0.8548387096774194\n",
            "train: step: 373, loss: 0.268483430147171, acc: 0.8828125, recall: 0.8524590163934426, precision: 0.896551724137931, f_beta: 0.8739495798319327\n",
            "train: step: 374, loss: 0.24559099972248077, acc: 0.890625, recall: 0.9, precision: 0.9, f_beta: 0.9\n",
            "train: step: 375, loss: 0.23354652523994446, acc: 0.90625, recall: 0.9076923076923077, precision: 0.9076923076923077, f_beta: 0.9076923076923076\n",
            "train: step: 376, loss: 0.38381993770599365, acc: 0.8203125, recall: 0.8928571428571429, precision: 0.746268656716418, f_beta: 0.8130081300813009\n",
            "train: step: 377, loss: 0.39665448665618896, acc: 0.8046875, recall: 0.8253968253968254, precision: 0.7878787878787878, f_beta: 0.8062015503875969\n",
            "train: step: 378, loss: 0.34538859128952026, acc: 0.828125, recall: 0.9322033898305084, precision: 0.7534246575342466, f_beta: 0.8333333333333333\n",
            "train: step: 379, loss: 0.3685600459575653, acc: 0.828125, recall: 0.8275862068965517, precision: 0.8, f_beta: 0.8135593220338982\n",
            "train: step: 380, loss: 0.3192359209060669, acc: 0.8671875, recall: 0.8529411764705882, precision: 0.8923076923076924, f_beta: 0.8721804511278195\n",
            "train: step: 381, loss: 0.23347818851470947, acc: 0.90625, recall: 0.8983050847457628, precision: 0.8983050847457628, f_beta: 0.8983050847457628\n",
            "train: step: 382, loss: 0.3050600290298462, acc: 0.921875, recall: 0.9090909090909091, precision: 0.9375, f_beta: 0.923076923076923\n",
            "train: step: 383, loss: 0.26331841945648193, acc: 0.8828125, recall: 0.7704918032786885, precision: 0.9791666666666666, f_beta: 0.8623853211009173\n",
            "train: step: 384, loss: 0.26024866104125977, acc: 0.90625, recall: 0.9393939393939394, precision: 0.8857142857142857, f_beta: 0.9117647058823529\n",
            "train: step: 385, loss: 0.3353103995323181, acc: 0.859375, recall: 0.8636363636363636, precision: 0.8636363636363636, f_beta: 0.8636363636363636\n",
            "train: step: 386, loss: 0.28257864713668823, acc: 0.8671875, recall: 0.8769230769230769, precision: 0.8636363636363636, f_beta: 0.8702290076335878\n",
            "train: step: 387, loss: 0.22308805584907532, acc: 0.9140625, recall: 0.9428571428571428, precision: 0.9041095890410958, f_beta: 0.923076923076923\n",
            "train: step: 388, loss: 0.28063488006591797, acc: 0.8828125, recall: 0.9666666666666667, precision: 0.8169014084507042, f_beta: 0.8854961832061069\n",
            "train: step: 389, loss: 0.34329068660736084, acc: 0.84375, recall: 0.8636363636363636, precision: 0.8382352941176471, f_beta: 0.8507462686567164\n",
            "train: step: 390, loss: 0.2784861922264099, acc: 0.8671875, recall: 0.9295774647887324, precision: 0.8461538461538461, f_beta: 0.8859060402684563\n",
            "train: step: 391, loss: 0.25412434339523315, acc: 0.890625, recall: 0.9180327868852459, precision: 0.8615384615384616, f_beta: 0.8888888888888888\n",
            "train: step: 392, loss: 0.2855321168899536, acc: 0.8828125, recall: 0.8709677419354839, precision: 0.8852459016393442, f_beta: 0.8780487804878049\n",
            "train: step: 393, loss: 0.2834710478782654, acc: 0.890625, recall: 0.8305084745762712, precision: 0.9245283018867925, f_beta: 0.875\n",
            "train: step: 394, loss: 0.40670472383499146, acc: 0.8359375, recall: 0.8026315789473685, precision: 0.9104477611940298, f_beta: 0.8531468531468532\n",
            "train: step: 395, loss: 0.2784770727157593, acc: 0.8828125, recall: 0.8378378378378378, precision: 0.9538461538461539, f_beta: 0.8920863309352518\n",
            "train: step: 396, loss: 0.3219934105873108, acc: 0.8828125, recall: 0.873015873015873, precision: 0.8870967741935484, f_beta: 0.88\n",
            "train: step: 397, loss: 0.2972698211669922, acc: 0.84375, recall: 0.9516129032258065, precision: 0.7763157894736842, f_beta: 0.855072463768116\n",
            "train: step: 398, loss: 0.271660715341568, acc: 0.8984375, recall: 0.9433962264150944, precision: 0.8333333333333334, f_beta: 0.8849557522123894\n",
            "train: step: 399, loss: 0.22005316615104675, acc: 0.90625, recall: 0.9411764705882353, precision: 0.8888888888888888, f_beta: 0.9142857142857143\n",
            "train: step: 400, loss: 0.29648342728614807, acc: 0.8984375, recall: 0.9245283018867925, precision: 0.8448275862068966, f_beta: 0.8828828828828829\n",
            "\n",
            "Evaluation:\n",
            "2019-10-10T08:35:10.985097, step: 400, loss: 0.3396758276682634, acc: 0.8567708333333334,precision: 0.8181753137342341, recall: 0.8898156037793665, f_beta: 0.8514691836090034\n",
            "Saved model checkpoint to ../model/textCNN/model/my-model-400\n",
            "\n",
            "train: step: 401, loss: 0.21491116285324097, acc: 0.9375, recall: 0.9384615384615385, precision: 0.9384615384615385, f_beta: 0.9384615384615385\n",
            "train: step: 402, loss: 0.2969731092453003, acc: 0.890625, recall: 0.8412698412698413, precision: 0.9298245614035088, f_beta: 0.8833333333333334\n",
            "train: step: 403, loss: 0.25417762994766235, acc: 0.890625, recall: 0.835820895522388, precision: 0.9491525423728814, f_beta: 0.888888888888889\n",
            "train: step: 404, loss: 0.27048778533935547, acc: 0.890625, recall: 0.828125, precision: 0.9464285714285714, f_beta: 0.8833333333333333\n",
            "train: step: 405, loss: 0.25948628783226013, acc: 0.8984375, recall: 0.8857142857142857, precision: 0.9253731343283582, f_beta: 0.9051094890510949\n",
            "train: step: 406, loss: 0.20413891971111298, acc: 0.9375, recall: 0.9552238805970149, precision: 0.927536231884058, f_beta: 0.9411764705882353\n",
            "train: step: 407, loss: 0.2715805768966675, acc: 0.8984375, recall: 0.9411764705882353, precision: 0.8767123287671232, f_beta: 0.9078014184397163\n",
            "train: step: 408, loss: 0.2675371766090393, acc: 0.875, recall: 0.9384615384615385, precision: 0.8356164383561644, f_beta: 0.8840579710144928\n",
            "train: step: 409, loss: 0.2514827847480774, acc: 0.8671875, recall: 0.92, precision: 0.8625, f_beta: 0.8903225806451613\n",
            "train: step: 410, loss: 0.4197756052017212, acc: 0.8203125, recall: 0.9692307692307692, precision: 0.75, f_beta: 0.8456375838926173\n",
            "train: step: 411, loss: 0.26956892013549805, acc: 0.8828125, recall: 0.967741935483871, precision: 0.821917808219178, f_beta: 0.8888888888888888\n",
            "train: step: 412, loss: 0.21779994666576385, acc: 0.9296875, recall: 0.9, precision: 0.9692307692307692, f_beta: 0.9333333333333333\n",
            "train: step: 413, loss: 0.2542130649089813, acc: 0.90625, recall: 0.873015873015873, precision: 0.9322033898305084, f_beta: 0.9016393442622951\n",
            "train: step: 414, loss: 0.42433977127075195, acc: 0.84375, recall: 0.78125, precision: 0.8928571428571429, f_beta: 0.8333333333333334\n",
            "train: step: 415, loss: 0.3594156503677368, acc: 0.8359375, recall: 0.7096774193548387, precision: 0.9361702127659575, f_beta: 0.8073394495412844\n",
            "train: step: 416, loss: 0.2511226236820221, acc: 0.8984375, recall: 0.8732394366197183, precision: 0.9393939393939394, f_beta: 0.9051094890510948\n",
            "train: step: 417, loss: 0.2966575026512146, acc: 0.8515625, recall: 0.8253968253968254, precision: 0.8666666666666667, f_beta: 0.8455284552845528\n",
            "train: step: 418, loss: 0.24786396324634552, acc: 0.9140625, recall: 0.9230769230769231, precision: 0.9090909090909091, f_beta: 0.9160305343511451\n",
            "train: step: 419, loss: 0.29590290784835815, acc: 0.8828125, recall: 0.9333333333333333, precision: 0.835820895522388, f_beta: 0.8818897637795275\n",
            "train: step: 420, loss: 0.3280128836631775, acc: 0.828125, recall: 0.9180327868852459, precision: 0.7671232876712328, f_beta: 0.8358208955223879\n",
            "train: step: 421, loss: 0.2260952889919281, acc: 0.8984375, recall: 0.9523809523809523, precision: 0.8571428571428571, f_beta: 0.9022556390977443\n",
            "train: step: 422, loss: 0.27176445722579956, acc: 0.8828125, recall: 0.8823529411764706, precision: 0.8955223880597015, f_beta: 0.888888888888889\n",
            "train: step: 423, loss: 0.30216625332832336, acc: 0.859375, recall: 0.9076923076923077, precision: 0.8309859154929577, f_beta: 0.8676470588235293\n",
            "train: step: 424, loss: 0.23413389921188354, acc: 0.9296875, recall: 0.9, precision: 0.9473684210526315, f_beta: 0.9230769230769231\n",
            "train: step: 425, loss: 0.2469256967306137, acc: 0.8984375, recall: 0.8431372549019608, precision: 0.8958333333333334, f_beta: 0.8686868686868686\n",
            "train: step: 426, loss: 0.24744179844856262, acc: 0.890625, recall: 0.8235294117647058, precision: 0.9655172413793104, f_beta: 0.888888888888889\n",
            "train: step: 427, loss: 0.2484515905380249, acc: 0.90625, recall: 0.8939393939393939, precision: 0.921875, f_beta: 0.9076923076923077\n",
            "train: step: 428, loss: 0.325237512588501, acc: 0.859375, recall: 0.84375, precision: 0.8709677419354839, f_beta: 0.8571428571428571\n",
            "train: step: 429, loss: 0.2897035479545593, acc: 0.8671875, recall: 0.890625, precision: 0.8507462686567164, f_beta: 0.8702290076335878\n",
            "train: step: 430, loss: 0.30512434244155884, acc: 0.8671875, recall: 0.8666666666666667, precision: 0.8524590163934426, f_beta: 0.8595041322314049\n",
            "train: step: 431, loss: 0.25462475419044495, acc: 0.8671875, recall: 0.8888888888888888, precision: 0.8484848484848485, f_beta: 0.8682170542635659\n",
            "train: step: 432, loss: 0.28514206409454346, acc: 0.875, recall: 0.9180327868852459, precision: 0.835820895522388, f_beta: 0.875\n",
            "train: step: 433, loss: 0.2521565556526184, acc: 0.921875, recall: 0.9354838709677419, precision: 0.90625, f_beta: 0.9206349206349206\n",
            "train: step: 434, loss: 0.27480167150497437, acc: 0.8671875, recall: 0.9242424242424242, precision: 0.8356164383561644, f_beta: 0.8776978417266188\n",
            "train: step: 435, loss: 0.24479776620864868, acc: 0.9140625, recall: 0.9230769230769231, precision: 0.9090909090909091, f_beta: 0.9160305343511451\n",
            "train: step: 436, loss: 0.2978563904762268, acc: 0.84375, recall: 0.821917808219178, precision: 0.8955223880597015, f_beta: 0.8571428571428571\n",
            "train: step: 437, loss: 0.22826650738716125, acc: 0.9296875, recall: 0.9523809523809523, precision: 0.9090909090909091, f_beta: 0.9302325581395349\n",
            "train: step: 438, loss: 0.21547159552574158, acc: 0.921875, recall: 0.9137931034482759, precision: 0.9137931034482759, f_beta: 0.9137931034482759\n",
            "train: step: 439, loss: 0.3443872928619385, acc: 0.8359375, recall: 0.7846153846153846, precision: 0.8793103448275862, f_beta: 0.8292682926829268\n",
            "train: step: 440, loss: 0.21566632390022278, acc: 0.90625, recall: 0.8909090909090909, precision: 0.8909090909090909, f_beta: 0.8909090909090909\n",
            "train: step: 441, loss: 0.2688382863998413, acc: 0.875, recall: 0.86, precision: 0.8269230769230769, f_beta: 0.8431372549019608\n",
            "train: step: 442, loss: 0.2696361839771271, acc: 0.8828125, recall: 0.9, precision: 0.8571428571428571, f_beta: 0.8780487804878048\n",
            "train: step: 443, loss: 0.2316780686378479, acc: 0.9140625, recall: 0.9090909090909091, precision: 0.8928571428571429, f_beta: 0.9009009009009009\n",
            "train: step: 444, loss: 0.269852876663208, acc: 0.9140625, recall: 0.8955223880597015, precision: 0.9375, f_beta: 0.9160305343511451\n",
            "train: step: 445, loss: 0.23036721348762512, acc: 0.890625, recall: 0.868421052631579, precision: 0.9428571428571428, f_beta: 0.904109589041096\n",
            "train: step: 446, loss: 0.24633805453777313, acc: 0.890625, recall: 0.8392857142857143, precision: 0.9038461538461539, f_beta: 0.8703703703703703\n",
            "train: step: 447, loss: 0.24322766065597534, acc: 0.921875, recall: 0.95, precision: 0.890625, f_beta: 0.9193548387096774\n",
            "train: step: 448, loss: 0.21430163085460663, acc: 0.9140625, recall: 0.9264705882352942, precision: 0.9130434782608695, f_beta: 0.9197080291970804\n",
            "train: step: 449, loss: 0.2777434289455414, acc: 0.875, recall: 0.8857142857142857, precision: 0.8857142857142857, f_beta: 0.8857142857142857\n",
            "train: step: 450, loss: 0.27320003509521484, acc: 0.8984375, recall: 0.9104477611940298, precision: 0.8970588235294118, f_beta: 0.9037037037037037\n",
            "train: step: 451, loss: 0.3903128504753113, acc: 0.859375, recall: 0.8732394366197183, precision: 0.8732394366197183, f_beta: 0.8732394366197183\n",
            "train: step: 452, loss: 0.24057595431804657, acc: 0.8984375, recall: 0.9107142857142857, precision: 0.864406779661017, f_beta: 0.8869565217391304\n",
            "train: step: 453, loss: 0.29410678148269653, acc: 0.875, recall: 0.890625, precision: 0.8636363636363636, f_beta: 0.8769230769230768\n",
            "train: step: 454, loss: 0.32802534103393555, acc: 0.859375, recall: 0.8923076923076924, precision: 0.8405797101449275, f_beta: 0.8656716417910447\n",
            "train: step: 455, loss: 0.2079029083251953, acc: 0.921875, recall: 0.9180327868852459, precision: 0.9180327868852459, f_beta: 0.9180327868852459\n",
            "train: step: 456, loss: 0.3419642448425293, acc: 0.8671875, recall: 0.8360655737704918, precision: 0.8793103448275862, f_beta: 0.8571428571428572\n",
            "train: step: 457, loss: 0.21563394367694855, acc: 0.8984375, recall: 0.8636363636363636, precision: 0.9344262295081968, f_beta: 0.8976377952755905\n",
            "train: step: 458, loss: 0.29705679416656494, acc: 0.890625, recall: 0.8840579710144928, precision: 0.9104477611940298, f_beta: 0.8970588235294118\n",
            "train: step: 459, loss: 0.27772146463394165, acc: 0.8671875, recall: 0.8412698412698413, precision: 0.8833333333333333, f_beta: 0.8617886178861788\n",
            "train: step: 460, loss: 0.3295493721961975, acc: 0.84375, recall: 0.7931034482758621, precision: 0.8518518518518519, f_beta: 0.8214285714285715\n",
            "train: step: 461, loss: 0.2396305352449417, acc: 0.890625, recall: 0.875, precision: 0.9032258064516129, f_beta: 0.8888888888888888\n",
            "train: step: 462, loss: 0.25227585434913635, acc: 0.921875, recall: 0.9428571428571428, precision: 0.9166666666666666, f_beta: 0.9295774647887323\n",
            "train: step: 463, loss: 0.2212257981300354, acc: 0.9296875, recall: 0.9594594594594594, precision: 0.922077922077922, f_beta: 0.9403973509933775\n",
            "train: step: 464, loss: 0.2916226387023926, acc: 0.890625, recall: 0.9206349206349206, precision: 0.8656716417910447, f_beta: 0.8923076923076922\n",
            "train: step: 465, loss: 0.27450841665267944, acc: 0.8984375, recall: 0.9848484848484849, precision: 0.8441558441558441, f_beta: 0.9090909090909091\n",
            "train: step: 466, loss: 0.2610347270965576, acc: 0.921875, recall: 0.9491525423728814, precision: 0.8888888888888888, f_beta: 0.9180327868852458\n",
            "train: step: 467, loss: 0.25765085220336914, acc: 0.9140625, recall: 0.9666666666666667, precision: 0.8656716417910447, f_beta: 0.9133858267716535\n",
            "train: step: 468, loss: 0.24927079677581787, acc: 0.921875, recall: 0.9210526315789473, precision: 0.9459459459459459, f_beta: 0.9333333333333332\n",
            "start training model\n",
            "train: step: 469, loss: 0.23074115812778473, acc: 0.90625, recall: 0.9032258064516129, precision: 0.9032258064516129, f_beta: 0.9032258064516129\n",
            "train: step: 470, loss: 0.23829147219657898, acc: 0.875, recall: 0.8382352941176471, precision: 0.9193548387096774, f_beta: 0.8769230769230769\n",
            "train: step: 471, loss: 0.24679960310459137, acc: 0.890625, recall: 0.8620689655172413, precision: 0.8928571428571429, f_beta: 0.8771929824561403\n",
            "train: step: 472, loss: 0.29258280992507935, acc: 0.90625, recall: 0.8767123287671232, precision: 0.9552238805970149, f_beta: 0.9142857142857143\n",
            "train: step: 473, loss: 0.20380786061286926, acc: 0.9296875, recall: 0.9516129032258065, precision: 0.9076923076923077, f_beta: 0.9291338582677167\n",
            "train: step: 474, loss: 0.1896415799856186, acc: 0.9296875, recall: 0.9464285714285714, precision: 0.8983050847457628, f_beta: 0.9217391304347826\n",
            "train: step: 475, loss: 0.14393427968025208, acc: 0.9609375, recall: 0.9411764705882353, precision: 0.9846153846153847, f_beta: 0.962406015037594\n",
            "train: step: 476, loss: 0.1590459644794464, acc: 0.9453125, recall: 0.9696969696969697, precision: 0.927536231884058, f_beta: 0.9481481481481481\n",
            "train: step: 477, loss: 0.2577779293060303, acc: 0.890625, recall: 0.8904109589041096, precision: 0.9154929577464789, f_beta: 0.9027777777777778\n",
            "train: step: 478, loss: 0.24282002449035645, acc: 0.90625, recall: 0.9577464788732394, precision: 0.8831168831168831, f_beta: 0.918918918918919\n",
            "train: step: 479, loss: 0.1942960023880005, acc: 0.9375, recall: 0.9523809523809523, precision: 0.9230769230769231, f_beta: 0.9375\n",
            "train: step: 480, loss: 0.18436062335968018, acc: 0.9296875, recall: 0.9516129032258065, precision: 0.9076923076923077, f_beta: 0.9291338582677167\n",
            "train: step: 481, loss: 0.1879655420780182, acc: 0.9296875, recall: 0.9411764705882353, precision: 0.927536231884058, f_beta: 0.9343065693430658\n",
            "train: step: 482, loss: 0.14677995443344116, acc: 0.96875, recall: 0.984375, precision: 0.9545454545454546, f_beta: 0.9692307692307692\n",
            "train: step: 483, loss: 0.14775025844573975, acc: 0.9453125, recall: 0.95, precision: 0.9344262295081968, f_beta: 0.9421487603305784\n",
            "train: step: 484, loss: 0.145573228597641, acc: 0.9609375, recall: 0.9420289855072463, precision: 0.9848484848484849, f_beta: 0.962962962962963\n",
            "train: step: 485, loss: 0.1945050060749054, acc: 0.953125, recall: 0.9264705882352942, precision: 0.984375, f_beta: 0.9545454545454545\n",
            "train: step: 486, loss: 0.16044744849205017, acc: 0.9453125, recall: 0.918918918918919, precision: 0.9855072463768116, f_beta: 0.951048951048951\n",
            "train: step: 487, loss: 0.22085356712341309, acc: 0.890625, recall: 0.9142857142857143, precision: 0.8888888888888888, f_beta: 0.9014084507042254\n",
            "train: step: 488, loss: 0.2074548900127411, acc: 0.9375, recall: 0.9661016949152542, precision: 0.9047619047619048, f_beta: 0.9344262295081968\n",
            "train: step: 489, loss: 0.20516711473464966, acc: 0.890625, recall: 0.9166666666666666, precision: 0.859375, f_beta: 0.8870967741935484\n",
            "train: step: 490, loss: 0.30491816997528076, acc: 0.8828125, recall: 0.8985507246376812, precision: 0.8857142857142857, f_beta: 0.8920863309352518\n",
            "train: step: 491, loss: 0.258113294839859, acc: 0.9140625, recall: 0.9393939393939394, precision: 0.8985507246376812, f_beta: 0.9185185185185185\n",
            "train: step: 492, loss: 0.24288299679756165, acc: 0.890625, recall: 0.9117647058823529, precision: 0.8857142857142857, f_beta: 0.8985507246376812\n",
            "train: step: 493, loss: 0.2213032841682434, acc: 0.90625, recall: 0.9078947368421053, precision: 0.9324324324324325, f_beta: 0.92\n",
            "train: step: 494, loss: 0.1895589828491211, acc: 0.9375, recall: 0.9206349206349206, precision: 0.9508196721311475, f_beta: 0.9354838709677418\n",
            "train: step: 495, loss: 0.27344292402267456, acc: 0.890625, recall: 0.9154929577464789, precision: 0.8904109589041096, f_beta: 0.9027777777777778\n",
            "train: step: 496, loss: 0.24221809208393097, acc: 0.9296875, recall: 0.9833333333333333, precision: 0.8805970149253731, f_beta: 0.9291338582677166\n",
            "train: step: 497, loss: 0.22401320934295654, acc: 0.9140625, recall: 0.9696969696969697, precision: 0.8767123287671232, f_beta: 0.920863309352518\n",
            "train: step: 498, loss: 0.14668339490890503, acc: 0.9765625, recall: 0.9850746268656716, precision: 0.9705882352941176, f_beta: 0.9777777777777777\n",
            "train: step: 499, loss: 0.14185833930969238, acc: 0.9609375, recall: 0.9666666666666667, precision: 0.9508196721311475, f_beta: 0.9586776859504132\n",
            "train: step: 500, loss: 0.19199132919311523, acc: 0.9296875, recall: 0.873015873015873, precision: 0.9821428571428571, f_beta: 0.9243697478991596\n",
            "\n",
            "Evaluation:\n",
            "2019-10-10T08:40:08.820312, step: 500, loss: 0.3273369211416978, acc: 0.8625801282051282,precision: 0.8395018435479347, recall: 0.884702066217637, f_beta: 0.8602700020186279\n",
            "Saved model checkpoint to ../model/textCNN/model/my-model-500\n",
            "\n",
            "train: step: 501, loss: 0.22054056823253632, acc: 0.9296875, recall: 0.9245283018867925, precision: 0.9074074074074074, f_beta: 0.9158878504672898\n",
            "train: step: 502, loss: 0.1655464470386505, acc: 0.953125, recall: 0.9411764705882353, precision: 0.9696969696969697, f_beta: 0.955223880597015\n",
            "train: step: 503, loss: 0.2255549281835556, acc: 0.9140625, recall: 0.9090909090909091, precision: 0.9230769230769231, f_beta: 0.9160305343511451\n",
            "train: step: 504, loss: 0.25928425788879395, acc: 0.9140625, recall: 0.9393939393939394, precision: 0.8985507246376812, f_beta: 0.9185185185185185\n",
            "train: step: 505, loss: 0.22223174571990967, acc: 0.921875, recall: 0.9523809523809523, precision: 0.8955223880597015, f_beta: 0.923076923076923\n",
            "train: step: 506, loss: 0.2524369955062866, acc: 0.921875, recall: 0.9354838709677419, precision: 0.90625, f_beta: 0.9206349206349206\n",
            "train: step: 507, loss: 0.2049969732761383, acc: 0.9296875, recall: 0.9473684210526315, precision: 0.9, f_beta: 0.9230769230769231\n",
            "train: step: 508, loss: 0.19431275129318237, acc: 0.9375, recall: 0.9459459459459459, precision: 0.9459459459459459, f_beta: 0.9459459459459459\n",
            "train: step: 509, loss: 0.18239524960517883, acc: 0.9140625, recall: 0.9375, precision: 0.8955223880597015, f_beta: 0.9160305343511451\n",
            "train: step: 510, loss: 0.25527557730674744, acc: 0.9140625, recall: 0.8823529411764706, precision: 0.9523809523809523, f_beta: 0.916030534351145\n",
            "train: step: 511, loss: 0.2630772888660431, acc: 0.8984375, recall: 0.9032258064516129, precision: 0.8888888888888888, f_beta: 0.8959999999999999\n",
            "train: step: 512, loss: 0.24473896622657776, acc: 0.9140625, recall: 0.9538461538461539, precision: 0.8857142857142857, f_beta: 0.9185185185185185\n",
            "train: step: 513, loss: 0.27886122465133667, acc: 0.890625, recall: 0.8870967741935484, precision: 0.8870967741935484, f_beta: 0.8870967741935484\n",
            "train: step: 514, loss: 0.18088439106941223, acc: 0.9453125, recall: 0.9464285714285714, precision: 0.9298245614035088, f_beta: 0.9380530973451328\n",
            "train: step: 515, loss: 0.2121095061302185, acc: 0.8984375, recall: 0.9137931034482759, precision: 0.8688524590163934, f_beta: 0.8907563025210085\n",
            "train: step: 516, loss: 0.17462697625160217, acc: 0.921875, recall: 0.9, precision: 0.9545454545454546, f_beta: 0.9264705882352942\n",
            "train: step: 517, loss: 0.1992897391319275, acc: 0.9296875, recall: 0.9272727272727272, precision: 0.9107142857142857, f_beta: 0.918918918918919\n",
            "train: step: 518, loss: 0.17250975966453552, acc: 0.9453125, recall: 0.9375, precision: 0.9523809523809523, f_beta: 0.9448818897637795\n",
            "train: step: 519, loss: 0.2198508232831955, acc: 0.9453125, recall: 0.9180327868852459, precision: 0.9655172413793104, f_beta: 0.9411764705882353\n",
            "train: step: 520, loss: 0.17219406366348267, acc: 0.9375, recall: 0.9047619047619048, precision: 0.9661016949152542, f_beta: 0.9344262295081968\n",
            "train: step: 521, loss: 0.15428426861763, acc: 0.953125, recall: 0.9838709677419355, precision: 0.9242424242424242, f_beta: 0.9531249999999999\n",
            "train: step: 522, loss: 0.186586394906044, acc: 0.921875, recall: 0.9152542372881356, precision: 0.9152542372881356, f_beta: 0.9152542372881356\n",
            "train: step: 523, loss: 0.2159866988658905, acc: 0.8984375, recall: 0.90625, precision: 0.8923076923076924, f_beta: 0.8992248062015504\n",
            "train: step: 524, loss: 0.143226757645607, acc: 0.9609375, recall: 1.0, precision: 0.9253731343283582, f_beta: 0.9612403100775194\n",
            "train: step: 525, loss: 0.17715109884738922, acc: 0.9296875, recall: 0.9571428571428572, precision: 0.9178082191780822, f_beta: 0.9370629370629371\n",
            "train: step: 526, loss: 0.19471020996570587, acc: 0.90625, recall: 0.9152542372881356, precision: 0.8852459016393442, f_beta: 0.9\n",
            "train: step: 527, loss: 0.16347567737102509, acc: 0.9765625, recall: 0.9846153846153847, precision: 0.9696969696969697, f_beta: 0.9770992366412214\n",
            "train: step: 528, loss: 0.27422159910202026, acc: 0.875, recall: 0.8767123287671232, precision: 0.9014084507042254, f_beta: 0.8888888888888888\n",
            "train: step: 529, loss: 0.17614464461803436, acc: 0.9453125, recall: 0.96, precision: 0.9056603773584906, f_beta: 0.9320388349514563\n",
            "train: step: 530, loss: 0.23518455028533936, acc: 0.921875, recall: 0.9142857142857143, precision: 0.9411764705882353, f_beta: 0.9275362318840579\n",
            "train: step: 531, loss: 0.18089580535888672, acc: 0.953125, recall: 0.9666666666666667, precision: 0.9354838709677419, f_beta: 0.9508196721311476\n",
            "train: step: 532, loss: 0.17384256422519684, acc: 0.9453125, recall: 0.9, precision: 0.9818181818181818, f_beta: 0.9391304347826087\n",
            "train: step: 533, loss: 0.20537421107292175, acc: 0.921875, recall: 0.9264705882352942, precision: 0.9264705882352942, f_beta: 0.9264705882352942\n",
            "train: step: 534, loss: 0.17784273624420166, acc: 0.9609375, recall: 1.0, precision: 0.9193548387096774, f_beta: 0.9579831932773109\n",
            "train: step: 535, loss: 0.20187871158123016, acc: 0.90625, recall: 0.8923076923076924, precision: 0.9206349206349206, f_beta: 0.90625\n",
            "train: step: 536, loss: 0.15219074487686157, acc: 0.9296875, recall: 0.8913043478260869, precision: 0.9111111111111111, f_beta: 0.9010989010989011\n",
            "train: step: 537, loss: 0.14551228284835815, acc: 0.953125, recall: 0.9705882352941176, precision: 0.9428571428571428, f_beta: 0.9565217391304348\n",
            "train: step: 538, loss: 0.2137618064880371, acc: 0.9375, recall: 0.9354838709677419, precision: 0.9354838709677419, f_beta: 0.9354838709677419\n",
            "train: step: 539, loss: 0.17981365323066711, acc: 0.9296875, recall: 0.9375, precision: 0.9230769230769231, f_beta: 0.9302325581395349\n",
            "train: step: 540, loss: 0.20518027245998383, acc: 0.8984375, recall: 0.8857142857142857, precision: 0.9253731343283582, f_beta: 0.9051094890510949\n",
            "train: step: 541, loss: 0.18604987859725952, acc: 0.90625, recall: 0.8840579710144928, precision: 0.9384615384615385, f_beta: 0.9104477611940298\n",
            "train: step: 542, loss: 0.2471509426832199, acc: 0.890625, recall: 0.9464285714285714, precision: 0.828125, f_beta: 0.8833333333333333\n",
            "train: step: 543, loss: 0.20893600583076477, acc: 0.90625, recall: 0.9230769230769231, precision: 0.8955223880597015, f_beta: 0.9090909090909091\n",
            "train: step: 544, loss: 0.2076161652803421, acc: 0.9140625, recall: 0.95, precision: 0.8769230769230769, f_beta: 0.912\n",
            "train: step: 545, loss: 0.23406502604484558, acc: 0.8828125, recall: 0.875, precision: 0.8888888888888888, f_beta: 0.8818897637795274\n",
            "train: step: 546, loss: 0.2113822102546692, acc: 0.9296875, recall: 0.9076923076923077, precision: 0.9516129032258065, f_beta: 0.9291338582677167\n",
            "train: step: 547, loss: 0.2134341597557068, acc: 0.921875, recall: 0.8709677419354839, precision: 0.9642857142857143, f_beta: 0.9152542372881356\n",
            "train: step: 548, loss: 0.2065366804599762, acc: 0.921875, recall: 0.8939393939393939, precision: 0.9516129032258065, f_beta: 0.921875\n",
            "train: step: 549, loss: 0.1832234263420105, acc: 0.921875, recall: 0.8888888888888888, precision: 0.9230769230769231, f_beta: 0.9056603773584906\n",
            "train: step: 550, loss: 0.22524061799049377, acc: 0.8984375, recall: 0.9285714285714286, precision: 0.8524590163934426, f_beta: 0.888888888888889\n",
            "train: step: 551, loss: 0.24191726744174957, acc: 0.8984375, recall: 0.875, precision: 0.9180327868852459, f_beta: 0.8959999999999999\n",
            "train: step: 552, loss: 0.2186214029788971, acc: 0.9296875, recall: 0.9090909090909091, precision: 0.9523809523809523, f_beta: 0.9302325581395349\n",
            "train: step: 553, loss: 0.26514050364494324, acc: 0.9140625, recall: 0.9253731343283582, precision: 0.9117647058823529, f_beta: 0.9185185185185185\n",
            "train: step: 554, loss: 0.22423237562179565, acc: 0.90625, recall: 0.9696969696969697, precision: 0.8648648648648649, f_beta: 0.9142857142857143\n",
            "train: step: 555, loss: 0.21939116716384888, acc: 0.8984375, recall: 0.8909090909090909, precision: 0.875, f_beta: 0.8828828828828829\n",
            "train: step: 556, loss: 0.15169084072113037, acc: 0.9453125, recall: 1.0, precision: 0.9027777777777778, f_beta: 0.9489051094890512\n",
            "train: step: 557, loss: 0.13623856008052826, acc: 0.9765625, recall: 0.9672131147540983, precision: 0.9833333333333333, f_beta: 0.9752066115702478\n",
            "train: step: 558, loss: 0.3178974986076355, acc: 0.84375, recall: 0.7121212121212122, precision: 0.9791666666666666, f_beta: 0.8245614035087719\n",
            "train: step: 559, loss: 0.1655394285917282, acc: 0.9453125, recall: 0.9019607843137255, precision: 0.9583333333333334, f_beta: 0.9292929292929293\n",
            "train: step: 560, loss: 0.17921225726604462, acc: 0.921875, recall: 0.9090909090909091, precision: 0.9375, f_beta: 0.923076923076923\n",
            "train: step: 561, loss: 0.2097044140100479, acc: 0.890625, recall: 0.9298245614035088, precision: 0.8412698412698413, f_beta: 0.8833333333333334\n",
            "train: step: 562, loss: 0.23134258389472961, acc: 0.921875, recall: 0.8923076923076924, precision: 0.9508196721311475, f_beta: 0.9206349206349206\n",
            "train: step: 563, loss: 0.1686742752790451, acc: 0.921875, recall: 0.9230769230769231, precision: 0.9230769230769231, f_beta: 0.9230769230769231\n",
            "train: step: 564, loss: 0.19327647984027863, acc: 0.9296875, recall: 0.9649122807017544, precision: 0.8870967741935484, f_beta: 0.9243697478991597\n",
            "train: step: 565, loss: 0.2573620676994324, acc: 0.890625, recall: 0.9672131147540983, precision: 0.8309859154929577, f_beta: 0.8939393939393939\n",
            "train: step: 566, loss: 0.190114825963974, acc: 0.9296875, recall: 0.9516129032258065, precision: 0.9076923076923077, f_beta: 0.9291338582677167\n",
            "train: step: 567, loss: 0.19538812339305878, acc: 0.9375, recall: 0.967741935483871, precision: 0.9090909090909091, f_beta: 0.9374999999999999\n",
            "train: step: 568, loss: 0.1382383406162262, acc: 0.953125, recall: 0.9333333333333333, precision: 0.9655172413793104, f_beta: 0.9491525423728815\n",
            "train: step: 569, loss: 0.19012412428855896, acc: 0.9375, recall: 0.9, precision: 0.984375, f_beta: 0.9402985074626866\n",
            "train: step: 570, loss: 0.18618813157081604, acc: 0.9140625, recall: 0.8909090909090909, precision: 0.9074074074074074, f_beta: 0.8990825688073394\n",
            "train: step: 571, loss: 0.2760393023490906, acc: 0.8984375, recall: 0.8636363636363636, precision: 0.9344262295081968, f_beta: 0.8976377952755905\n",
            "train: step: 572, loss: 0.2149379551410675, acc: 0.90625, recall: 0.921875, precision: 0.8939393939393939, f_beta: 0.9076923076923077\n",
            "train: step: 573, loss: 0.1924401819705963, acc: 0.921875, recall: 0.9047619047619048, precision: 0.9344262295081968, f_beta: 0.9193548387096775\n",
            "train: step: 574, loss: 0.1779971718788147, acc: 0.9375, recall: 0.9047619047619048, precision: 0.9661016949152542, f_beta: 0.9344262295081968\n",
            "train: step: 575, loss: 0.22011703252792358, acc: 0.921875, recall: 0.9384615384615385, precision: 0.9104477611940298, f_beta: 0.9242424242424243\n",
            "train: step: 576, loss: 0.1999320387840271, acc: 0.9140625, recall: 0.9324324324324325, precision: 0.92, f_beta: 0.9261744966442953\n",
            "train: step: 577, loss: 0.2571183145046234, acc: 0.90625, recall: 0.921875, precision: 0.8939393939393939, f_beta: 0.9076923076923077\n",
            "train: step: 578, loss: 0.20922735333442688, acc: 0.9375, recall: 0.9565217391304348, precision: 0.9295774647887324, f_beta: 0.9428571428571428\n",
            "train: step: 579, loss: 0.15343385934829712, acc: 0.9609375, recall: 0.9636363636363636, precision: 0.9464285714285714, f_beta: 0.9549549549549549\n",
            "train: step: 580, loss: 0.18481723964214325, acc: 0.9453125, recall: 0.9344262295081968, precision: 0.95, f_beta: 0.9421487603305784\n",
            "train: step: 581, loss: 0.14698638021945953, acc: 0.9609375, recall: 0.9864864864864865, precision: 0.948051948051948, f_beta: 0.9668874172185431\n",
            "train: step: 582, loss: 0.17960798740386963, acc: 0.9140625, recall: 0.8793103448275862, precision: 0.9272727272727272, f_beta: 0.902654867256637\n",
            "train: step: 583, loss: 0.23598752915859222, acc: 0.8984375, recall: 0.9014084507042254, precision: 0.9142857142857143, f_beta: 0.9078014184397163\n",
            "train: step: 584, loss: 0.25545933842658997, acc: 0.9375, recall: 0.9047619047619048, precision: 0.9661016949152542, f_beta: 0.9344262295081968\n",
            "train: step: 585, loss: 0.16965621709823608, acc: 0.90625, recall: 0.9285714285714286, precision: 0.9027777777777778, f_beta: 0.9154929577464788\n",
            "train: step: 586, loss: 0.18546567857265472, acc: 0.953125, recall: 0.9852941176470589, precision: 0.9305555555555556, f_beta: 0.9571428571428572\n",
            "train: step: 587, loss: 0.1844814419746399, acc: 0.9140625, recall: 0.9577464788732394, precision: 0.8947368421052632, f_beta: 0.9251700680272109\n",
            "train: step: 588, loss: 0.20738665759563446, acc: 0.921875, recall: 0.9821428571428571, precision: 0.859375, f_beta: 0.9166666666666665\n",
            "train: step: 589, loss: 0.21702024340629578, acc: 0.90625, recall: 0.9523809523809523, precision: 0.8695652173913043, f_beta: 0.909090909090909\n",
            "train: step: 590, loss: 0.17623937129974365, acc: 0.9375, recall: 0.9393939393939394, precision: 0.9393939393939394, f_beta: 0.9393939393939394\n",
            "train: step: 591, loss: 0.16836614906787872, acc: 0.953125, recall: 0.9411764705882353, precision: 0.9696969696969697, f_beta: 0.955223880597015\n",
            "train: step: 592, loss: 0.1752316951751709, acc: 0.9453125, recall: 0.9552238805970149, precision: 0.9411764705882353, f_beta: 0.9481481481481482\n",
            "train: step: 593, loss: 0.17446668446063995, acc: 0.9296875, recall: 0.8727272727272727, precision: 0.96, f_beta: 0.9142857142857144\n",
            "train: step: 594, loss: 0.20074820518493652, acc: 0.9296875, recall: 0.9230769230769231, precision: 0.9375, f_beta: 0.9302325581395349\n",
            "train: step: 595, loss: 0.21838408708572388, acc: 0.8984375, recall: 0.8484848484848485, precision: 0.9491525423728814, f_beta: 0.896\n",
            "train: step: 596, loss: 0.1973915547132492, acc: 0.921875, recall: 0.9375, precision: 0.9090909090909091, f_beta: 0.923076923076923\n",
            "train: step: 597, loss: 0.24660786986351013, acc: 0.890625, recall: 0.896551724137931, precision: 0.8666666666666667, f_beta: 0.8813559322033899\n",
            "train: step: 598, loss: 0.2482985258102417, acc: 0.890625, recall: 0.8793103448275862, precision: 0.8793103448275862, f_beta: 0.8793103448275863\n",
            "train: step: 599, loss: 0.17416822910308838, acc: 0.9375, recall: 0.9473684210526315, precision: 0.9152542372881356, f_beta: 0.9310344827586206\n",
            "train: step: 600, loss: 0.1749906986951828, acc: 0.9375, recall: 0.9482758620689655, precision: 0.9166666666666666, f_beta: 0.9322033898305084\n",
            "\n",
            "Evaluation:\n",
            "2019-10-10T08:45:06.278532, step: 600, loss: 0.3175067068674626, acc: 0.8653846153846154,precision: 0.8683211804624771, recall: 0.8655416780180217, f_beta: 0.866121937229274\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Saved model checkpoint to ../model/textCNN/model/my-model-600\n",
            "\n",
            "train: step: 601, loss: 0.18419107794761658, acc: 0.9375, recall: 0.9473684210526315, precision: 0.9152542372881356, f_beta: 0.9310344827586206\n",
            "train: step: 602, loss: 0.1419563889503479, acc: 0.9609375, recall: 0.9428571428571428, precision: 0.9850746268656716, f_beta: 0.9635036496350364\n",
            "train: step: 603, loss: 0.23379138112068176, acc: 0.8984375, recall: 0.8548387096774194, precision: 0.9298245614035088, f_beta: 0.8907563025210085\n",
            "train: step: 604, loss: 0.20922406017780304, acc: 0.9140625, recall: 0.8529411764705882, precision: 0.9830508474576272, f_beta: 0.9133858267716536\n",
            "train: step: 605, loss: 0.1933947503566742, acc: 0.9140625, recall: 0.9206349206349206, precision: 0.90625, f_beta: 0.9133858267716536\n",
            "train: step: 606, loss: 0.19036561250686646, acc: 0.9296875, recall: 0.953125, precision: 0.9104477611940298, f_beta: 0.931297709923664\n",
            "train: step: 607, loss: 0.17731694877147675, acc: 0.921875, recall: 0.9710144927536232, precision: 0.8933333333333333, f_beta: 0.9305555555555556\n",
            "train: step: 608, loss: 0.2238379567861557, acc: 0.90625, recall: 0.9571428571428572, precision: 0.881578947368421, f_beta: 0.9178082191780822\n",
            "train: step: 609, loss: 0.21209076046943665, acc: 0.921875, recall: 0.9305555555555556, precision: 0.9305555555555556, f_beta: 0.9305555555555556\n",
            "train: step: 610, loss: 0.21017149090766907, acc: 0.8984375, recall: 0.9142857142857143, precision: 0.9014084507042254, f_beta: 0.9078014184397163\n",
            "train: step: 611, loss: 0.2769894003868103, acc: 0.90625, recall: 0.8840579710144928, precision: 0.9384615384615385, f_beta: 0.9104477611940298\n",
            "train: step: 612, loss: 0.2268790304660797, acc: 0.8984375, recall: 0.921875, precision: 0.8805970149253731, f_beta: 0.9007633587786259\n",
            "train: step: 613, loss: 0.19477050006389618, acc: 0.921875, recall: 0.9428571428571428, precision: 0.9166666666666666, f_beta: 0.9295774647887323\n",
            "train: step: 614, loss: 0.20708468556404114, acc: 0.9296875, recall: 0.9696969696969697, precision: 0.9014084507042254, f_beta: 0.9343065693430657\n",
            "train: step: 615, loss: 0.17145447432994843, acc: 0.9453125, recall: 0.9076923076923077, precision: 0.9833333333333333, f_beta: 0.944\n",
            "train: step: 616, loss: 0.23019324243068695, acc: 0.921875, recall: 0.9180327868852459, precision: 0.9180327868852459, f_beta: 0.9180327868852459\n",
            "train: step: 617, loss: 0.2530948519706726, acc: 0.9296875, recall: 0.9344262295081968, precision: 0.9193548387096774, f_beta: 0.9268292682926829\n",
            "train: step: 618, loss: 0.16443654894828796, acc: 0.9296875, recall: 0.921875, precision: 0.9365079365079365, f_beta: 0.9291338582677166\n",
            "train: step: 619, loss: 0.16985587775707245, acc: 0.9609375, recall: 0.9545454545454546, precision: 0.9692307692307692, f_beta: 0.9618320610687022\n",
            "train: step: 620, loss: 0.22918164730072021, acc: 0.9140625, recall: 0.9259259259259259, precision: 0.8771929824561403, f_beta: 0.9009009009009009\n",
            "train: step: 621, loss: 0.23363837599754333, acc: 0.8828125, recall: 0.9122807017543859, precision: 0.8387096774193549, f_beta: 0.8739495798319329\n",
            "train: step: 622, loss: 0.1763727217912674, acc: 0.921875, recall: 0.9365079365079365, precision: 0.9076923076923077, f_beta: 0.9218749999999999\n",
            "train: step: 623, loss: 0.20499037206172943, acc: 0.9296875, recall: 0.8939393939393939, precision: 0.9672131147540983, f_beta: 0.9291338582677166\n",
            "train: step: 624, loss: 0.20717161893844604, acc: 0.890625, recall: 0.8648648648648649, precision: 0.9411764705882353, f_beta: 0.9014084507042254\n",
            "start training model\n",
            "train: step: 625, loss: 0.13300934433937073, acc: 0.984375, recall: 0.9649122807017544, precision: 1.0, f_beta: 0.9821428571428572\n",
            "train: step: 626, loss: 0.12658441066741943, acc: 0.9765625, recall: 0.953125, precision: 1.0, f_beta: 0.976\n",
            "train: step: 627, loss: 0.15896695852279663, acc: 0.953125, recall: 0.9594594594594594, precision: 0.9594594594594594, f_beta: 0.9594594594594594\n",
            "train: step: 628, loss: 0.15391302108764648, acc: 0.9453125, recall: 0.9193548387096774, precision: 0.9661016949152542, f_beta: 0.9421487603305785\n",
            "train: step: 629, loss: 0.19055916368961334, acc: 0.9375, recall: 0.9552238805970149, precision: 0.927536231884058, f_beta: 0.9411764705882353\n",
            "train: step: 630, loss: 0.12600991129875183, acc: 0.96875, recall: 1.0, precision: 0.9459459459459459, f_beta: 0.9722222222222222\n",
            "train: step: 631, loss: 0.13231512904167175, acc: 0.953125, recall: 0.9833333333333333, precision: 0.921875, f_beta: 0.9516129032258064\n",
            "train: step: 632, loss: 0.1092827320098877, acc: 0.9765625, recall: 1.0, precision: 0.953125, f_beta: 0.976\n",
            "train: step: 633, loss: 0.17881590127944946, acc: 0.9375, recall: 0.9193548387096774, precision: 0.95, f_beta: 0.9344262295081968\n",
            "train: step: 634, loss: 0.12276797741651535, acc: 0.96875, recall: 0.9594594594594594, precision: 0.9861111111111112, f_beta: 0.9726027397260274\n",
            "train: step: 635, loss: 0.13928984105587006, acc: 0.9453125, recall: 0.9047619047619048, precision: 0.9827586206896551, f_beta: 0.9421487603305785\n",
            "train: step: 636, loss: 0.1540387123823166, acc: 0.9375, recall: 0.8955223880597015, precision: 0.9836065573770492, f_beta: 0.9375\n",
            "train: step: 637, loss: 0.1262720227241516, acc: 0.96875, recall: 0.9538461538461539, precision: 0.9841269841269841, f_beta: 0.96875\n",
            "train: step: 638, loss: 0.13534829020500183, acc: 0.96875, recall: 0.9714285714285714, precision: 0.9714285714285714, f_beta: 0.9714285714285714\n",
            "train: step: 639, loss: 0.0828418880701065, acc: 0.9921875, recall: 1.0, precision: 0.9852941176470589, f_beta: 0.9925925925925926\n",
            "train: step: 640, loss: 0.1381310373544693, acc: 0.9453125, recall: 0.9420289855072463, precision: 0.9558823529411765, f_beta: 0.9489051094890512\n",
            "train: step: 641, loss: 0.17185959219932556, acc: 0.9375, recall: 0.9538461538461539, precision: 0.9253731343283582, f_beta: 0.9393939393939394\n",
            "train: step: 642, loss: 0.14809177815914154, acc: 0.9609375, recall: 0.967741935483871, precision: 0.9523809523809523, f_beta: 0.96\n",
            "train: step: 643, loss: 0.11514796316623688, acc: 0.9609375, recall: 0.9523809523809523, precision: 0.967741935483871, f_beta: 0.96\n",
            "train: step: 644, loss: 0.13815242052078247, acc: 0.96875, recall: 0.9846153846153847, precision: 0.9552238805970149, f_beta: 0.9696969696969696\n",
            "train: step: 645, loss: 0.12145645916461945, acc: 0.96875, recall: 0.9375, precision: 1.0, f_beta: 0.967741935483871\n",
            "train: step: 646, loss: 0.12366190552711487, acc: 0.9609375, recall: 0.9655172413793104, precision: 0.9491525423728814, f_beta: 0.9572649572649573\n",
            "train: step: 647, loss: 0.11982553452253342, acc: 0.9609375, recall: 0.9420289855072463, precision: 0.9848484848484849, f_beta: 0.962962962962963\n",
            "train: step: 648, loss: 0.1811484396457672, acc: 0.921875, recall: 0.9538461538461539, precision: 0.8985507246376812, f_beta: 0.9253731343283582\n",
            "train: step: 649, loss: 0.15340231359004974, acc: 0.953125, recall: 0.9666666666666667, precision: 0.9354838709677419, f_beta: 0.9508196721311476\n",
            "train: step: 650, loss: 0.17250922322273254, acc: 0.9296875, recall: 0.9090909090909091, precision: 0.9523809523809523, f_beta: 0.9302325581395349\n",
            "train: step: 651, loss: 0.14716073870658875, acc: 0.96875, recall: 0.9583333333333334, precision: 0.9857142857142858, f_beta: 0.971830985915493\n",
            "train: step: 652, loss: 0.15137842297554016, acc: 0.9609375, recall: 0.9642857142857143, precision: 0.9473684210526315, f_beta: 0.9557522123893805\n",
            "train: step: 653, loss: 0.12335450947284698, acc: 0.9609375, recall: 0.9516129032258065, precision: 0.9672131147540983, f_beta: 0.959349593495935\n",
            "train: step: 654, loss: 0.22956334054470062, acc: 0.9140625, recall: 0.9047619047619048, precision: 0.9193548387096774, f_beta: 0.912\n",
            "train: step: 655, loss: 0.2003163993358612, acc: 0.953125, recall: 0.9607843137254902, precision: 0.9245283018867925, f_beta: 0.9423076923076923\n",
            "train: step: 656, loss: 0.1492641121149063, acc: 0.96875, recall: 0.9859154929577465, precision: 0.958904109589041, f_beta: 0.9722222222222222\n",
            "train: step: 657, loss: 0.11153728514909744, acc: 0.953125, recall: 0.9516129032258065, precision: 0.9516129032258065, f_beta: 0.9516129032258065\n",
            "train: step: 658, loss: 0.16542363166809082, acc: 0.9453125, recall: 0.9411764705882353, precision: 0.9552238805970149, f_beta: 0.9481481481481482\n",
            "train: step: 659, loss: 0.13602854311466217, acc: 0.9609375, recall: 0.9672131147540983, precision: 0.9516129032258065, f_beta: 0.959349593495935\n",
            "train: step: 660, loss: 0.12832918763160706, acc: 0.9453125, recall: 0.9615384615384616, precision: 0.9090909090909091, f_beta: 0.9345794392523366\n",
            "train: step: 661, loss: 0.19025419652462006, acc: 0.9296875, recall: 0.9152542372881356, precision: 0.9310344827586207, f_beta: 0.923076923076923\n",
            "train: step: 662, loss: 0.1340312659740448, acc: 0.9375, recall: 0.9016393442622951, precision: 0.9649122807017544, f_beta: 0.9322033898305084\n",
            "train: step: 663, loss: 0.14466208219528198, acc: 0.953125, recall: 0.9523809523809523, precision: 0.9523809523809523, f_beta: 0.9523809523809523\n",
            "train: step: 664, loss: 0.12600299715995789, acc: 0.9453125, recall: 0.9692307692307692, precision: 0.9264705882352942, f_beta: 0.9473684210526316\n",
            "train: step: 665, loss: 0.19355988502502441, acc: 0.9453125, recall: 0.94, precision: 0.9215686274509803, f_beta: 0.9306930693069307\n",
            "train: step: 666, loss: 0.17034783959388733, acc: 0.9375, recall: 0.9428571428571428, precision: 0.9428571428571428, f_beta: 0.9428571428571428\n",
            "train: step: 667, loss: 0.1752941906452179, acc: 0.921875, recall: 0.9206349206349206, precision: 0.9206349206349206, f_beta: 0.9206349206349206\n",
            "train: step: 668, loss: 0.12785682082176208, acc: 0.9609375, recall: 0.9682539682539683, precision: 0.953125, f_beta: 0.9606299212598425\n",
            "train: step: 669, loss: 0.18123596906661987, acc: 0.921875, recall: 0.8923076923076924, precision: 0.9508196721311475, f_beta: 0.9206349206349206\n",
            "train: step: 670, loss: 0.13597877323627472, acc: 0.953125, recall: 0.9506172839506173, precision: 0.9746835443037974, f_beta: 0.9625\n",
            "train: step: 671, loss: 0.10784700512886047, acc: 0.96875, recall: 0.9841269841269841, precision: 0.9538461538461539, f_beta: 0.96875\n",
            "train: step: 672, loss: 0.10257293283939362, acc: 0.9765625, recall: 0.9661016949152542, precision: 0.9827586206896551, f_beta: 0.9743589743589743\n",
            "train: step: 673, loss: 0.12458999454975128, acc: 0.953125, recall: 0.9649122807017544, precision: 0.9322033898305084, f_beta: 0.9482758620689654\n",
            "train: step: 674, loss: 0.1614997833967209, acc: 0.953125, recall: 0.9411764705882353, precision: 0.9696969696969697, f_beta: 0.955223880597015\n",
            "train: step: 675, loss: 0.15302667021751404, acc: 0.9453125, recall: 0.9583333333333334, precision: 0.9452054794520548, f_beta: 0.9517241379310345\n",
            "train: step: 676, loss: 0.16234071552753448, acc: 0.9453125, recall: 0.9322033898305084, precision: 0.9482758620689655, f_beta: 0.94017094017094\n",
            "train: step: 677, loss: 0.1918291300535202, acc: 0.9375, recall: 0.9508196721311475, precision: 0.9206349206349206, f_beta: 0.9354838709677418\n",
            "train: step: 678, loss: 0.11930515617132187, acc: 0.9609375, recall: 0.9710144927536232, precision: 0.9571428571428572, f_beta: 0.9640287769784173\n",
            "train: step: 679, loss: 0.17118306457996368, acc: 0.9296875, recall: 0.9310344827586207, precision: 0.9152542372881356, f_beta: 0.923076923076923\n",
            "train: step: 680, loss: 0.12966947257518768, acc: 0.9609375, recall: 0.9508196721311475, precision: 0.9666666666666667, f_beta: 0.9586776859504132\n",
            "train: step: 681, loss: 0.16659042239189148, acc: 0.9453125, recall: 0.9298245614035088, precision: 0.9464285714285714, f_beta: 0.9380530973451328\n",
            "train: step: 682, loss: 0.17079029977321625, acc: 0.9453125, recall: 0.9482758620689655, precision: 0.9322033898305084, f_beta: 0.94017094017094\n",
            "train: step: 683, loss: 0.16981926560401917, acc: 0.9375, recall: 0.8970588235294118, precision: 0.9838709677419355, f_beta: 0.9384615384615386\n",
            "train: step: 684, loss: 0.14339439570903778, acc: 0.96875, recall: 0.967741935483871, precision: 0.967741935483871, f_beta: 0.967741935483871\n",
            "train: step: 685, loss: 0.11698450148105621, acc: 0.984375, recall: 1.0, precision: 0.9726027397260274, f_beta: 0.9861111111111112\n",
            "train: step: 686, loss: 0.12883280217647552, acc: 0.9609375, recall: 0.984375, precision: 0.9402985074626866, f_beta: 0.9618320610687023\n",
            "train: step: 687, loss: 0.16225609183311462, acc: 0.96875, recall: 0.9855072463768116, precision: 0.9577464788732394, f_beta: 0.9714285714285714\n",
            "train: step: 688, loss: 0.13890224695205688, acc: 0.953125, recall: 0.9824561403508771, precision: 0.9180327868852459, f_beta: 0.9491525423728813\n",
            "train: step: 689, loss: 0.09831561893224716, acc: 0.9765625, recall: 0.9818181818181818, precision: 0.9642857142857143, f_beta: 0.972972972972973\n",
            "train: step: 690, loss: 0.14047680795192719, acc: 0.9609375, recall: 0.9482758620689655, precision: 0.9649122807017544, f_beta: 0.9565217391304347\n",
            "train: step: 691, loss: 0.10474522411823273, acc: 0.9609375, recall: 0.9365079365079365, precision: 0.9833333333333333, f_beta: 0.9593495934959351\n",
            "train: step: 692, loss: 0.16852852702140808, acc: 0.921875, recall: 0.9122807017543859, precision: 0.9122807017543859, f_beta: 0.9122807017543859\n",
            "train: step: 693, loss: 0.14293064177036285, acc: 0.953125, recall: 0.9178082191780822, precision: 1.0, f_beta: 0.9571428571428571\n",
            "train: step: 694, loss: 0.15492065250873566, acc: 0.9609375, recall: 0.9571428571428572, precision: 0.9710144927536232, f_beta: 0.9640287769784173\n",
            "train: step: 695, loss: 0.16750988364219666, acc: 0.96875, recall: 0.9838709677419355, precision: 0.953125, f_beta: 0.9682539682539683\n",
            "train: step: 696, loss: 0.12989816069602966, acc: 0.984375, recall: 0.9821428571428571, precision: 0.9821428571428571, f_beta: 0.9821428571428571\n",
            "train: step: 697, loss: 0.13031160831451416, acc: 0.9609375, recall: 0.9841269841269841, precision: 0.9393939393939394, f_beta: 0.9612403100775193\n",
            "train: step: 698, loss: 0.10019859671592712, acc: 0.9765625, recall: 0.9859154929577465, precision: 0.9722222222222222, f_beta: 0.979020979020979\n",
            "train: step: 699, loss: 0.1073717400431633, acc: 0.9765625, recall: 0.9855072463768116, precision: 0.9714285714285714, f_beta: 0.9784172661870504\n",
            "train: step: 700, loss: 0.17967413365840912, acc: 0.9375, recall: 0.9672131147540983, precision: 0.9076923076923077, f_beta: 0.9365079365079365\n",
            "\n",
            "Evaluation:\n",
            "2019-10-10T08:50:03.753529, step: 700, loss: 0.32307152182627946, acc: 0.8617788461538461,precision: 0.8975520084277682, recall: 0.8403614457376696, f_beta: 0.8670800723151487\n",
            "Saved model checkpoint to ../model/textCNN/model/my-model-700\n",
            "\n",
            "train: step: 701, loss: 0.11712450534105301, acc: 0.9609375, recall: 0.96875, precision: 0.9538461538461539, f_beta: 0.9612403100775193\n",
            "train: step: 702, loss: 0.15545395016670227, acc: 0.984375, recall: 0.984375, precision: 0.984375, f_beta: 0.984375\n",
            "train: step: 703, loss: 0.14670130610466003, acc: 0.9609375, recall: 0.9830508474576272, precision: 0.9354838709677419, f_beta: 0.9586776859504132\n",
            "train: step: 704, loss: 0.09775807708501816, acc: 0.9765625, recall: 0.9696969696969697, precision: 0.9846153846153847, f_beta: 0.9770992366412214\n",
            "train: step: 705, loss: 0.11696253716945648, acc: 0.96875, recall: 0.9365079365079365, precision: 1.0, f_beta: 0.9672131147540983\n",
            "train: step: 706, loss: 0.1862155795097351, acc: 0.9140625, recall: 0.8620689655172413, precision: 0.9433962264150944, f_beta: 0.9009009009009009\n",
            "train: step: 707, loss: 0.13428756594657898, acc: 0.9609375, recall: 0.9411764705882353, precision: 0.9846153846153847, f_beta: 0.962406015037594\n",
            "train: step: 708, loss: 0.13687747716903687, acc: 0.953125, recall: 0.9354838709677419, precision: 0.9666666666666667, f_beta: 0.9508196721311476\n",
            "train: step: 709, loss: 0.13455721735954285, acc: 0.9609375, recall: 0.9423076923076923, precision: 0.9607843137254902, f_beta: 0.9514563106796117\n",
            "train: step: 710, loss: 0.16498436033725739, acc: 0.9296875, recall: 0.9166666666666666, precision: 0.9565217391304348, f_beta: 0.9361702127659574\n",
            "train: step: 711, loss: 0.1724579781293869, acc: 0.90625, recall: 0.9857142857142858, precision: 0.8625, f_beta: 0.9200000000000002\n",
            "train: step: 712, loss: 0.17301073670387268, acc: 0.9140625, recall: 0.9850746268656716, precision: 0.868421052631579, f_beta: 0.923076923076923\n",
            "train: step: 713, loss: 0.14424708485603333, acc: 0.9453125, recall: 0.9333333333333333, precision: 0.9491525423728814, f_beta: 0.9411764705882353\n",
            "train: step: 714, loss: 0.20237372815608978, acc: 0.9296875, recall: 0.9166666666666666, precision: 0.9565217391304348, f_beta: 0.9361702127659574\n",
            "train: step: 715, loss: 0.14781197905540466, acc: 0.953125, recall: 0.9552238805970149, precision: 0.9552238805970149, f_beta: 0.9552238805970149\n",
            "train: step: 716, loss: 0.1279703676700592, acc: 0.9609375, recall: 0.9701492537313433, precision: 0.9558823529411765, f_beta: 0.962962962962963\n",
            "train: step: 717, loss: 0.17248955368995667, acc: 0.9375, recall: 0.9090909090909091, precision: 0.967741935483871, f_beta: 0.9374999999999999\n",
            "train: step: 718, loss: 0.154596745967865, acc: 0.953125, recall: 0.9473684210526315, precision: 0.9473684210526315, f_beta: 0.9473684210526315\n",
            "train: step: 719, loss: 0.16114306449890137, acc: 0.9453125, recall: 0.9253731343283582, precision: 0.96875, f_beta: 0.9465648854961832\n",
            "train: step: 720, loss: 0.1583726555109024, acc: 0.953125, recall: 0.9577464788732394, precision: 0.9577464788732394, f_beta: 0.9577464788732394\n",
            "train: step: 721, loss: 0.1178327202796936, acc: 0.96875, recall: 0.9701492537313433, precision: 0.9701492537313433, f_beta: 0.9701492537313433\n",
            "train: step: 722, loss: 0.18369895219802856, acc: 0.9375, recall: 0.9452054794520548, precision: 0.9452054794520548, f_beta: 0.9452054794520548\n",
            "train: step: 723, loss: 0.14228348433971405, acc: 0.953125, recall: 0.967741935483871, precision: 0.9375, f_beta: 0.9523809523809523\n",
            "train: step: 724, loss: 0.16639381647109985, acc: 0.9453125, recall: 0.9642857142857143, precision: 0.9152542372881356, f_beta: 0.9391304347826087\n",
            "train: step: 725, loss: 0.18813568353652954, acc: 0.9375, recall: 0.8928571428571429, precision: 0.9615384615384616, f_beta: 0.9259259259259259\n",
            "train: step: 726, loss: 0.1829095482826233, acc: 0.9375, recall: 0.9014084507042254, precision: 0.9846153846153847, f_beta: 0.9411764705882353\n",
            "train: step: 727, loss: 0.11009025573730469, acc: 0.984375, recall: 0.9836065573770492, precision: 0.9836065573770492, f_beta: 0.9836065573770492\n",
            "train: step: 728, loss: 0.134547621011734, acc: 0.9609375, recall: 0.9583333333333334, precision: 0.971830985915493, f_beta: 0.965034965034965\n",
            "train: step: 729, loss: 0.10908372700214386, acc: 0.9609375, recall: 0.927536231884058, precision: 1.0, f_beta: 0.9624060150375939\n",
            "train: step: 730, loss: 0.14487960934638977, acc: 0.953125, recall: 0.9861111111111112, precision: 0.9342105263157895, f_beta: 0.9594594594594595\n",
            "train: step: 731, loss: 0.1392131894826889, acc: 0.96875, recall: 0.9824561403508771, precision: 0.9491525423728814, f_beta: 0.9655172413793103\n",
            "train: step: 732, loss: 0.1356046199798584, acc: 0.9375, recall: 0.9682539682539683, precision: 0.9104477611940298, f_beta: 0.9384615384615386\n",
            "train: step: 733, loss: 0.10971806943416595, acc: 0.96875, recall: 1.0, precision: 0.9365079365079365, f_beta: 0.9672131147540983\n",
            "train: step: 734, loss: 0.16223520040512085, acc: 0.921875, recall: 0.9710144927536232, precision: 0.8933333333333333, f_beta: 0.9305555555555556\n",
            "train: step: 735, loss: 0.1572529673576355, acc: 0.9609375, recall: 0.9821428571428571, precision: 0.9322033898305084, f_beta: 0.9565217391304348\n",
            "train: step: 736, loss: 0.1518433690071106, acc: 0.9453125, recall: 0.9827586206896551, precision: 0.9047619047619048, f_beta: 0.9421487603305785\n",
            "train: step: 737, loss: 0.15601536631584167, acc: 0.9453125, recall: 0.8771929824561403, precision: 1.0, f_beta: 0.9345794392523363\n",
            "train: step: 738, loss: 0.13696616888046265, acc: 0.96875, recall: 0.9672131147540983, precision: 0.9672131147540983, f_beta: 0.9672131147540983\n",
            "train: step: 739, loss: 0.15535876154899597, acc: 0.953125, recall: 0.9295774647887324, precision: 0.9850746268656716, f_beta: 0.9565217391304348\n",
            "train: step: 740, loss: 0.14536504447460175, acc: 0.953125, recall: 0.9466666666666667, precision: 0.9726027397260274, f_beta: 0.9594594594594594\n",
            "train: step: 741, loss: 0.2430250197649002, acc: 0.9140625, recall: 0.9322033898305084, precision: 0.8870967741935484, f_beta: 0.9090909090909092\n",
            "train: step: 742, loss: 0.13686972856521606, acc: 0.9609375, recall: 0.9861111111111112, precision: 0.9466666666666667, f_beta: 0.9659863945578231\n",
            "train: step: 743, loss: 0.15765637159347534, acc: 0.9453125, recall: 0.9722222222222222, precision: 0.9333333333333333, f_beta: 0.9523809523809524\n",
            "train: step: 744, loss: 0.15279726684093475, acc: 0.9609375, recall: 0.9852941176470589, precision: 0.9436619718309859, f_beta: 0.9640287769784172\n",
            "train: step: 745, loss: 0.1829613447189331, acc: 0.921875, recall: 0.96875, precision: 0.8857142857142857, f_beta: 0.9253731343283582\n",
            "train: step: 746, loss: 0.1087905541062355, acc: 0.984375, recall: 0.971830985915493, precision: 1.0, f_beta: 0.9857142857142858\n",
            "train: step: 747, loss: 0.2084476351737976, acc: 0.9140625, recall: 0.9076923076923077, precision: 0.921875, f_beta: 0.9147286821705427\n",
            "train: step: 748, loss: 0.1111341193318367, acc: 0.9765625, recall: 0.9807692307692307, precision: 0.9622641509433962, f_beta: 0.9714285714285713\n",
            "train: step: 749, loss: 0.15883147716522217, acc: 0.9453125, recall: 0.9047619047619048, precision: 0.9827586206896551, f_beta: 0.9421487603305785\n",
            "train: step: 750, loss: 0.13911129534244537, acc: 0.9453125, recall: 0.921875, precision: 0.9672131147540983, f_beta: 0.944\n",
            "train: step: 751, loss: 0.14881828427314758, acc: 0.9296875, recall: 0.9117647058823529, precision: 0.9538461538461539, f_beta: 0.9323308270676691\n",
            "train: step: 752, loss: 0.14953453838825226, acc: 0.9609375, recall: 0.9701492537313433, precision: 0.9558823529411765, f_beta: 0.962962962962963\n",
            "train: step: 753, loss: 0.12661509215831757, acc: 0.953125, recall: 0.9661016949152542, precision: 0.9344262295081968, f_beta: 0.95\n",
            "train: step: 754, loss: 0.12812280654907227, acc: 0.9453125, recall: 0.9672131147540983, precision: 0.921875, f_beta: 0.944\n",
            "train: step: 755, loss: 0.14062902331352234, acc: 0.9296875, recall: 0.9692307692307692, precision: 0.9, f_beta: 0.9333333333333333\n",
            "train: step: 756, loss: 0.13425539433956146, acc: 0.96875, recall: 0.9833333333333333, precision: 0.9516129032258065, f_beta: 0.9672131147540983\n",
            "train: step: 757, loss: 0.12822702527046204, acc: 0.96875, recall: 0.9310344827586207, precision: 1.0, f_beta: 0.9642857142857143\n",
            "train: step: 758, loss: 0.17291098833084106, acc: 0.953125, recall: 0.9714285714285714, precision: 0.9444444444444444, f_beta: 0.9577464788732395\n",
            "train: step: 759, loss: 0.1802881360054016, acc: 0.9609375, recall: 0.9523809523809523, precision: 0.967741935483871, f_beta: 0.96\n",
            "train: step: 760, loss: 0.11435696482658386, acc: 0.96875, recall: 0.953125, precision: 0.9838709677419355, f_beta: 0.9682539682539683\n",
            "train: step: 761, loss: 0.1575973927974701, acc: 0.921875, recall: 0.9354838709677419, precision: 0.90625, f_beta: 0.9206349206349206\n",
            "train: step: 762, loss: 0.08651784062385559, acc: 0.984375, recall: 0.9696969696969697, precision: 1.0, f_beta: 0.9846153846153847\n",
            "train: step: 763, loss: 0.13250771164894104, acc: 0.9453125, recall: 0.9649122807017544, precision: 0.9166666666666666, f_beta: 0.9401709401709402\n",
            "train: step: 764, loss: 0.12212023884057999, acc: 0.9609375, recall: 0.953125, precision: 0.9682539682539683, f_beta: 0.9606299212598425\n",
            "train: step: 765, loss: 0.14915575087070465, acc: 0.9453125, recall: 0.9402985074626866, precision: 0.9545454545454546, f_beta: 0.9473684210526316\n",
            "train: step: 766, loss: 0.17681129276752472, acc: 0.9453125, recall: 0.9272727272727272, precision: 0.9444444444444444, f_beta: 0.9357798165137615\n",
            "train: step: 767, loss: 0.11690472066402435, acc: 0.96875, recall: 0.9672131147540983, precision: 0.9672131147540983, f_beta: 0.9672131147540983\n",
            "train: step: 768, loss: 0.11570096760988235, acc: 0.9765625, recall: 0.9850746268656716, precision: 0.9705882352941176, f_beta: 0.9777777777777777\n",
            "train: step: 769, loss: 0.14870047569274902, acc: 0.9609375, recall: 0.9846153846153847, precision: 0.9411764705882353, f_beta: 0.962406015037594\n",
            "train: step: 770, loss: 0.14735117554664612, acc: 0.9375, recall: 0.95, precision: 0.9193548387096774, f_beta: 0.9344262295081968\n",
            "train: step: 771, loss: 0.1270381212234497, acc: 0.9609375, recall: 0.9375, precision: 0.9574468085106383, f_beta: 0.9473684210526315\n",
            "train: step: 772, loss: 0.12651973962783813, acc: 0.953125, recall: 0.9166666666666666, precision: 0.9821428571428571, f_beta: 0.9482758620689654\n",
            "train: step: 773, loss: 0.12556719779968262, acc: 0.9453125, recall: 0.9032258064516129, precision: 0.9824561403508771, f_beta: 0.9411764705882352\n",
            "train: step: 774, loss: 0.11939804255962372, acc: 0.9609375, recall: 0.9411764705882353, precision: 0.9846153846153847, f_beta: 0.962406015037594\n",
            "train: step: 775, loss: 0.1478259563446045, acc: 0.953125, recall: 0.96875, precision: 0.9393939393939394, f_beta: 0.9538461538461539\n",
            "train: step: 776, loss: 0.11461691558361053, acc: 0.96875, recall: 0.9682539682539683, precision: 0.9682539682539683, f_beta: 0.9682539682539683\n",
            "train: step: 777, loss: 0.13922114670276642, acc: 0.9453125, recall: 0.9726027397260274, precision: 0.9342105263157895, f_beta: 0.9530201342281879\n",
            "train: step: 778, loss: 0.1770910918712616, acc: 0.9296875, recall: 0.9420289855072463, precision: 0.9285714285714286, f_beta: 0.935251798561151\n",
            "train: step: 779, loss: 0.11406563222408295, acc: 0.953125, recall: 0.9565217391304348, precision: 0.9565217391304348, f_beta: 0.9565217391304348\n",
            "train: step: 780, loss: 0.1408107876777649, acc: 0.9375, recall: 0.9482758620689655, precision: 0.9166666666666666, f_beta: 0.9322033898305084\n",
            "WARNING:tensorflow:From <ipython-input-25-f17fdbda3268>:146: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "WARNING:tensorflow:From <ipython-input-25-f17fdbda3268>:155: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Pass your op to the equivalent parameter main_op instead.\n",
            "INFO:tensorflow:No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: ../model/textCNN/savedModel/saved_model.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGpUN1raR2oe",
        "colab_type": "code",
        "colab": {},
        "outputId": "30b0c543-5505-41ae-c913-5e389935d55c"
      },
      "source": [
        "x = \"this movie is full of references like mad max ii the wild one and many others the ladybug´s face it´s a clear reference or tribute to peter lorre this movie is a masterpiece we´ll talk much more about in the future\"\n",
        "\n",
        "# 注：下面两个词典要保证和当前加载的模型对应的词典是一致的\n",
        "with open(mypath/\"data/wordJson/word2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    word2idx = json.load(f)\n",
        "        \n",
        "with open(mypath/\"data/wordJson/label2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    label2idx = json.load(f)\n",
        "idx2label = {value: key for key, value in label2idx.items()}\n",
        "    \n",
        "xIds = [word2idx.get(item, word2idx[\"UNK\"]) for item in x.split(\" \")]\n",
        "if len(xIds) >= config.sequenceLength:\n",
        "    xIds = xIds[:config.sequenceLength]\n",
        "else:\n",
        "    xIds = xIds + [word2idx[\"PAD\"]] * (config.sequenceLength - len(xIds))\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
        "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False, gpu_options=gpu_options)\n",
        "    sess = tf.Session(config=session_conf)\n",
        "\n",
        "    with sess.as_default():\n",
        "        checkpoint_file = tf.train.latest_checkpoint(mypath/\"model/textCNN/model/\")\n",
        "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
        "        saver.restore(sess, checkpoint_file)\n",
        "\n",
        "        # 获得需要喂给模型的参数，输出的结果依赖的输入值\n",
        "        inputX = graph.get_operation_by_name(\"inputX\").outputs[0]\n",
        "        dropoutKeepProb = graph.get_operation_by_name(\"dropoutKeepProb\").outputs[0]\n",
        "\n",
        "        # 获得输出的结果\n",
        "        predictions = graph.get_tensor_by_name(\"output/predictions:0\")\n",
        "\n",
        "        pred = sess.run(predictions, feed_dict={inputX: [xIds], dropoutKeepProb: 1.0})[0]\n",
        "        \n",
        "pred = [idx2label[item] for item in pred]     \n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ../model/textCNN/model/my-model-700\n",
            "['1']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}