{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "adversarialLSTM.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2EcIbtJZgrz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "63608c10-09da-4287-91c7-bc2b55934cd0"
      },
      "source": [
        "!git clone https://github.com/ljyslyc/comment-analysis.git\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'comment-analysis' already exists and is not an empty directory.\n",
            "Collecting Path\n",
            "\u001b[31m  ERROR: Could not find a version that satisfies the requirement Path (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for Path\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdjIBMRVXWg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import json\n",
        "import threading\n",
        "\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from math import sqrt\n",
        "\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
        "from pathlib import Path\n",
        "mypath = Path(\"/content/comment-analysis/textClassifier\")\n",
        "# mypath = Path(\"../\")\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LskS_-8pXWg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 配置参数\n",
        "\n",
        "class TrainingConfig(object):\n",
        "    epoches = 5\n",
        "    evaluateEvery = 100\n",
        "    checkpointEvery = 100\n",
        "    learningRate = 0.001\n",
        "    \n",
        "class ModelConfig(object):\n",
        "    embeddingSize = 200\n",
        "    \n",
        "    hiddenSizes = 128  # LSTM结构的神经元个数\n",
        "    \n",
        "    dropoutKeepProb = 0.5\n",
        "    l2RegLambda = 0.0\n",
        "    epsilon = 5\n",
        "    \n",
        "class Config(object):\n",
        "    sequenceLength = 200  # 取了所有序列长度的均值\n",
        "    batchSize = 128\n",
        "    \n",
        "    dataSource = mypath/\"data/preProcess/labeledTrain.csv\"\n",
        "    \n",
        "    stopWordSource = mypath/\"data/english\"\n",
        "    \n",
        "    numClasses = 1  # 二分类设置为1，多分类设置为类别的数目\n",
        "    \n",
        "    rate = 0.8  # 训练集的比例\n",
        "    \n",
        "    training = TrainingConfig()\n",
        "    \n",
        "    model = ModelConfig()\n",
        "\n",
        "    \n",
        "# 实例化配置参数对象\n",
        "config = Config()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbE54lMZXWhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 数据预处理的类，生成训练集和测试集\n",
        "\n",
        "class Dataset(object):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self._dataSource = config.dataSource\n",
        "        self._stopWordSource = config.stopWordSource  \n",
        "        \n",
        "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
        "        self._embeddingSize = config.model.embeddingSize\n",
        "        self._batchSize = config.batchSize\n",
        "        self._rate = config.rate\n",
        "        \n",
        "        self._stopWordDict = {}\n",
        "        \n",
        "        self.trainReviews = []\n",
        "        self.trainLabels = []\n",
        "        \n",
        "        self.evalReviews = []\n",
        "        self.evalLabels = []\n",
        "        \n",
        "        self.wordEmbedding =None\n",
        "        self.indexFreqs = []  # 统计词空间中的词在出现在多少个review中\n",
        "        \n",
        "        self.labelList = []\n",
        "        \n",
        "    def _readData(self, filePath):\n",
        "        \"\"\"\n",
        "        从csv文件中读取数据集\n",
        "        \"\"\"\n",
        "        \n",
        "        df = pd.read_csv(filePath)\n",
        "        \n",
        "        if self.config.numClasses == 1:\n",
        "            labels = df[\"sentiment\"].tolist()\n",
        "        elif self.config.numClasses > 1:\n",
        "            labels = df[\"rate\"].tolist()\n",
        "            \n",
        "        review = df[\"review\"].tolist()\n",
        "        reviews = [line.strip().split() for line in review]\n",
        "\n",
        "        return reviews, labels\n",
        "    \n",
        "    def _labelToIndex(self, labels, label2idx):\n",
        "        \"\"\"\n",
        "        将标签转换成索引表示\n",
        "        \"\"\"\n",
        "        labelIds = [label2idx[label] for label in labels]\n",
        "        return labelIds\n",
        "    \n",
        "    def _wordToIndex(self, reviews, word2idx):\n",
        "        \"\"\"\n",
        "        将词转换成索引\n",
        "        \"\"\"\n",
        "        reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
        "        return reviewIds\n",
        "        \n",
        "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
        "        \"\"\"\n",
        "        生成训练集和验证集\n",
        "        \"\"\"\n",
        "        reviews = []\n",
        "        for review in x:\n",
        "            if len(review) >= self._sequenceLength:\n",
        "                reviews.append(review[:self._sequenceLength])\n",
        "            else:\n",
        "                reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
        "            \n",
        "        trainIndex = int(len(x) * rate)\n",
        "        \n",
        "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
        "        trainLabels = np.array(y[:trainIndex], dtype=\"float32\")\n",
        "        \n",
        "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
        "        evalLabels = np.array(y[trainIndex:], dtype=\"float32\")\n",
        "\n",
        "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
        "        \n",
        "    def _genVocabulary(self, reviews, labels):\n",
        "        \"\"\"\n",
        "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
        "        \"\"\"\n",
        "        \n",
        "        allWords = [word for review in reviews for word in review]\n",
        "        \n",
        "        # 去掉停用词\n",
        "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
        "        \n",
        "        wordCount = Counter(subWords)  # 统计词频\n",
        "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        # 去除低频词\n",
        "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
        "        \n",
        "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
        "        self.wordEmbedding = wordEmbedding\n",
        "        \n",
        "        word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
        "        \n",
        "        # 得到逆词频\n",
        "        self._getWordIndexFreq(vocab, reviews, word2idx)\n",
        "        \n",
        "        uniqueLabel = list(set(labels))\n",
        "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
        "        self.labelList = list(range(len(uniqueLabel)))\n",
        "        \n",
        "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
        "        with open(mypath/\"data/wordJson/word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(word2idx, f)\n",
        "        \n",
        "        with open(mypath/\"data/wordJson/label2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(label2idx, f)\n",
        "        \n",
        "        return word2idx, label2idx\n",
        "            \n",
        "    def _getWordEmbedding(self, words):\n",
        "        \"\"\"\n",
        "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
        "        \"\"\"\n",
        "        \n",
        "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(mypath/\"word2vec/word2Vec.bin\", binary=True)\n",
        "        vocab = []\n",
        "        wordEmbedding = []\n",
        "        \n",
        "        # 添加 \"pad\" 和 \"UNK\", \n",
        "        vocab.append(\"PAD\")\n",
        "        vocab.append(\"UNK\")\n",
        "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
        "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
        "        \n",
        "        for word in words:\n",
        "            try:\n",
        "                vector = wordVec.wv[word]\n",
        "                vocab.append(word)\n",
        "                wordEmbedding.append(vector)\n",
        "            except:\n",
        "                print(word + \"不存在于词向量中\")\n",
        "                \n",
        "        return vocab, np.array(wordEmbedding)\n",
        "    \n",
        "    def _getWordIndexFreq(self, vocab, reviews, word2idx):\n",
        "        \"\"\"\n",
        "        统计词汇空间中各个词出现在多少个文本中\n",
        "        \"\"\"\n",
        "        reviewDicts = [dict(zip(review, range(len(review)))) for review in reviews]\n",
        "        indexFreqs = [0] * len(vocab)\n",
        "        for word in vocab:\n",
        "            count = 0\n",
        "            for review in reviewDicts:\n",
        "                if word in review:\n",
        "                    count += 1\n",
        "            indexFreqs[word2idx[word]] = count\n",
        "        \n",
        "        self.indexFreqs = indexFreqs\n",
        "    \n",
        "    def _readStopWord(self, stopWordPath):\n",
        "        \"\"\"\n",
        "        读取停用词\n",
        "        \"\"\"\n",
        "        \n",
        "        with open(stopWordPath, \"r\") as f:\n",
        "            stopWords = f.read()\n",
        "            stopWordList = stopWords.splitlines()\n",
        "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
        "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
        "            \n",
        "    def dataGen(self):\n",
        "        \"\"\"\n",
        "        初始化训练集和验证集\n",
        "        \"\"\"\n",
        "        \n",
        "        # 初始化停用词\n",
        "        self._readStopWord(self._stopWordSource)\n",
        "        \n",
        "        # 初始化数据集\n",
        "        reviews, labels = self._readData(self._dataSource)\n",
        "        \n",
        "        # 初始化词汇-索引映射表和词向量矩阵\n",
        "        word2idx, label2idx = self._genVocabulary(reviews, labels)\n",
        "        \n",
        "        # 将标签和句子数值化\n",
        "        labelIds = self._labelToIndex(labels, label2idx)\n",
        "        reviewIds = self._wordToIndex(reviews, word2idx)\n",
        "        \n",
        "        # 初始化训练集和测试集\n",
        "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviewIds, labelIds, word2idx, self._rate)\n",
        "        self.trainReviews = trainReviews\n",
        "        self.trainLabels = trainLabels\n",
        "        \n",
        "        self.evalReviews = evalReviews\n",
        "        self.evalLabels = evalLabels\n",
        "        \n",
        "        \n",
        "data = Dataset(config)\n",
        "data.dataGen()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M76hYJXWXWhD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "5fc78c7f-397b-4cfa-efcf-777066774848"
      },
      "source": [
        "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
        "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
        "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data shape: (20000, 200)\n",
            "train label shape: (20000,)\n",
            "eval data shape: (5000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS2D6NqoXWhH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d01ae2af-9efd-45b7-f37a-c8d09b90e051"
      },
      "source": [
        "print(data.wordEmbedding.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31983, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3cx-iHyXWhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 输出batch数据集\n",
        "\n",
        "def nextBatch(x, y, batchSize):\n",
        "        \"\"\"\n",
        "        生成batch数据集，用生成器的方式输出\n",
        "        \"\"\"\n",
        "    \n",
        "        perm = np.arange(len(x))\n",
        "        np.random.shuffle(perm)\n",
        "        x = x[perm]\n",
        "        y = y[perm]\n",
        "        \n",
        "        numBatches = len(x) // batchSize\n",
        "\n",
        "        for i in range(numBatches):\n",
        "            start = i * batchSize\n",
        "            end = start + batchSize\n",
        "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
        "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
        "            \n",
        "            yield batchX, batchY"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1KENctoXWhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 构建模型\n",
        "class AdversarialLSTM(object):\n",
        "    \"\"\"\n",
        "    Text CNN 用于文本分类\n",
        "    \"\"\"\n",
        "    def __init__(self, config, wordEmbedding, indexFreqs):\n",
        "\n",
        "        # 定义模型的输入\n",
        "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
        "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
        "        \n",
        "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
        "        self.config = config\n",
        "        \n",
        "        # 根据词的频率计算权重\n",
        "        indexFreqs[0], indexFreqs[1] = 20000, 10000\n",
        "        weights = tf.cast(tf.reshape(indexFreqs / tf.reduce_sum(indexFreqs), [1, len(indexFreqs)]), dtype=tf.float32)\n",
        "        \n",
        "        # 词嵌入层\n",
        "        with tf.name_scope(\"embedding\"):\n",
        "\n",
        "            # 利用词频计算新的词嵌入矩阵\n",
        "            normWordEmbedding = self._normalize(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), weights)\n",
        "            \n",
        "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
        "            self.embeddedWords = tf.nn.embedding_lookup(normWordEmbedding, self.inputX)\n",
        "            \n",
        "         # 计算二元交叉熵损失 \n",
        "        with tf.name_scope(\"loss\"):\n",
        "            with tf.variable_scope(\"Bi-LSTM\", reuse=None):\n",
        "                self.logits = self._Bi_LSTMAttention(self.embeddedWords)\n",
        "                \n",
        "                if config.numClasses == 1:\n",
        "                    self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.float32, name=\"predictions\")\n",
        "                    losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
        "                                                                                                    dtype=tf.float32))\n",
        "                elif config.numClasses > 1:\n",
        "                    self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
        "                    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY)\n",
        "                \n",
        "                loss = tf.reduce_mean(losses)\n",
        "        \n",
        "        with tf.name_scope(\"perturLoss\"):\n",
        "            with tf.variable_scope(\"Bi-LSTM\", reuse=True):\n",
        "                perturWordEmbedding = self._addPerturbation(self.embeddedWords, loss)\n",
        "                perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n",
        "                perturLosses = tf.nn.sigmoid_cross_entropy_with_logits(logits=perturPredictions, labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
        "                                                                                                    dtype=tf.float32))\n",
        "                perturLoss = tf.reduce_mean(perturLosses)\n",
        "        \n",
        "        self.loss = loss + perturLoss\n",
        "            \n",
        "    def _Bi_LSTMAttention(self, embeddedWords):\n",
        "        \"\"\"\n",
        "        Bi-LSTM + Attention 的模型结构\n",
        "        \"\"\"\n",
        "        \n",
        "        config = self.config\n",
        "        \n",
        "        # 定义双向LSTM的模型结构\n",
        "        with tf.name_scope(\"Bi-LSTM\"):\n",
        "           \n",
        "            # 定义前向LSTM结构\n",
        "            lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=config.model.hiddenSizes, state_is_tuple=True),\n",
        "                                                         output_keep_prob=self.dropoutKeepProb)\n",
        "            # 定义反向LSTM结构\n",
        "            lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=config.model.hiddenSizes, state_is_tuple=True),\n",
        "                                                         output_keep_prob=self.dropoutKeepProb)\n",
        "\n",
        "\n",
        "            # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
        "            # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
        "            # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
        "            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
        "                                                                          self.embeddedWords, dtype=tf.float32,\n",
        "                                                                          scope=\"bi-lstm\")\n",
        "\n",
        "        \n",
        "        # 在Bi-LSTM+Attention的论文中，将前向和后向的输出相加\n",
        "        with tf.name_scope(\"Attention\"):\n",
        "            H = outputs[0] + outputs[1]\n",
        "\n",
        "            # 得到Attention的输出\n",
        "            output = self._attention(H)\n",
        "            outputSize = config.model.hiddenSizes\n",
        "        \n",
        "        # 全连接层的输出\n",
        "        with tf.name_scope(\"output\"):\n",
        "            outputW = tf.get_variable(\n",
        "                \"outputW\",\n",
        "                shape=[outputSize, config.numClasses],\n",
        "                initializer=tf.contrib.layers.xavier_initializer())\n",
        "            \n",
        "            outputB= tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
        "            predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
        "            \n",
        "        return predictions\n",
        "    \n",
        "    def _attention(self, H):\n",
        "        \"\"\"\n",
        "        利用Attention机制得到句子的向量表示\n",
        "        \"\"\"\n",
        "        # 获得最后一层LSTM的神经元数量\n",
        "        hiddenSize = config.model.hiddenSizes\n",
        "        \n",
        "        # 初始化一个权重向量，是可训练的参数\n",
        "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
        "        \n",
        "        # 对Bi-LSTM的输出用激活函数做非线性转换\n",
        "        M = tf.tanh(H)\n",
        "        \n",
        "        # 对W和M做矩阵运算，W=[batch_size, time_step, hidden_size]，计算前做维度转换成[batch_size * time_step, hidden_size]\n",
        "        # newM = [batch_size, time_step, 1]，每一个时间步的输出由向量转换成一个数字\n",
        "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
        "        \n",
        "        # 对newM做维度转换成[batch_size, time_step]\n",
        "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
        "        \n",
        "        # 用softmax做归一化处理[batch_size, time_step]\n",
        "        self.alpha = tf.nn.softmax(restoreM)\n",
        "        \n",
        "        # 利用求得的alpha的值对H进行加权求和，用矩阵运算直接操作\n",
        "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
        "        \n",
        "        # 将三维压缩成二维sequeezeR=[batch_size, hidden_size]\n",
        "        sequeezeR = tf.squeeze(r)\n",
        "        \n",
        "        sentenceRepren = tf.tanh(sequeezeR)\n",
        "        \n",
        "        # 对Attention的输出可以做dropout处理\n",
        "        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    def _normalize(self, wordEmbedding, weights):\n",
        "        \"\"\"\n",
        "        对word embedding 结合权重做标准化处理\n",
        "        \"\"\"\n",
        "        \n",
        "        mean = tf.matmul(weights, wordEmbedding)\n",
        "        print(mean)\n",
        "        powWordEmbedding = tf.pow(wordEmbedding - mean, 2.)\n",
        "        \n",
        "        var = tf.matmul(weights, powWordEmbedding)\n",
        "        print(var)\n",
        "        stddev = tf.sqrt(1e-6 + var)\n",
        "        \n",
        "        return (wordEmbedding - mean) / stddev\n",
        "    \n",
        "    def _addPerturbation(self, embedded, loss):\n",
        "        \"\"\"\n",
        "        添加波动到word embedding\n",
        "        \"\"\"\n",
        "        grad, = tf.gradients(\n",
        "            loss,\n",
        "            embedded,\n",
        "            aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n",
        "        grad = tf.stop_gradient(grad)\n",
        "        perturb = self._scaleL2(grad, self.config.model.epsilon)\n",
        "        return embedded + perturb\n",
        "    \n",
        "    def _scaleL2(self, x, norm_length):\n",
        "        # shape(x) = (batch, num_timesteps, d)\n",
        "        # Divide x by max(abs(x)) for a numerically stable L2 norm.\n",
        "        # 2norm(x) = a * 2norm(x/a)\n",
        "        # Scale over the full sequence, dims (1, 2)\n",
        "        alpha = tf.reduce_max(tf.abs(x), (1, 2), keepdims=True) + 1e-12\n",
        "        l2_norm = alpha * tf.sqrt(\n",
        "            tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keepdims=True) + 1e-6)\n",
        "        x_unit = x / l2_norm\n",
        "        return norm_length * x_unit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCPirue2XWhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "定义各类性能指标\n",
        "\"\"\"\n",
        "\n",
        "def mean(item: list) -> float:\n",
        "    \"\"\"\n",
        "    计算列表中元素的平均值\n",
        "    :param item: 列表对象\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    res = sum(item) / len(item) if len(item) > 0 else 0\n",
        "    return res\n",
        "\n",
        "\n",
        "def accuracy(pred_y, true_y):\n",
        "    \"\"\"\n",
        "    计算二类和多类的准确率\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if isinstance(pred_y[0], list):\n",
        "        pred_y = [item[0] for item in pred_y]\n",
        "    corr = 0\n",
        "    for i in range(len(pred_y)):\n",
        "        if pred_y[i] == true_y[i]:\n",
        "            corr += 1\n",
        "    acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
        "    return acc\n",
        "\n",
        "\n",
        "def binary_precision(pred_y, true_y, positive=1):\n",
        "    \"\"\"\n",
        "    二类的精确率计算\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param positive: 正例的索引表示\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    corr = 0\n",
        "    pred_corr = 0\n",
        "    for i in range(len(pred_y)):\n",
        "        if pred_y[i] == positive:\n",
        "            pred_corr += 1\n",
        "            if pred_y[i] == true_y[i]:\n",
        "                corr += 1\n",
        "\n",
        "    prec = corr / pred_corr if pred_corr > 0 else 0\n",
        "    return prec\n",
        "\n",
        "\n",
        "def binary_recall(pred_y, true_y, positive=1):\n",
        "    \"\"\"\n",
        "    二类的召回率\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param positive: 正例的索引表示\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    corr = 0\n",
        "    true_corr = 0\n",
        "    for i in range(len(pred_y)):\n",
        "        if true_y[i] == positive:\n",
        "            true_corr += 1\n",
        "            if pred_y[i] == true_y[i]:\n",
        "                corr += 1\n",
        "\n",
        "    rec = corr / true_corr if true_corr > 0 else 0\n",
        "    return rec\n",
        "\n",
        "\n",
        "def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
        "    \"\"\"\n",
        "    二类的f beta值\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param beta: beta值\n",
        "    :param positive: 正例的索引表示\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    precision = binary_precision(pred_y, true_y, positive)\n",
        "    recall = binary_recall(pred_y, true_y, positive)\n",
        "    try:\n",
        "        f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
        "    except:\n",
        "        f_b = 0\n",
        "    return f_b\n",
        "\n",
        "\n",
        "def multi_precision(pred_y, true_y, labels):\n",
        "    \"\"\"\n",
        "    多类的精确率\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param labels: 标签列表\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if isinstance(pred_y[0], list):\n",
        "        pred_y = [item[0] for item in pred_y]\n",
        "\n",
        "    precisions = [binary_precision(pred_y, true_y, label) for label in labels]\n",
        "    prec = mean(precisions)\n",
        "    return prec\n",
        "\n",
        "\n",
        "def multi_recall(pred_y, true_y, labels):\n",
        "    \"\"\"\n",
        "    多类的召回率\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param labels: 标签列表\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if isinstance(pred_y[0], list):\n",
        "        pred_y = [item[0] for item in pred_y]\n",
        "\n",
        "    recalls = [binary_recall(pred_y, true_y, label) for label in labels]\n",
        "    rec = mean(recalls)\n",
        "    return rec\n",
        "\n",
        "\n",
        "def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
        "    \"\"\"\n",
        "    多类的f beta值\n",
        "    :param pred_y: 预测结果\n",
        "    :param true_y: 真实结果\n",
        "    :param labels: 标签列表\n",
        "    :param beta: beta值\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if isinstance(pred_y[0], list):\n",
        "        pred_y = [item[0] for item in pred_y]\n",
        "\n",
        "    f_betas = [binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
        "    f_beta = mean(f_betas)\n",
        "    return f_beta\n",
        "\n",
        "\n",
        "def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
        "    \"\"\"\n",
        "    得到二分类的性能指标\n",
        "    :param pred_y:\n",
        "    :param true_y:\n",
        "    :param f_beta:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    acc = accuracy(pred_y, true_y)\n",
        "    recall = binary_recall(pred_y, true_y)\n",
        "    precision = binary_precision(pred_y, true_y)\n",
        "    f_beta = binary_f_beta(pred_y, true_y, f_beta)\n",
        "    return acc, recall, precision, f_beta\n",
        "\n",
        "\n",
        "def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
        "    \"\"\"\n",
        "    得到多分类的性能指标\n",
        "    :param pred_y:\n",
        "    :param true_y:\n",
        "    :param labels:\n",
        "    :param f_beta:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    acc = accuracy(pred_y, true_y)\n",
        "    recall = multi_recall(pred_y, true_y, labels)\n",
        "    precision = multi_precision(pred_y, true_y, labels)\n",
        "    f_beta = multi_f_beta(pred_y, true_y, labels, f_beta)\n",
        "    return acc, recall, precision, f_beta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU12QmctXWhR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cebaa2de-ec3b-43cf-fcec-242b88f2be3c"
      },
      "source": [
        "# 训练模型\n",
        "\n",
        "# 生成训练集和验证集\n",
        "trainReviews = data.trainReviews\n",
        "trainLabels = data.trainLabels\n",
        "evalReviews = data.evalReviews\n",
        "evalLabels = data.evalLabels\n",
        "\n",
        "wordEmbedding = data.wordEmbedding\n",
        "indexFreqs = data.indexFreqs\n",
        "labelList = data.labelList\n",
        "\n",
        "# 定义计算图\n",
        "with tf.Graph().as_default():\n",
        "\n",
        "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
        "    session_conf.gpu_options.allow_growth=True\n",
        "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
        "\n",
        "    sess = tf.Session(config=session_conf)\n",
        "    \n",
        "    # 定义会话\n",
        "    with sess.as_default():\n",
        "        lstm = AdversarialLSTM(config, wordEmbedding, indexFreqs)\n",
        "        \n",
        "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
        "        # 定义优化函数，传入学习速率参数\n",
        "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
        "        # 计算梯度,得到梯度和变量\n",
        "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
        "        # 将梯度应用到变量下，生成训练器\n",
        "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
        "        \n",
        "        # 用summary绘制tensorBoard\n",
        "        gradSummaries = []\n",
        "        for g, v in gradsAndVars:\n",
        "            if g is not None:\n",
        "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
        "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
        "        \n",
        "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
        "        print(\"Writing to {}\\n\".format(outDir))\n",
        "        \n",
        "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
        "        summaryOp = tf.summary.merge_all()\n",
        "        \n",
        "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
        "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
        "        \n",
        "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
        "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
        "        \n",
        "        \n",
        "        # 初始化所有变量\n",
        "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
        "        \n",
        "        # 保存模型的一种方式，保存为pb文件\n",
        "        savedModelPath = \"../model/adversarialLSTM/savedModel\"\n",
        "        if os.path.exists(savedModelPath):\n",
        "            os.rmdir(savedModelPath)\n",
        "        builder = tf.saved_model.builder.SavedModelBuilder(savedModelPath)\n",
        "            \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        def trainStep(batchX, batchY):\n",
        "            \"\"\"\n",
        "            训练函数\n",
        "            \"\"\"   \n",
        "            feed_dict = {\n",
        "              lstm.inputX: batchX,\n",
        "              lstm.inputY: batchY,\n",
        "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
        "            }\n",
        "            _, summary, step, loss, predictions = sess.run(\n",
        "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions],\n",
        "                feed_dict)\n",
        "            \n",
        "            if config.numClasses == 1:\n",
        "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
        "\n",
        "                \n",
        "            elif config.numClasses > 1:\n",
        "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY,\n",
        "                                                              labels=labelList)\n",
        "                \n",
        "            trainSummaryWriter.add_summary(summary, step)\n",
        "            \n",
        "            return loss, acc, prec, recall, f_beta\n",
        "\n",
        "        def devStep(batchX, batchY):\n",
        "            \"\"\"\n",
        "            验证函数\n",
        "            \"\"\"\n",
        "            feed_dict = {\n",
        "              lstm.inputX: batchX,\n",
        "              lstm.inputY: batchY,\n",
        "              lstm.dropoutKeepProb: 1.0\n",
        "            }\n",
        "            summary, step, loss, predictions = sess.run(\n",
        "                [summaryOp, globalStep, lstm.loss, lstm.predictions],\n",
        "                feed_dict)\n",
        "            \n",
        "            if config.numClasses == 1:\n",
        "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
        "\n",
        "                \n",
        "            elif config.numClasses > 1:\n",
        "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY,\n",
        "                                                              labels=labelList)\n",
        "                \n",
        "            trainSummaryWriter.add_summary(summary, step)\n",
        "            \n",
        "            return loss, acc, prec, recall, f_beta\n",
        "        \n",
        "        for i in range(config.training.epoches):\n",
        "            # 训练模型\n",
        "            print(\"start training model\")\n",
        "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
        "                loss, acc, prec, recall, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
        "                \n",
        "                currentStep = tf.train.global_step(sess, globalStep) \n",
        "                print(\"train: step: {}, loss: {}, acc: {}, recall: {}, precision: {}, f_beta: {}\".format(\n",
        "                    currentStep, loss, acc, recall, prec, f_beta))\n",
        "                if currentStep % config.training.evaluateEvery == 0:\n",
        "                    print(\"\\nEvaluation:\")\n",
        "                    \n",
        "                    losses = []\n",
        "                    accs = []\n",
        "                    f_betas = []\n",
        "                    precisions = []\n",
        "                    recalls = []\n",
        "                    \n",
        "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
        "                        loss, acc, precision, recall, f_beta = devStep(batchEval[0], batchEval[1])\n",
        "                        losses.append(loss)\n",
        "                        accs.append(acc)\n",
        "                        f_betas.append(f_beta)\n",
        "                        precisions.append(precision)\n",
        "                        recalls.append(recall)\n",
        "                        \n",
        "                    time_str = datetime.datetime.now().isoformat()\n",
        "                    print(\"{}, step: {}, loss: {}, acc: {},precision: {}, recall: {}, f_beta: {}\".format(time_str, currentStep, mean(losses), \n",
        "                                                                                                       mean(accs), mean(precisions),\n",
        "                                                                                                       mean(recalls), mean(f_betas)))\n",
        "                    \n",
        "                if currentStep % config.training.checkpointEvery == 0:\n",
        "                    # 保存模型的另一种方法，保存checkpoint文件\n",
        "                    path = saver.save(sess, \"../model/adversarialLSTM/model/my-model\", global_step=currentStep)\n",
        "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
        "                    \n",
        "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
        "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
        "\n",
        "        outputs = {\"predictions\": tf.saved_model.utils.build_tensor_info(lstm.predictions)}\n",
        "\n",
        "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
        "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
        "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
        "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
        "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
        "\n",
        "        builder.save()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"embedding/MatMul:0\", shape=(1, 200), dtype=float32)\n",
            "Tensor(\"embedding/MatMul_1:0\", shape=(1, 200), dtype=float32)\n",
            "WARNING:tensorflow:From <ipython-input-18-d7375c04b208>:63: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-18-d7375c04b208>:75: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-18-d7375c04b208>:130: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/kernel:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/kernel_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/bias:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/bias_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/bias:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/kernel:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/kernel_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/bias:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/bias_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/bias:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name loss/Bi-LSTM/Attention/Variable:0/grad/hist is illegal; using loss/Bi-LSTM/Attention/Variable_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name loss/Bi-LSTM/Attention/Variable:0/grad/sparsity is illegal; using loss/Bi-LSTM/Attention/Variable_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name Bi-LSTM/outputW:0/grad/hist is illegal; using Bi-LSTM/outputW_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name Bi-LSTM/outputW:0/grad/sparsity is illegal; using Bi-LSTM/outputW_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name loss/Bi-LSTM/output/outputB:0/grad/hist is illegal; using loss/Bi-LSTM/output/outputB_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name loss/Bi-LSTM/output/outputB:0/grad/sparsity is illegal; using loss/Bi-LSTM/output/outputB_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/Attention/Variable:0/grad/hist is illegal; using perturLoss/Bi-LSTM/Attention/Variable_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/Attention/Variable:0/grad/sparsity is illegal; using perturLoss/Bi-LSTM/Attention/Variable_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/output/outputB:0/grad/hist is illegal; using perturLoss/Bi-LSTM/output/outputB_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/output/outputB:0/grad/sparsity is illegal; using perturLoss/Bi-LSTM/output/outputB_0/grad/sparsity instead.\n",
            "Writing to /content/summarys\n",
            "\n",
            "start training model\n",
            "train: step: 1, loss: 1.40168297290802, acc: 0.53125, recall: 0.640625, precision: 0.5256410256410257, f_beta: 0.5774647887323944\n",
            "train: step: 2, loss: 1.6853760480880737, acc: 0.484375, recall: 0.07246376811594203, precision: 0.7142857142857143, f_beta: 0.13157894736842105\n",
            "train: step: 3, loss: 1.4353193044662476, acc: 0.515625, recall: 0.6724137931034483, precision: 0.47560975609756095, f_beta: 0.5571428571428572\n",
            "train: step: 4, loss: 1.4864153861999512, acc: 0.53125, recall: 0.8333333333333334, precision: 0.5555555555555556, f_beta: 0.6666666666666667\n",
            "train: step: 5, loss: 1.4508885145187378, acc: 0.5390625, recall: 0.8676470588235294, precision: 0.5412844036697247, f_beta: 0.6666666666666666\n",
            "train: step: 6, loss: 1.4791357517242432, acc: 0.46875, recall: 0.8148148148148148, precision: 0.43137254901960786, f_beta: 0.5641025641025642\n",
            "train: step: 7, loss: 1.3962736129760742, acc: 0.5859375, recall: 0.5223880597014925, precision: 0.625, f_beta: 0.5691056910569106\n",
            "train: step: 8, loss: 1.429212212562561, acc: 0.53125, recall: 0.3283582089552239, precision: 0.5945945945945946, f_beta: 0.42307692307692313\n",
            "train: step: 9, loss: 1.3866188526153564, acc: 0.5859375, recall: 0.288135593220339, precision: 0.6071428571428571, f_beta: 0.39080459770114945\n",
            "train: step: 10, loss: 1.4797112941741943, acc: 0.5, recall: 0.30158730158730157, precision: 0.48717948717948717, f_beta: 0.37254901960784315\n",
            "train: step: 11, loss: 1.4124141931533813, acc: 0.5390625, recall: 0.3939393939393939, precision: 0.5777777777777777, f_beta: 0.4684684684684684\n",
            "train: step: 12, loss: 1.313217282295227, acc: 0.6328125, recall: 0.6166666666666667, precision: 0.6065573770491803, f_beta: 0.6115702479338844\n",
            "train: step: 13, loss: 1.3328847885131836, acc: 0.6171875, recall: 0.6885245901639344, precision: 0.5833333333333334, f_beta: 0.631578947368421\n",
            "train: step: 14, loss: 1.3083926439285278, acc: 0.609375, recall: 0.6792452830188679, precision: 0.5217391304347826, f_beta: 0.5901639344262294\n",
            "train: step: 15, loss: 1.323520302772522, acc: 0.59375, recall: 0.5087719298245614, precision: 0.5471698113207547, f_beta: 0.5272727272727273\n",
            "train: step: 16, loss: 1.2684249877929688, acc: 0.6484375, recall: 0.484375, precision: 0.7209302325581395, f_beta: 0.5794392523364486\n",
            "train: step: 17, loss: 1.2525975704193115, acc: 0.625, recall: 0.4444444444444444, precision: 0.8, f_beta: 0.5714285714285714\n",
            "train: step: 18, loss: 1.3243075609207153, acc: 0.6015625, recall: 0.4642857142857143, precision: 0.5531914893617021, f_beta: 0.5048543689320388\n",
            "train: step: 19, loss: 1.2343850135803223, acc: 0.671875, recall: 0.546875, precision: 0.7291666666666666, f_beta: 0.625\n",
            "train: step: 20, loss: 1.207491159439087, acc: 0.6484375, recall: 0.6491228070175439, precision: 0.5967741935483871, f_beta: 0.6218487394957983\n",
            "train: step: 21, loss: 1.1527334451675415, acc: 0.765625, recall: 0.7777777777777778, precision: 0.8, f_beta: 0.7887323943661971\n",
            "train: step: 22, loss: 1.155251383781433, acc: 0.7265625, recall: 0.7377049180327869, precision: 0.703125, f_beta: 0.72\n",
            "train: step: 23, loss: 1.0934914350509644, acc: 0.7890625, recall: 0.7903225806451613, precision: 0.7777777777777778, f_beta: 0.7839999999999999\n",
            "train: step: 24, loss: 1.1341410875320435, acc: 0.703125, recall: 0.65, precision: 0.6964285714285714, f_beta: 0.6724137931034482\n",
            "train: step: 25, loss: 1.065544843673706, acc: 0.7734375, recall: 0.8051948051948052, precision: 0.8157894736842105, f_beta: 0.8104575163398693\n",
            "train: step: 26, loss: 1.1250674724578857, acc: 0.6875, recall: 0.7936507936507936, precision: 0.6493506493506493, f_beta: 0.7142857142857143\n",
            "train: step: 27, loss: 1.1168317794799805, acc: 0.7421875, recall: 0.7681159420289855, precision: 0.7571428571428571, f_beta: 0.762589928057554\n",
            "train: step: 28, loss: 1.0860471725463867, acc: 0.71875, recall: 0.7638888888888888, precision: 0.7432432432432432, f_beta: 0.7534246575342465\n",
            "train: step: 29, loss: 1.0388853549957275, acc: 0.78125, recall: 0.7323943661971831, precision: 0.8524590163934426, f_beta: 0.787878787878788\n",
            "train: step: 30, loss: 0.9342004656791687, acc: 0.8046875, recall: 0.8507462686567164, precision: 0.7916666666666666, f_beta: 0.8201438848920863\n",
            "train: step: 31, loss: 0.9208318591117859, acc: 0.8203125, recall: 0.8615384615384616, precision: 0.8, f_beta: 0.8296296296296297\n",
            "train: step: 32, loss: 0.9851841926574707, acc: 0.78125, recall: 0.8448275862068966, precision: 0.7205882352941176, f_beta: 0.7777777777777778\n",
            "train: step: 33, loss: 0.9184318780899048, acc: 0.7578125, recall: 0.7377049180327869, precision: 0.75, f_beta: 0.743801652892562\n",
            "train: step: 34, loss: 0.9552846550941467, acc: 0.75, recall: 0.7213114754098361, precision: 0.7457627118644068, f_beta: 0.7333333333333334\n",
            "train: step: 35, loss: 0.8809068202972412, acc: 0.7734375, recall: 0.7571428571428571, precision: 0.8153846153846154, f_beta: 0.7851851851851851\n",
            "train: step: 36, loss: 0.9692510366439819, acc: 0.7578125, recall: 0.6857142857142857, precision: 0.8421052631578947, f_beta: 0.7559055118110235\n",
            "train: step: 37, loss: 0.873389720916748, acc: 0.8046875, recall: 0.7846153846153846, precision: 0.8225806451612904, f_beta: 0.8031496062992126\n",
            "train: step: 38, loss: 0.9167129993438721, acc: 0.78125, recall: 0.8113207547169812, precision: 0.7049180327868853, f_beta: 0.7543859649122806\n",
            "train: step: 39, loss: 0.8128849267959595, acc: 0.8359375, recall: 0.8805970149253731, precision: 0.8194444444444444, f_beta: 0.8489208633093526\n",
            "train: step: 40, loss: 0.9431279301643372, acc: 0.78125, recall: 0.796875, precision: 0.7727272727272727, f_beta: 0.7846153846153846\n",
            "train: step: 41, loss: 0.8556239604949951, acc: 0.8046875, recall: 0.8392857142857143, precision: 0.746031746031746, f_beta: 0.7899159663865546\n",
            "train: step: 42, loss: 0.9779630303382874, acc: 0.7734375, recall: 0.7230769230769231, precision: 0.8103448275862069, f_beta: 0.7642276422764227\n",
            "train: step: 43, loss: 0.7438843250274658, acc: 0.875, recall: 0.8571428571428571, precision: 0.8852459016393442, f_beta: 0.8709677419354839\n",
            "train: step: 44, loss: 0.6800104379653931, acc: 0.84375, recall: 0.8641975308641975, precision: 0.8860759493670886, f_beta: 0.8749999999999999\n",
            "train: step: 45, loss: 0.6808971166610718, acc: 0.8984375, recall: 0.9090909090909091, precision: 0.8955223880597015, f_beta: 0.9022556390977443\n",
            "train: step: 46, loss: 0.6136361360549927, acc: 0.8984375, recall: 0.8870967741935484, precision: 0.9016393442622951, f_beta: 0.8943089430894309\n",
            "train: step: 47, loss: 0.7849957346916199, acc: 0.8125, recall: 0.8714285714285714, precision: 0.8026315789473685, f_beta: 0.8356164383561644\n",
            "train: step: 48, loss: 0.9493376016616821, acc: 0.765625, recall: 0.8548387096774194, precision: 0.7162162162162162, f_beta: 0.7794117647058824\n",
            "train: step: 49, loss: 0.8017605543136597, acc: 0.8125, recall: 0.8082191780821918, precision: 0.855072463768116, f_beta: 0.8309859154929579\n",
            "train: step: 50, loss: 0.6814396381378174, acc: 0.859375, recall: 0.8035714285714286, precision: 0.8653846153846154, f_beta: 0.8333333333333334\n",
            "train: step: 51, loss: 0.6649878025054932, acc: 0.8671875, recall: 0.8727272727272727, precision: 0.8275862068965517, f_beta: 0.8495575221238938\n",
            "train: step: 52, loss: 0.7190443277359009, acc: 0.8359375, recall: 0.8032786885245902, precision: 0.8448275862068966, f_beta: 0.823529411764706\n",
            "train: step: 53, loss: 0.624555230140686, acc: 0.890625, recall: 0.8813559322033898, precision: 0.8813559322033898, f_beta: 0.8813559322033898\n",
            "train: step: 54, loss: 0.6158551573753357, acc: 0.8828125, recall: 0.8431372549019608, precision: 0.86, f_beta: 0.8514851485148515\n",
            "train: step: 55, loss: 0.836807131767273, acc: 0.8046875, recall: 0.759493670886076, precision: 0.9090909090909091, f_beta: 0.8275862068965516\n",
            "train: step: 56, loss: 0.7385267019271851, acc: 0.84375, recall: 0.864406779661017, precision: 0.8095238095238095, f_beta: 0.8360655737704918\n",
            "train: step: 57, loss: 0.7915180921554565, acc: 0.8046875, recall: 0.8448275862068966, precision: 0.7538461538461538, f_beta: 0.7967479674796748\n",
            "train: step: 58, loss: 0.8729999661445618, acc: 0.75, recall: 0.8928571428571429, precision: 0.6578947368421053, f_beta: 0.7575757575757577\n",
            "train: step: 59, loss: 0.598426103591919, acc: 0.8984375, recall: 0.921875, precision: 0.8805970149253731, f_beta: 0.9007633587786259\n",
            "train: step: 60, loss: 0.5517105460166931, acc: 0.90625, recall: 0.8857142857142857, precision: 0.9393939393939394, f_beta: 0.9117647058823529\n",
            "train: step: 61, loss: 0.940924346446991, acc: 0.7890625, recall: 0.7638888888888888, precision: 0.8461538461538461, f_beta: 0.802919708029197\n",
            "train: step: 62, loss: 0.8370625972747803, acc: 0.8125, recall: 0.7619047619047619, precision: 0.8421052631578947, f_beta: 0.8\n",
            "train: step: 63, loss: 0.8618177175521851, acc: 0.796875, recall: 0.803030303030303, precision: 0.803030303030303, f_beta: 0.803030303030303\n",
            "train: step: 64, loss: 0.6000316143035889, acc: 0.890625, recall: 0.9206349206349206, precision: 0.8656716417910447, f_beta: 0.8923076923076922\n",
            "train: step: 65, loss: 0.8693368434906006, acc: 0.8046875, recall: 0.8472222222222222, precision: 0.8133333333333334, f_beta: 0.8299319727891157\n",
            "train: step: 66, loss: 0.6156468391418457, acc: 0.8671875, recall: 0.9454545454545454, precision: 0.7878787878787878, f_beta: 0.8595041322314049\n",
            "train: step: 67, loss: 0.6087636947631836, acc: 0.8984375, recall: 0.9666666666666667, precision: 0.8405797101449275, f_beta: 0.8992248062015503\n",
            "train: step: 68, loss: 0.7341777682304382, acc: 0.84375, recall: 0.8709677419354839, precision: 0.8181818181818182, f_beta: 0.84375\n",
            "train: step: 69, loss: 0.6836799383163452, acc: 0.84375, recall: 0.855072463768116, precision: 0.855072463768116, f_beta: 0.855072463768116\n",
            "train: step: 70, loss: 0.5589170455932617, acc: 0.8828125, recall: 0.9076923076923077, precision: 0.8676470588235294, f_beta: 0.887218045112782\n",
            "train: step: 71, loss: 0.608131468296051, acc: 0.859375, recall: 0.8125, precision: 0.896551724137931, f_beta: 0.8524590163934426\n",
            "train: step: 72, loss: 0.6935721635818481, acc: 0.8515625, recall: 0.8235294117647058, precision: 0.8888888888888888, f_beta: 0.8549618320610687\n",
            "train: step: 73, loss: 0.7524256706237793, acc: 0.859375, recall: 0.8518518518518519, precision: 0.8214285714285714, f_beta: 0.8363636363636364\n",
            "train: step: 74, loss: 0.7504336833953857, acc: 0.875, recall: 0.9, precision: 0.875, f_beta: 0.8873239436619719\n",
            "train: step: 75, loss: 0.628530740737915, acc: 0.875, recall: 0.9076923076923077, precision: 0.855072463768116, f_beta: 0.8805970149253731\n",
            "train: step: 76, loss: 0.6693047285079956, acc: 0.8203125, recall: 0.8615384615384616, precision: 0.8, f_beta: 0.8296296296296297\n",
            "train: step: 77, loss: 0.6810615062713623, acc: 0.84375, recall: 0.8620689655172413, precision: 0.8064516129032258, f_beta: 0.8333333333333334\n",
            "train: step: 78, loss: 0.8289047479629517, acc: 0.8359375, recall: 0.8363636363636363, precision: 0.7931034482758621, f_beta: 0.8141592920353982\n",
            "train: step: 79, loss: 0.8282672166824341, acc: 0.8125, recall: 0.828125, precision: 0.803030303030303, f_beta: 0.8153846153846153\n",
            "train: step: 80, loss: 0.5705283880233765, acc: 0.875, recall: 0.8857142857142857, precision: 0.8857142857142857, f_beta: 0.8857142857142857\n",
            "train: step: 81, loss: 0.7562738656997681, acc: 0.84375, recall: 0.8356164383561644, precision: 0.8840579710144928, f_beta: 0.8591549295774648\n",
            "train: step: 82, loss: 0.7281777858734131, acc: 0.8359375, recall: 0.8225806451612904, precision: 0.8360655737704918, f_beta: 0.829268292682927\n",
            "train: step: 83, loss: 0.6420567035675049, acc: 0.84375, recall: 0.8676470588235294, precision: 0.8428571428571429, f_beta: 0.855072463768116\n",
            "train: step: 84, loss: 0.6501705050468445, acc: 0.8515625, recall: 0.873015873015873, precision: 0.8333333333333334, f_beta: 0.8527131782945736\n",
            "train: step: 85, loss: 0.6991661787033081, acc: 0.8828125, recall: 0.9272727272727272, precision: 0.8225806451612904, f_beta: 0.8717948717948718\n",
            "train: step: 86, loss: 0.7171649932861328, acc: 0.8359375, recall: 0.8076923076923077, precision: 0.7924528301886793, f_beta: 0.7999999999999999\n",
            "train: step: 87, loss: 0.7203840613365173, acc: 0.828125, recall: 0.8166666666666667, precision: 0.8166666666666667, f_beta: 0.8166666666666667\n",
            "train: step: 88, loss: 0.6244063377380371, acc: 0.8515625, recall: 0.8115942028985508, precision: 0.9032258064516129, f_beta: 0.8549618320610687\n",
            "train: step: 89, loss: 0.5926138758659363, acc: 0.90625, recall: 0.8666666666666667, precision: 0.9285714285714286, f_beta: 0.896551724137931\n",
            "train: step: 90, loss: 0.6084948778152466, acc: 0.875, recall: 0.8833333333333333, precision: 0.8548387096774194, f_beta: 0.8688524590163934\n",
            "train: step: 91, loss: 0.9066281318664551, acc: 0.8046875, recall: 0.8032786885245902, precision: 0.7903225806451613, f_beta: 0.7967479674796747\n",
            "train: step: 92, loss: 0.5947072505950928, acc: 0.8671875, recall: 0.8656716417910447, precision: 0.8787878787878788, f_beta: 0.8721804511278195\n",
            "train: step: 93, loss: 0.6870876550674438, acc: 0.8359375, recall: 0.8260869565217391, precision: 0.8636363636363636, f_beta: 0.8444444444444444\n",
            "train: step: 94, loss: 0.631655216217041, acc: 0.859375, recall: 0.8412698412698413, precision: 0.8688524590163934, f_beta: 0.8548387096774194\n",
            "train: step: 95, loss: 0.577759861946106, acc: 0.8828125, recall: 0.8529411764705882, precision: 0.9206349206349206, f_beta: 0.8854961832061068\n",
            "train: step: 96, loss: 0.6230864524841309, acc: 0.859375, recall: 0.9375, precision: 0.75, f_beta: 0.8333333333333334\n",
            "train: step: 97, loss: 0.6210344433784485, acc: 0.8984375, recall: 0.9523809523809523, precision: 0.8571428571428571, f_beta: 0.9022556390977443\n",
            "train: step: 98, loss: 0.7228683233261108, acc: 0.8984375, recall: 0.9344262295081968, precision: 0.8636363636363636, f_beta: 0.8976377952755905\n",
            "train: step: 99, loss: 0.7072156071662903, acc: 0.8359375, recall: 0.8305084745762712, precision: 0.8166666666666667, f_beta: 0.8235294117647058\n",
            "train: step: 100, loss: 0.6763782501220703, acc: 0.8671875, recall: 0.8309859154929577, precision: 0.921875, f_beta: 0.874074074074074\n",
            "\n",
            "Evaluation:\n",
            "2019-10-16T03:15:31.045942, step: 100, loss: 0.668425916097103, acc: 0.8573717948717948,precision: 0.9007775203400774, recall: 0.8047535350657297, f_beta: 0.8490877830671218\n",
            "Saved model checkpoint to ../model/adversarialLSTM/model/my-model-100\n",
            "\n",
            "train: step: 101, loss: 0.5891713500022888, acc: 0.8828125, recall: 0.8870967741935484, precision: 0.873015873015873, f_beta: 0.88\n",
            "train: step: 102, loss: 0.6350373029708862, acc: 0.875, recall: 0.796875, precision: 0.9444444444444444, f_beta: 0.8644067796610169\n",
            "train: step: 103, loss: 0.6106307506561279, acc: 0.875, recall: 0.8615384615384616, precision: 0.8888888888888888, f_beta: 0.8750000000000001\n",
            "train: step: 104, loss: 0.6853144764900208, acc: 0.8828125, recall: 0.9166666666666666, precision: 0.88, f_beta: 0.8979591836734694\n",
            "train: step: 105, loss: 0.9052144289016724, acc: 0.8203125, recall: 0.8679245283018868, precision: 0.7419354838709677, f_beta: 0.8\n",
            "train: step: 106, loss: 0.7446188926696777, acc: 0.8359375, recall: 0.819672131147541, precision: 0.8333333333333334, f_beta: 0.8264462809917356\n",
            "train: step: 107, loss: 0.7040464878082275, acc: 0.859375, recall: 0.875, precision: 0.875, f_beta: 0.875\n",
            "train: step: 108, loss: 0.5086833834648132, acc: 0.8984375, recall: 0.8923076923076924, precision: 0.90625, f_beta: 0.8992248062015504\n",
            "train: step: 109, loss: 0.6848207712173462, acc: 0.84375, recall: 0.90625, precision: 0.8055555555555556, f_beta: 0.8529411764705882\n",
            "train: step: 110, loss: 0.6258736848831177, acc: 0.875, recall: 0.9295774647887324, precision: 0.8571428571428571, f_beta: 0.8918918918918918\n",
            "train: step: 111, loss: 0.7671669125556946, acc: 0.8359375, recall: 0.8840579710144928, precision: 0.8243243243243243, f_beta: 0.853146853146853\n",
            "train: step: 112, loss: 0.7926753759384155, acc: 0.859375, recall: 0.8450704225352113, precision: 0.8955223880597015, f_beta: 0.8695652173913043\n",
            "train: step: 113, loss: 0.6120178699493408, acc: 0.875, recall: 0.859375, precision: 0.8870967741935484, f_beta: 0.8730158730158729\n",
            "train: step: 114, loss: 0.7227201461791992, acc: 0.84375, recall: 0.8405797101449275, precision: 0.8656716417910447, f_beta: 0.8529411764705882\n",
            "train: step: 115, loss: 0.731440544128418, acc: 0.84375, recall: 0.8529411764705882, precision: 0.8529411764705882, f_beta: 0.8529411764705882\n",
            "train: step: 116, loss: 0.7424582242965698, acc: 0.859375, recall: 0.8448275862068966, precision: 0.8448275862068966, f_beta: 0.8448275862068967\n",
            "train: step: 117, loss: 0.5106679201126099, acc: 0.875, recall: 0.8307692307692308, precision: 0.9152542372881356, f_beta: 0.870967741935484\n",
            "train: step: 118, loss: 0.7621515989303589, acc: 0.8125, recall: 0.8421052631578947, precision: 0.7619047619047619, f_beta: 0.8\n",
            "train: step: 119, loss: 0.6593811511993408, acc: 0.8828125, recall: 0.8405797101449275, precision: 0.9354838709677419, f_beta: 0.8854961832061068\n",
            "train: step: 120, loss: 0.6106129884719849, acc: 0.875, recall: 0.90625, precision: 0.8529411764705882, f_beta: 0.8787878787878787\n",
            "train: step: 121, loss: 0.5426849126815796, acc: 0.890625, recall: 0.9452054794520548, precision: 0.8734177215189873, f_beta: 0.9078947368421053\n",
            "train: step: 122, loss: 0.6152993440628052, acc: 0.875, recall: 0.8571428571428571, precision: 0.8852459016393442, f_beta: 0.8709677419354839\n",
            "train: step: 123, loss: 0.5972731113433838, acc: 0.8984375, recall: 0.8656716417910447, precision: 0.9354838709677419, f_beta: 0.8992248062015503\n",
            "train: step: 124, loss: 0.7227767705917358, acc: 0.8671875, recall: 0.890625, precision: 0.8507462686567164, f_beta: 0.8702290076335878\n",
            "train: step: 125, loss: 0.6766510009765625, acc: 0.8359375, recall: 0.8970588235294118, precision: 0.8133333333333334, f_beta: 0.8531468531468531\n",
            "train: step: 126, loss: 0.5410139560699463, acc: 0.9140625, recall: 0.9411764705882353, precision: 0.9014084507042254, f_beta: 0.920863309352518\n",
            "train: step: 127, loss: 0.6197894811630249, acc: 0.859375, recall: 0.86, precision: 0.7962962962962963, f_beta: 0.826923076923077\n",
            "train: step: 128, loss: 0.5658652782440186, acc: 0.90625, recall: 0.8823529411764706, precision: 0.9375, f_beta: 0.9090909090909091\n",
            "train: step: 129, loss: 0.5674825310707092, acc: 0.8984375, recall: 0.84375, precision: 0.9473684210526315, f_beta: 0.8925619834710744\n",
            "train: step: 130, loss: 0.6919040679931641, acc: 0.8515625, recall: 0.8615384615384616, precision: 0.8484848484848485, f_beta: 0.8549618320610687\n",
            "train: step: 131, loss: 0.6051468849182129, acc: 0.8359375, recall: 0.8166666666666667, precision: 0.8305084745762712, f_beta: 0.8235294117647058\n",
            "train: step: 132, loss: 0.5074157118797302, acc: 0.890625, recall: 0.9117647058823529, precision: 0.8857142857142857, f_beta: 0.8985507246376812\n",
            "train: step: 133, loss: 0.5756563544273376, acc: 0.8671875, recall: 0.8412698412698413, precision: 0.8833333333333333, f_beta: 0.8617886178861788\n",
            "train: step: 134, loss: 0.6439629793167114, acc: 0.859375, recall: 0.9310344827586207, precision: 0.7941176470588235, f_beta: 0.8571428571428571\n",
            "train: step: 135, loss: 0.6932364702224731, acc: 0.8515625, recall: 0.859375, precision: 0.8461538461538461, f_beta: 0.8527131782945736\n",
            "train: step: 136, loss: 0.6419103741645813, acc: 0.875, recall: 0.8461538461538461, precision: 0.9016393442622951, f_beta: 0.873015873015873\n",
            "train: step: 137, loss: 0.4815070331096649, acc: 0.8828125, recall: 0.8382352941176471, precision: 0.9344262295081968, f_beta: 0.8837209302325582\n",
            "train: step: 138, loss: 0.5853983163833618, acc: 0.875, recall: 0.9180327868852459, precision: 0.835820895522388, f_beta: 0.875\n",
            "train: step: 139, loss: 0.8368971347808838, acc: 0.8125, recall: 0.8275862068965517, precision: 0.7741935483870968, f_beta: 0.7999999999999999\n",
            "train: step: 140, loss: 0.5180870890617371, acc: 0.9140625, recall: 0.8923076923076924, precision: 0.9354838709677419, f_beta: 0.9133858267716536\n",
            "train: step: 141, loss: 0.6203945279121399, acc: 0.875, recall: 0.8524590163934426, precision: 0.8813559322033898, f_beta: 0.8666666666666666\n",
            "train: step: 142, loss: 0.6738713979721069, acc: 0.875, recall: 0.8507462686567164, precision: 0.9047619047619048, f_beta: 0.8769230769230769\n",
            "train: step: 143, loss: 0.6926690936088562, acc: 0.875, recall: 0.9047619047619048, precision: 0.8507462686567164, f_beta: 0.8769230769230769\n",
            "train: step: 144, loss: 0.5762543082237244, acc: 0.890625, recall: 0.8676470588235294, precision: 0.921875, f_beta: 0.893939393939394\n",
            "train: step: 145, loss: 0.5740014910697937, acc: 0.859375, recall: 0.8615384615384616, precision: 0.8615384615384616, f_beta: 0.8615384615384615\n",
            "train: step: 146, loss: 0.762255072593689, acc: 0.828125, recall: 0.8867924528301887, precision: 0.746031746031746, f_beta: 0.8103448275862069\n",
            "train: step: 147, loss: 0.6853175759315491, acc: 0.8515625, recall: 0.8983050847457628, precision: 0.803030303030303, f_beta: 0.8480000000000001\n",
            "train: step: 148, loss: 0.5775379538536072, acc: 0.84375, recall: 0.8387096774193549, precision: 0.8387096774193549, f_beta: 0.8387096774193549\n",
            "train: step: 149, loss: 0.5610330700874329, acc: 0.8828125, recall: 0.890625, precision: 0.8769230769230769, f_beta: 0.883720930232558\n",
            "train: step: 150, loss: 0.6295088529586792, acc: 0.875, recall: 0.9, precision: 0.875, f_beta: 0.8873239436619719\n",
            "train: step: 151, loss: 0.7302017211914062, acc: 0.8671875, recall: 0.8275862068965517, precision: 0.8727272727272727, f_beta: 0.8495575221238938\n",
            "train: step: 152, loss: 0.7071717977523804, acc: 0.8359375, recall: 0.8088235294117647, precision: 0.873015873015873, f_beta: 0.8396946564885497\n",
            "train: step: 153, loss: 0.6813212633132935, acc: 0.84375, recall: 0.8507462686567164, precision: 0.8507462686567164, f_beta: 0.8507462686567164\n",
            "train: step: 154, loss: 0.6901681423187256, acc: 0.875, recall: 0.8823529411764706, precision: 0.8823529411764706, f_beta: 0.8823529411764706\n",
            "train: step: 155, loss: 0.6838701963424683, acc: 0.828125, recall: 0.8666666666666667, precision: 0.7878787878787878, f_beta: 0.8253968253968254\n",
            "train: step: 156, loss: 0.6203854084014893, acc: 0.8671875, recall: 0.9047619047619048, precision: 0.8382352941176471, f_beta: 0.8702290076335878\n",
            "start training model\n",
            "train: step: 157, loss: 0.5130637884140015, acc: 0.8984375, recall: 0.8970588235294118, precision: 0.9104477611940298, f_beta: 0.9037037037037037\n",
            "train: step: 158, loss: 0.6929000020027161, acc: 0.8515625, recall: 0.8701298701298701, precision: 0.881578947368421, f_beta: 0.8758169934640522\n",
            "train: step: 159, loss: 0.5550792217254639, acc: 0.890625, recall: 0.9322033898305084, precision: 0.8461538461538461, f_beta: 0.8870967741935484\n",
            "train: step: 160, loss: 0.7034947872161865, acc: 0.8828125, recall: 0.9178082191780822, precision: 0.881578947368421, f_beta: 0.8993288590604027\n",
            "train: step: 161, loss: 0.6066932678222656, acc: 0.9140625, recall: 0.9508196721311475, precision: 0.8787878787878788, f_beta: 0.9133858267716536\n",
            "train: step: 162, loss: 0.7749874591827393, acc: 0.8125, recall: 0.847457627118644, precision: 0.7692307692307693, f_beta: 0.8064516129032259\n",
            "train: step: 163, loss: 0.5654177665710449, acc: 0.8828125, recall: 0.8867924528301887, precision: 0.8392857142857143, f_beta: 0.8623853211009174\n",
            "train: step: 164, loss: 0.46346530318260193, acc: 0.921875, recall: 0.9482758620689655, precision: 0.8870967741935484, f_beta: 0.9166666666666667\n",
            "train: step: 165, loss: 0.7569547891616821, acc: 0.8046875, recall: 0.8, precision: 0.7586206896551724, f_beta: 0.7787610619469026\n",
            "train: step: 166, loss: 0.5129649043083191, acc: 0.90625, recall: 0.7959183673469388, precision: 0.9512195121951219, f_beta: 0.8666666666666666\n",
            "train: step: 167, loss: 0.7179341316223145, acc: 0.8359375, recall: 0.7777777777777778, precision: 0.875, f_beta: 0.823529411764706\n",
            "train: step: 168, loss: 0.6298799514770508, acc: 0.8515625, recall: 0.828125, precision: 0.8688524590163934, f_beta: 0.8480000000000001\n",
            "train: step: 169, loss: 0.4359312057495117, acc: 0.9453125, recall: 0.9365079365079365, precision: 0.9516129032258065, f_beta: 0.944\n",
            "train: step: 170, loss: 0.6201802492141724, acc: 0.859375, recall: 0.8676470588235294, precision: 0.8676470588235294, f_beta: 0.8676470588235294\n",
            "train: step: 171, loss: 0.662624716758728, acc: 0.875, recall: 0.819672131147541, precision: 0.9090909090909091, f_beta: 0.8620689655172413\n",
            "train: step: 172, loss: 0.5930908918380737, acc: 0.8828125, recall: 0.8870967741935484, precision: 0.873015873015873, f_beta: 0.88\n",
            "train: step: 173, loss: 0.7506539225578308, acc: 0.8515625, recall: 0.8709677419354839, precision: 0.8307692307692308, f_beta: 0.8503937007874016\n",
            "train: step: 174, loss: 0.6047199964523315, acc: 0.921875, recall: 0.9516129032258065, precision: 0.8939393939393939, f_beta: 0.921875\n",
            "train: step: 175, loss: 0.49130427837371826, acc: 0.90625, recall: 0.9090909090909091, precision: 0.9090909090909091, f_beta: 0.9090909090909091\n",
            "train: step: 176, loss: 0.535427987575531, acc: 0.9140625, recall: 0.9508196721311475, precision: 0.8787878787878788, f_beta: 0.9133858267716536\n",
            "train: step: 177, loss: 0.5529855489730835, acc: 0.90625, recall: 0.8833333333333333, precision: 0.9137931034482759, f_beta: 0.8983050847457628\n",
            "train: step: 178, loss: 0.4911155104637146, acc: 0.9140625, recall: 0.9322033898305084, precision: 0.8870967741935484, f_beta: 0.9090909090909092\n",
            "train: step: 179, loss: 0.5999972820281982, acc: 0.8828125, recall: 0.9016393442622951, precision: 0.859375, f_beta: 0.88\n",
            "train: step: 180, loss: 0.43944716453552246, acc: 0.8984375, recall: 0.8679245283018868, precision: 0.8846153846153846, f_beta: 0.8761904761904762\n",
            "train: step: 181, loss: 0.5615960955619812, acc: 0.8515625, recall: 0.8571428571428571, precision: 0.8135593220338984, f_beta: 0.8347826086956522\n",
            "train: step: 182, loss: 0.7503105401992798, acc: 0.828125, recall: 0.8024691358024691, precision: 0.9154929577464789, f_beta: 0.8552631578947367\n",
            "train: step: 183, loss: 0.6197472214698792, acc: 0.84375, recall: 0.7846153846153846, precision: 0.8947368421052632, f_beta: 0.8360655737704918\n",
            "train: step: 184, loss: 0.5357487201690674, acc: 0.875, recall: 0.8307692307692308, precision: 0.9152542372881356, f_beta: 0.870967741935484\n",
            "train: step: 185, loss: 0.6152498722076416, acc: 0.8515625, recall: 0.875, precision: 0.863013698630137, f_beta: 0.8689655172413793\n",
            "train: step: 186, loss: 0.6327342987060547, acc: 0.84375, recall: 0.9027777777777778, precision: 0.8333333333333334, f_beta: 0.8666666666666667\n",
            "train: step: 187, loss: 0.5055375099182129, acc: 0.8984375, recall: 0.8703703703703703, precision: 0.8867924528301887, f_beta: 0.8785046728971964\n",
            "train: step: 188, loss: 0.5580718517303467, acc: 0.890625, recall: 0.9253731343283582, precision: 0.8732394366197183, f_beta: 0.8985507246376812\n",
            "train: step: 189, loss: 0.6065419912338257, acc: 0.84375, recall: 0.9076923076923077, precision: 0.8082191780821918, f_beta: 0.8550724637681159\n",
            "train: step: 190, loss: 0.6559553146362305, acc: 0.9140625, recall: 0.9661016949152542, precision: 0.8636363636363636, f_beta: 0.912\n",
            "train: step: 191, loss: 0.393596887588501, acc: 0.90625, recall: 0.9230769230769231, precision: 0.8955223880597015, f_beta: 0.9090909090909091\n",
            "train: step: 192, loss: 0.840491771697998, acc: 0.8125, recall: 0.8805970149253731, precision: 0.7866666666666666, f_beta: 0.8309859154929576\n",
            "train: step: 193, loss: 0.5228764414787292, acc: 0.859375, recall: 0.8783783783783784, precision: 0.8783783783783784, f_beta: 0.8783783783783784\n",
            "train: step: 194, loss: 0.4834159016609192, acc: 0.921875, recall: 0.8571428571428571, precision: 0.9818181818181818, f_beta: 0.9152542372881356\n",
            "train: step: 195, loss: 0.6168162226676941, acc: 0.8515625, recall: 0.8253968253968254, precision: 0.8666666666666667, f_beta: 0.8455284552845528\n",
            "train: step: 196, loss: 0.7148038148880005, acc: 0.828125, recall: 0.7638888888888888, precision: 0.9166666666666666, f_beta: 0.8333333333333334\n",
            "train: step: 197, loss: 0.42352068424224854, acc: 0.921875, recall: 0.9090909090909091, precision: 0.9375, f_beta: 0.923076923076923\n",
            "train: step: 198, loss: 0.5246766805648804, acc: 0.9140625, recall: 0.88, precision: 0.9705882352941176, f_beta: 0.9230769230769231\n",
            "train: step: 199, loss: 0.5456255674362183, acc: 0.8671875, recall: 0.8428571428571429, precision: 0.9076923076923077, f_beta: 0.8740740740740741\n",
            "train: step: 200, loss: 0.5702004432678223, acc: 0.90625, recall: 0.927536231884058, precision: 0.9014084507042254, f_beta: 0.9142857142857144\n",
            "\n",
            "Evaluation:\n",
            "2019-10-16T03:23:50.272640, step: 200, loss: 0.6238189354920999, acc: 0.8645833333333334,precision: 0.8393811062027765, recall: 0.9046937832900138, f_beta: 0.8698583053409349\n",
            "Saved model checkpoint to ../model/adversarialLSTM/model/my-model-200\n",
            "\n",
            "train: step: 201, loss: 0.7178648114204407, acc: 0.8359375, recall: 0.9142857142857143, precision: 0.810126582278481, f_beta: 0.8590604026845637\n",
            "train: step: 202, loss: 0.5652762651443481, acc: 0.8671875, recall: 0.9375, precision: 0.821917808219178, f_beta: 0.8759124087591241\n",
            "train: step: 203, loss: 0.6491806507110596, acc: 0.8828125, recall: 0.9821428571428571, precision: 0.7971014492753623, f_beta: 0.8799999999999999\n",
            "train: step: 204, loss: 0.5091336369514465, acc: 0.8984375, recall: 0.9594594594594594, precision: 0.8765432098765432, f_beta: 0.9161290322580644\n",
            "train: step: 205, loss: 0.5044842958450317, acc: 0.9375, recall: 0.9821428571428571, precision: 0.8870967741935484, f_beta: 0.9322033898305085\n",
            "train: step: 206, loss: 0.712154746055603, acc: 0.8515625, recall: 0.8524590163934426, precision: 0.8387096774193549, f_beta: 0.8455284552845529\n",
            "train: step: 207, loss: 0.5703743696212769, acc: 0.875, recall: 0.90625, precision: 0.8529411764705882, f_beta: 0.8787878787878787\n",
            "train: step: 208, loss: 0.533408522605896, acc: 0.8984375, recall: 0.9242424242424242, precision: 0.8840579710144928, f_beta: 0.9037037037037037\n",
            "train: step: 209, loss: 0.7319686412811279, acc: 0.84375, recall: 0.821917808219178, precision: 0.8955223880597015, f_beta: 0.8571428571428571\n",
            "train: step: 210, loss: 0.6856884956359863, acc: 0.8203125, recall: 0.7166666666666667, precision: 0.8775510204081632, f_beta: 0.7889908256880733\n",
            "train: step: 211, loss: 0.6381075978279114, acc: 0.8515625, recall: 0.7536231884057971, precision: 0.9629629629629629, f_beta: 0.8455284552845528\n",
            "train: step: 212, loss: 0.5435792207717896, acc: 0.8984375, recall: 0.8709677419354839, precision: 0.9152542372881356, f_beta: 0.8925619834710744\n",
            "train: step: 213, loss: 0.6356717944145203, acc: 0.8671875, recall: 0.8309859154929577, precision: 0.921875, f_beta: 0.874074074074074\n",
            "train: step: 214, loss: 0.5959289073944092, acc: 0.90625, recall: 0.8524590163934426, precision: 0.9454545454545454, f_beta: 0.8965517241379309\n",
            "train: step: 215, loss: 0.4996129274368286, acc: 0.8828125, recall: 0.890625, precision: 0.8769230769230769, f_beta: 0.883720930232558\n",
            "train: step: 216, loss: 0.5698801279067993, acc: 0.890625, recall: 0.9836065573770492, precision: 0.821917808219178, f_beta: 0.8955223880597014\n",
            "train: step: 217, loss: 0.6420261859893799, acc: 0.875, recall: 0.9230769230769231, precision: 0.8450704225352113, f_beta: 0.8823529411764706\n",
            "train: step: 218, loss: 0.6323075294494629, acc: 0.8515625, recall: 0.9166666666666666, precision: 0.7971014492753623, f_beta: 0.8527131782945736\n",
            "train: step: 219, loss: 0.5767723321914673, acc: 0.875, recall: 0.9420289855072463, precision: 0.8441558441558441, f_beta: 0.8904109589041096\n",
            "train: step: 220, loss: 0.6377890706062317, acc: 0.8515625, recall: 0.9411764705882353, precision: 0.810126582278481, f_beta: 0.8707482993197279\n",
            "train: step: 221, loss: 0.5049759745597839, acc: 0.890625, recall: 0.9384615384615385, precision: 0.8591549295774648, f_beta: 0.8970588235294118\n",
            "train: step: 222, loss: 0.5887818336486816, acc: 0.875, recall: 0.921875, precision: 0.8428571428571429, f_beta: 0.880597014925373\n",
            "train: step: 223, loss: 0.41121506690979004, acc: 0.921875, recall: 0.9508196721311475, precision: 0.8923076923076924, f_beta: 0.9206349206349206\n",
            "train: step: 224, loss: 0.4677272439002991, acc: 0.90625, recall: 0.890625, precision: 0.9193548387096774, f_beta: 0.9047619047619047\n",
            "train: step: 225, loss: 0.534130334854126, acc: 0.890625, recall: 0.85, precision: 0.9107142857142857, f_beta: 0.8793103448275861\n",
            "train: step: 226, loss: 0.7191156148910522, acc: 0.8515625, recall: 0.7464788732394366, precision: 0.9814814814814815, f_beta: 0.848\n",
            "train: step: 227, loss: 0.83866286277771, acc: 0.8203125, recall: 0.7857142857142857, precision: 0.873015873015873, f_beta: 0.8270676691729324\n",
            "train: step: 228, loss: 0.8971260786056519, acc: 0.8203125, recall: 0.7794117647058824, precision: 0.8688524590163934, f_beta: 0.8217054263565892\n",
            "train: step: 229, loss: 0.7865572571754456, acc: 0.8125, recall: 0.8166666666666667, precision: 0.7903225806451613, f_beta: 0.8032786885245902\n",
            "train: step: 230, loss: 0.6423405408859253, acc: 0.859375, recall: 0.8793103448275862, precision: 0.8225806451612904, f_beta: 0.8500000000000001\n",
            "train: step: 231, loss: 0.6273655295372009, acc: 0.890625, recall: 0.9215686274509803, precision: 0.8245614035087719, f_beta: 0.8703703703703703\n",
            "train: step: 232, loss: 0.4675600230693817, acc: 0.8984375, recall: 0.8985507246376812, precision: 0.9117647058823529, f_beta: 0.9051094890510949\n",
            "train: step: 233, loss: 0.6359498500823975, acc: 0.90625, recall: 0.9032258064516129, precision: 0.9032258064516129, f_beta: 0.9032258064516129\n",
            "train: step: 234, loss: 0.7522766590118408, acc: 0.84375, recall: 0.8548387096774194, precision: 0.828125, f_beta: 0.8412698412698412\n",
            "train: step: 235, loss: 0.5719157457351685, acc: 0.875, recall: 0.9310344827586207, precision: 0.8181818181818182, f_beta: 0.8709677419354839\n",
            "train: step: 236, loss: 0.5477907657623291, acc: 0.890625, recall: 0.890625, precision: 0.890625, f_beta: 0.890625\n",
            "train: step: 237, loss: 0.5552589893341064, acc: 0.8828125, recall: 0.875, precision: 0.8888888888888888, f_beta: 0.8818897637795274\n",
            "train: step: 238, loss: 0.6621721982955933, acc: 0.8515625, recall: 0.7910447761194029, precision: 0.9137931034482759, f_beta: 0.848\n",
            "train: step: 239, loss: 0.7124277353286743, acc: 0.8515625, recall: 0.8, precision: 0.896551724137931, f_beta: 0.8455284552845529\n",
            "train: step: 240, loss: 0.46692806482315063, acc: 0.890625, recall: 0.8852459016393442, precision: 0.8852459016393442, f_beta: 0.8852459016393442\n",
            "train: step: 241, loss: 0.47154492139816284, acc: 0.8828125, recall: 0.8513513513513513, precision: 0.9402985074626866, f_beta: 0.8936170212765957\n",
            "train: step: 242, loss: 0.5513381361961365, acc: 0.875, recall: 0.873015873015873, precision: 0.873015873015873, f_beta: 0.8730158730158731\n",
            "train: step: 243, loss: 0.6203312873840332, acc: 0.8515625, recall: 0.819672131147541, precision: 0.8620689655172413, f_beta: 0.8403361344537814\n",
            "train: step: 244, loss: 0.6045736074447632, acc: 0.8671875, recall: 0.8285714285714286, precision: 0.9206349206349206, f_beta: 0.8721804511278196\n",
            "train: step: 245, loss: 0.6057302355766296, acc: 0.875, recall: 0.8939393939393939, precision: 0.8676470588235294, f_beta: 0.8805970149253731\n",
            "train: step: 246, loss: 0.6204990744590759, acc: 0.859375, recall: 0.9107142857142857, precision: 0.796875, f_beta: 0.85\n",
            "train: step: 247, loss: 0.5964915752410889, acc: 0.8828125, recall: 0.9242424242424242, precision: 0.8591549295774648, f_beta: 0.8905109489051095\n",
            "train: step: 248, loss: 0.4633045196533203, acc: 0.921875, recall: 0.9245283018867925, precision: 0.8909090909090909, f_beta: 0.9074074074074073\n",
            "train: step: 249, loss: 0.5252641439437866, acc: 0.8828125, recall: 0.8615384615384616, precision: 0.9032258064516129, f_beta: 0.8818897637795274\n",
            "train: step: 250, loss: 0.5554684996604919, acc: 0.8828125, recall: 0.9649122807017544, precision: 0.8088235294117647, f_beta: 0.88\n",
            "train: step: 251, loss: 0.6627855896949768, acc: 0.859375, recall: 0.8387096774193549, precision: 0.8666666666666667, f_beta: 0.8524590163934426\n",
            "train: step: 252, loss: 0.5987522602081299, acc: 0.859375, recall: 0.8805970149253731, precision: 0.855072463768116, f_beta: 0.8676470588235295\n",
            "train: step: 253, loss: 0.5884032249450684, acc: 0.875, recall: 0.803030303030303, precision: 0.9464285714285714, f_beta: 0.8688524590163934\n",
            "train: step: 254, loss: 0.5915395021438599, acc: 0.859375, recall: 0.8260869565217391, precision: 0.9047619047619048, f_beta: 0.8636363636363636\n",
            "train: step: 255, loss: 0.7002342343330383, acc: 0.8515625, recall: 0.8059701492537313, precision: 0.9, f_beta: 0.8503937007874016\n",
            "train: step: 256, loss: 0.57252436876297, acc: 0.8828125, recall: 0.8840579710144928, precision: 0.8970588235294118, f_beta: 0.8905109489051095\n",
            "train: step: 257, loss: 0.8098462820053101, acc: 0.8359375, recall: 0.746031746031746, precision: 0.9038461538461539, f_beta: 0.8173913043478261\n",
            "train: step: 258, loss: 0.3920949101448059, acc: 0.9140625, recall: 0.9516129032258065, precision: 0.8805970149253731, f_beta: 0.9147286821705426\n",
            "train: step: 259, loss: 0.49607306718826294, acc: 0.8984375, recall: 0.9384615384615385, precision: 0.8714285714285714, f_beta: 0.9037037037037037\n",
            "train: step: 260, loss: 0.5132490992546082, acc: 0.9140625, recall: 0.9672131147540983, precision: 0.8676470588235294, f_beta: 0.9147286821705426\n",
            "train: step: 261, loss: 0.6700942516326904, acc: 0.8828125, recall: 0.9117647058823529, precision: 0.8732394366197183, f_beta: 0.8920863309352517\n",
            "train: step: 262, loss: 0.5912262201309204, acc: 0.8671875, recall: 0.9178082191780822, precision: 0.8589743589743589, f_beta: 0.8874172185430463\n",
            "train: step: 263, loss: 0.6856183409690857, acc: 0.8671875, recall: 0.9402985074626866, precision: 0.8289473684210527, f_beta: 0.8811188811188811\n",
            "train: step: 264, loss: 0.5727229118347168, acc: 0.8984375, recall: 0.9242424242424242, precision: 0.8840579710144928, f_beta: 0.9037037037037037\n",
            "train: step: 265, loss: 0.7484523057937622, acc: 0.84375, recall: 0.8360655737704918, precision: 0.8360655737704918, f_beta: 0.8360655737704918\n",
            "train: step: 266, loss: 0.575702428817749, acc: 0.8828125, recall: 0.873015873015873, precision: 0.8870967741935484, f_beta: 0.88\n",
            "train: step: 267, loss: 0.5093771815299988, acc: 0.890625, recall: 0.873015873015873, precision: 0.9016393442622951, f_beta: 0.8870967741935485\n",
            "train: step: 268, loss: 0.3948507606983185, acc: 0.9296875, recall: 0.9154929577464789, precision: 0.9558823529411765, f_beta: 0.9352517985611511\n",
            "train: step: 269, loss: 0.5502419471740723, acc: 0.875, recall: 0.875, precision: 0.875, f_beta: 0.875\n",
            "train: step: 270, loss: 0.7490018606185913, acc: 0.8359375, recall: 0.7538461538461538, precision: 0.9074074074074074, f_beta: 0.8235294117647058\n",
            "train: step: 271, loss: 0.5417076945304871, acc: 0.875, recall: 0.8714285714285714, precision: 0.8970588235294118, f_beta: 0.8840579710144928\n",
            "train: step: 272, loss: 0.48144444823265076, acc: 0.8984375, recall: 0.9137931034482759, precision: 0.8688524590163934, f_beta: 0.8907563025210085\n",
            "train: step: 273, loss: 0.641327977180481, acc: 0.8359375, recall: 0.9032258064516129, precision: 0.7887323943661971, f_beta: 0.8421052631578947\n",
            "train: step: 274, loss: 0.684515118598938, acc: 0.8515625, recall: 0.8955223880597015, precision: 0.8333333333333334, f_beta: 0.8633093525179857\n",
            "train: step: 275, loss: 0.9090298414230347, acc: 0.765625, recall: 0.8305084745762712, precision: 0.7101449275362319, f_beta: 0.7656250000000001\n",
            "train: step: 276, loss: 0.6699517965316772, acc: 0.875, recall: 0.8620689655172413, precision: 0.8620689655172413, f_beta: 0.8620689655172413\n",
            "train: step: 277, loss: 0.5858215093612671, acc: 0.8671875, recall: 0.9032258064516129, precision: 0.835820895522388, f_beta: 0.8682170542635659\n",
            "train: step: 278, loss: 0.5775818824768066, acc: 0.8984375, recall: 0.9152542372881356, precision: 0.8709677419354839, f_beta: 0.8925619834710744\n",
            "train: step: 279, loss: 0.578580915927887, acc: 0.8984375, recall: 0.8666666666666667, precision: 0.9122807017543859, f_beta: 0.8888888888888888\n",
            "train: step: 280, loss: 0.5988146066665649, acc: 0.8671875, recall: 0.8787878787878788, precision: 0.8656716417910447, f_beta: 0.8721804511278195\n",
            "train: step: 281, loss: 0.5221817493438721, acc: 0.8984375, recall: 0.9642857142857143, precision: 0.8307692307692308, f_beta: 0.8925619834710744\n",
            "train: step: 282, loss: 0.6134251356124878, acc: 0.890625, recall: 0.8524590163934426, precision: 0.9122807017543859, f_beta: 0.8813559322033898\n",
            "train: step: 283, loss: 0.5395275354385376, acc: 0.90625, recall: 0.9137931034482759, precision: 0.8833333333333333, f_beta: 0.8983050847457628\n",
            "train: step: 284, loss: 0.6539351344108582, acc: 0.8359375, recall: 0.8412698412698413, precision: 0.828125, f_beta: 0.8346456692913385\n",
            "train: step: 285, loss: 0.6255893707275391, acc: 0.8671875, recall: 0.875, precision: 0.8305084745762712, f_beta: 0.8521739130434782\n",
            "train: step: 286, loss: 0.45426779985427856, acc: 0.9453125, recall: 0.9253731343283582, precision: 0.96875, f_beta: 0.9465648854961832\n",
            "train: step: 287, loss: 0.4136676788330078, acc: 0.9140625, recall: 0.9166666666666666, precision: 0.9016393442622951, f_beta: 0.9090909090909091\n",
            "train: step: 288, loss: 0.5490862727165222, acc: 0.875, recall: 0.8225806451612904, precision: 0.9107142857142857, f_beta: 0.864406779661017\n",
            "train: step: 289, loss: 0.6857724785804749, acc: 0.859375, recall: 0.8387096774193549, precision: 0.8666666666666667, f_beta: 0.8524590163934426\n",
            "train: step: 290, loss: 0.6467819213867188, acc: 0.8515625, recall: 0.8571428571428571, precision: 0.8695652173913043, f_beta: 0.8633093525179856\n",
            "train: step: 291, loss: 0.5952877402305603, acc: 0.875, recall: 0.8135593220338984, precision: 0.9056603773584906, f_beta: 0.8571428571428572\n",
            "train: step: 292, loss: 0.3464224934577942, acc: 0.9453125, recall: 0.9558823529411765, precision: 0.9420289855072463, f_beta: 0.9489051094890512\n",
            "train: step: 293, loss: 0.4287801682949066, acc: 0.921875, recall: 0.90625, precision: 0.9354838709677419, f_beta: 0.9206349206349206\n",
            "train: step: 294, loss: 0.4550321102142334, acc: 0.9140625, recall: 0.9433962264150944, precision: 0.8620689655172413, f_beta: 0.9009009009009009\n",
            "train: step: 295, loss: 0.794380784034729, acc: 0.8203125, recall: 0.921875, precision: 0.7662337662337663, f_beta: 0.8368794326241135\n",
            "train: step: 296, loss: 0.7461346983909607, acc: 0.859375, recall: 0.9180327868852459, precision: 0.8115942028985508, f_beta: 0.8615384615384616\n",
            "train: step: 297, loss: 0.6366187930107117, acc: 0.859375, recall: 0.9322033898305084, precision: 0.7971014492753623, f_beta: 0.859375\n",
            "train: step: 298, loss: 0.5761797428131104, acc: 0.875, recall: 0.8955223880597015, precision: 0.8695652173913043, f_beta: 0.8823529411764706\n",
            "train: step: 299, loss: 0.6102150678634644, acc: 0.8671875, recall: 0.8333333333333334, precision: 0.9016393442622951, f_beta: 0.8661417322834646\n",
            "train: step: 300, loss: 0.5276632905006409, acc: 0.8984375, recall: 0.8805970149253731, precision: 0.921875, f_beta: 0.9007633587786259\n",
            "\n",
            "Evaluation:\n",
            "2019-10-16T03:32:01.479441, step: 300, loss: 0.6160021454860003, acc: 0.8752003205128205,precision: 0.9060881403194971, recall: 0.8382281974378196, f_beta: 0.8699805628464995\n",
            "Saved model checkpoint to ../model/adversarialLSTM/model/my-model-300\n",
            "\n",
            "train: step: 301, loss: 0.38736751675605774, acc: 0.9140625, recall: 0.9393939393939394, precision: 0.8985507246376812, f_beta: 0.9185185185185185\n",
            "train: step: 302, loss: 0.4397437572479248, acc: 0.8984375, recall: 0.896551724137931, precision: 0.8813559322033898, f_beta: 0.888888888888889\n",
            "train: step: 303, loss: 0.4426478147506714, acc: 0.8984375, recall: 0.8888888888888888, precision: 0.9032258064516129, f_beta: 0.8959999999999999\n",
            "train: step: 304, loss: 0.529671311378479, acc: 0.890625, recall: 0.8571428571428571, precision: 0.9152542372881356, f_beta: 0.8852459016393444\n",
            "train: step: 305, loss: 0.6495265960693359, acc: 0.8515625, recall: 0.8088235294117647, precision: 0.9016393442622951, f_beta: 0.8527131782945736\n",
            "train: step: 306, loss: 0.6399235725402832, acc: 0.8671875, recall: 0.8125, precision: 0.9122807017543859, f_beta: 0.859504132231405\n",
            "train: step: 307, loss: 0.7021945714950562, acc: 0.8359375, recall: 0.8064516129032258, precision: 0.847457627118644, f_beta: 0.8264462809917354\n",
            "train: step: 308, loss: 0.47458726167678833, acc: 0.9140625, recall: 0.967741935483871, precision: 0.8695652173913043, f_beta: 0.9160305343511451\n",
            "train: step: 309, loss: 0.6492726802825928, acc: 0.859375, recall: 0.835820895522388, precision: 0.8888888888888888, f_beta: 0.8615384615384615\n",
            "train: step: 310, loss: 0.5311715602874756, acc: 0.890625, recall: 0.9130434782608695, precision: 0.8873239436619719, f_beta: 0.9\n",
            "train: step: 311, loss: 0.6376152634620667, acc: 0.84375, recall: 0.8571428571428571, precision: 0.8307692307692308, f_beta: 0.84375\n",
            "train: step: 312, loss: 0.4917934834957123, acc: 0.890625, recall: 0.9538461538461539, precision: 0.8493150684931506, f_beta: 0.8985507246376813\n",
            "start training model\n",
            "train: step: 313, loss: 0.48841702938079834, acc: 0.8984375, recall: 0.9298245614035088, precision: 0.8548387096774194, f_beta: 0.8907563025210085\n",
            "train: step: 314, loss: 0.48524513840675354, acc: 0.8828125, recall: 0.9130434782608695, precision: 0.875, f_beta: 0.8936170212765957\n",
            "train: step: 315, loss: 0.4281671345233917, acc: 0.8984375, recall: 0.855072463768116, precision: 0.9516129032258065, f_beta: 0.900763358778626\n",
            "train: step: 316, loss: 0.5215435028076172, acc: 0.890625, recall: 0.8985507246376812, precision: 0.8985507246376812, f_beta: 0.8985507246376812\n",
            "train: step: 317, loss: 0.6531398296356201, acc: 0.859375, recall: 0.9090909090909091, precision: 0.8333333333333334, f_beta: 0.8695652173913043\n",
            "train: step: 318, loss: 0.44264060258865356, acc: 0.8984375, recall: 0.9117647058823529, precision: 0.8985507246376812, f_beta: 0.9051094890510949\n",
            "train: step: 319, loss: 0.4868069887161255, acc: 0.90625, recall: 0.9154929577464789, precision: 0.9154929577464789, f_beta: 0.9154929577464789\n",
            "train: step: 320, loss: 0.6728283166885376, acc: 0.859375, recall: 0.8153846153846154, precision: 0.8983050847457628, f_beta: 0.8548387096774194\n",
            "train: step: 321, loss: 0.5301727056503296, acc: 0.8515625, recall: 0.7846153846153846, precision: 0.9107142857142857, f_beta: 0.8429752066115703\n",
            "train: step: 322, loss: 0.5871044397354126, acc: 0.890625, recall: 0.88, precision: 0.8461538461538461, f_beta: 0.8627450980392156\n",
            "train: step: 323, loss: 0.4318201541900635, acc: 0.9453125, recall: 0.9074074074074074, precision: 0.9607843137254902, f_beta: 0.9333333333333333\n",
            "train: step: 324, loss: 0.6820579767227173, acc: 0.8515625, recall: 0.9122807017543859, precision: 0.7878787878787878, f_beta: 0.8455284552845528\n",
            "train: step: 325, loss: 0.6000882387161255, acc: 0.8984375, recall: 0.9027777777777778, precision: 0.9154929577464789, f_beta: 0.9090909090909091\n",
            "train: step: 326, loss: 0.45078718662261963, acc: 0.9140625, recall: 1.0, precision: 0.8428571428571429, f_beta: 0.9147286821705426\n",
            "train: step: 327, loss: 0.6364948749542236, acc: 0.8828125, recall: 0.8666666666666667, precision: 0.8813559322033898, f_beta: 0.8739495798319329\n",
            "train: step: 328, loss: 0.5681570768356323, acc: 0.921875, recall: 0.9242424242424242, precision: 0.9242424242424242, f_beta: 0.9242424242424242\n",
            "train: step: 329, loss: 0.8414160013198853, acc: 0.8515625, recall: 0.8636363636363636, precision: 0.8507462686567164, f_beta: 0.8571428571428571\n",
            "train: step: 330, loss: 0.4864826798439026, acc: 0.9296875, recall: 0.9193548387096774, precision: 0.9344262295081968, f_beta: 0.9268292682926829\n",
            "train: step: 331, loss: 0.44905024766921997, acc: 0.875, recall: 0.9016393442622951, precision: 0.8461538461538461, f_beta: 0.873015873015873\n",
            "train: step: 332, loss: 0.5028384923934937, acc: 0.8828125, recall: 0.873015873015873, precision: 0.8870967741935484, f_beta: 0.88\n",
            "train: step: 333, loss: 0.41566038131713867, acc: 0.9140625, recall: 0.90625, precision: 0.9206349206349206, f_beta: 0.9133858267716536\n",
            "train: step: 334, loss: 0.5831180810928345, acc: 0.8828125, recall: 0.8656716417910447, precision: 0.90625, f_beta: 0.8854961832061069\n",
            "train: step: 335, loss: 0.47598668932914734, acc: 0.9140625, recall: 0.9253731343283582, precision: 0.9117647058823529, f_beta: 0.9185185185185185\n",
            "train: step: 336, loss: 0.6015176773071289, acc: 0.8828125, recall: 0.8904109589041096, precision: 0.9027777777777778, f_beta: 0.896551724137931\n",
            "train: step: 337, loss: 0.4361244738101959, acc: 0.9453125, recall: 0.9393939393939394, precision: 0.9538461538461539, f_beta: 0.9465648854961831\n",
            "train: step: 338, loss: 0.5881341695785522, acc: 0.8984375, recall: 0.8852459016393442, precision: 0.9, f_beta: 0.8925619834710743\n",
            "train: step: 339, loss: 0.7083960771560669, acc: 0.84375, recall: 0.8840579710144928, precision: 0.8356164383561644, f_beta: 0.8591549295774648\n",
            "train: step: 340, loss: 0.6303952932357788, acc: 0.828125, recall: 0.8448275862068966, precision: 0.7903225806451613, f_beta: 0.8166666666666667\n",
            "train: step: 341, loss: 0.5683646202087402, acc: 0.890625, recall: 0.9210526315789473, precision: 0.8974358974358975, f_beta: 0.9090909090909091\n",
            "train: step: 342, loss: 0.5634760856628418, acc: 0.8984375, recall: 0.9285714285714286, precision: 0.8904109589041096, f_beta: 0.9090909090909091\n",
            "train: step: 343, loss: 0.3647932708263397, acc: 0.9140625, recall: 0.9402985074626866, precision: 0.9, f_beta: 0.9197080291970803\n",
            "train: step: 344, loss: 0.6843589544296265, acc: 0.8515625, recall: 0.8472222222222222, precision: 0.8840579710144928, f_beta: 0.8652482269503546\n",
            "train: step: 345, loss: 0.6445803046226501, acc: 0.8671875, recall: 0.8857142857142857, precision: 0.8732394366197183, f_beta: 0.8794326241134751\n",
            "train: step: 346, loss: 0.45205530524253845, acc: 0.921875, recall: 0.9466666666666667, precision: 0.922077922077922, f_beta: 0.9342105263157895\n",
            "train: step: 347, loss: 0.35552817583084106, acc: 0.9453125, recall: 0.921875, precision: 0.9672131147540983, f_beta: 0.944\n",
            "train: step: 348, loss: 0.414537250995636, acc: 0.9375, recall: 0.961038961038961, precision: 0.9367088607594937, f_beta: 0.9487179487179488\n",
            "train: step: 349, loss: 0.5778259038925171, acc: 0.8828125, recall: 0.9253731343283582, precision: 0.8611111111111112, f_beta: 0.8920863309352519\n",
            "train: step: 350, loss: 0.46010446548461914, acc: 0.8828125, recall: 0.953125, precision: 0.8356164383561644, f_beta: 0.8905109489051095\n",
            "train: step: 351, loss: 0.3919392228126526, acc: 0.9375, recall: 1.0, precision: 0.873015873015873, f_beta: 0.9322033898305084\n",
            "train: step: 352, loss: 0.5495985150337219, acc: 0.8359375, recall: 0.8285714285714286, precision: 0.8656716417910447, f_beta: 0.8467153284671534\n",
            "train: step: 353, loss: 0.3374156355857849, acc: 0.9296875, recall: 0.9402985074626866, precision: 0.9264705882352942, f_beta: 0.9333333333333335\n",
            "train: step: 354, loss: 0.4375975728034973, acc: 0.9140625, recall: 0.9365079365079365, precision: 0.8939393939393939, f_beta: 0.9147286821705426\n",
            "train: step: 355, loss: 0.5750787258148193, acc: 0.9140625, recall: 0.8939393939393939, precision: 0.9365079365079365, f_beta: 0.9147286821705426\n",
            "train: step: 356, loss: 0.5697245597839355, acc: 0.90625, recall: 0.9032258064516129, precision: 0.9032258064516129, f_beta: 0.9032258064516129\n",
            "train: step: 357, loss: 0.5761511325836182, acc: 0.8984375, recall: 0.8840579710144928, precision: 0.9242424242424242, f_beta: 0.9037037037037037\n",
            "train: step: 358, loss: 0.5044542551040649, acc: 0.8828125, recall: 0.864406779661017, precision: 0.8793103448275862, f_beta: 0.8717948717948718\n",
            "train: step: 359, loss: 0.4925417900085449, acc: 0.875, recall: 0.8529411764705882, precision: 0.90625, f_beta: 0.8787878787878787\n",
            "train: step: 360, loss: 0.41663339734077454, acc: 0.9140625, recall: 0.9354838709677419, precision: 0.8923076923076924, f_beta: 0.9133858267716536\n",
            "train: step: 361, loss: 0.49964404106140137, acc: 0.890625, recall: 0.8253968253968254, precision: 0.9454545454545454, f_beta: 0.8813559322033897\n",
            "train: step: 362, loss: 0.6006715297698975, acc: 0.890625, recall: 0.9285714285714286, precision: 0.8387096774193549, f_beta: 0.8813559322033899\n",
            "train: step: 363, loss: 0.5436641573905945, acc: 0.90625, recall: 0.92, precision: 0.92, f_beta: 0.92\n",
            "train: step: 364, loss: 0.6552387475967407, acc: 0.875, recall: 0.9253731343283582, precision: 0.8493150684931506, f_beta: 0.8857142857142857\n",
            "train: step: 365, loss: 0.4935060143470764, acc: 0.9296875, recall: 0.9661016949152542, precision: 0.890625, f_beta: 0.9268292682926829\n",
            "train: step: 366, loss: 0.5168543457984924, acc: 0.921875, recall: 0.9137931034482759, precision: 0.9137931034482759, f_beta: 0.9137931034482759\n",
            "train: step: 367, loss: 0.49251115322113037, acc: 0.890625, recall: 0.9180327868852459, precision: 0.8615384615384616, f_beta: 0.8888888888888888\n",
            "train: step: 368, loss: 0.5319879055023193, acc: 0.8828125, recall: 0.8253968253968254, precision: 0.9285714285714286, f_beta: 0.8739495798319328\n",
            "train: step: 369, loss: 0.5703935623168945, acc: 0.875, recall: 0.9436619718309859, precision: 0.8481012658227848, f_beta: 0.8933333333333333\n",
            "train: step: 370, loss: 0.5506242513656616, acc: 0.859375, recall: 0.8356164383561644, precision: 0.9104477611940298, f_beta: 0.8714285714285713\n",
            "train: step: 371, loss: 0.48794496059417725, acc: 0.890625, recall: 0.8695652173913043, precision: 0.9230769230769231, f_beta: 0.8955223880597014\n",
            "train: step: 372, loss: 0.5646900534629822, acc: 0.8828125, recall: 0.875, precision: 0.8888888888888888, f_beta: 0.8818897637795274\n",
            "train: step: 373, loss: 0.533631443977356, acc: 0.8984375, recall: 0.9264705882352942, precision: 0.8873239436619719, f_beta: 0.906474820143885\n",
            "train: step: 374, loss: 0.47265878319740295, acc: 0.8984375, recall: 0.9056603773584906, precision: 0.8571428571428571, f_beta: 0.8807339449541285\n",
            "train: step: 375, loss: 0.39275234937667847, acc: 0.9375, recall: 0.9264705882352942, precision: 0.9545454545454546, f_beta: 0.9402985074626866\n",
            "train: step: 376, loss: 0.47013843059539795, acc: 0.9140625, recall: 0.9516129032258065, precision: 0.8805970149253731, f_beta: 0.9147286821705426\n",
            "train: step: 377, loss: 0.44120800495147705, acc: 0.8828125, recall: 0.9016393442622951, precision: 0.859375, f_beta: 0.88\n",
            "train: step: 378, loss: 0.5480901002883911, acc: 0.8515625, recall: 0.8852459016393442, precision: 0.8181818181818182, f_beta: 0.8503937007874016\n",
            "train: step: 379, loss: 0.4436905086040497, acc: 0.9140625, recall: 0.9090909090909091, precision: 0.8928571428571429, f_beta: 0.9009009009009009\n",
            "train: step: 380, loss: 0.3141241669654846, acc: 0.9609375, recall: 0.9824561403508771, precision: 0.9333333333333333, f_beta: 0.9572649572649572\n",
            "train: step: 381, loss: 0.5940074920654297, acc: 0.8984375, recall: 0.875, precision: 0.8909090909090909, f_beta: 0.8828828828828829\n",
            "train: step: 382, loss: 0.578826904296875, acc: 0.875, recall: 0.8253968253968254, precision: 0.9122807017543859, f_beta: 0.8666666666666667\n",
            "train: step: 383, loss: 0.38143351674079895, acc: 0.9375, recall: 0.9272727272727272, precision: 0.9272727272727272, f_beta: 0.9272727272727272\n",
            "train: step: 384, loss: 0.4301237463951111, acc: 0.9140625, recall: 0.9193548387096774, precision: 0.9047619047619048, f_beta: 0.912\n",
            "train: step: 385, loss: 0.5434097051620483, acc: 0.90625, recall: 0.859375, precision: 0.9482758620689655, f_beta: 0.9016393442622951\n",
            "train: step: 386, loss: 0.3676970303058624, acc: 0.9140625, recall: 0.8833333333333333, precision: 0.9298245614035088, f_beta: 0.905982905982906\n",
            "train: step: 387, loss: 0.621589183807373, acc: 0.875, recall: 0.8166666666666667, precision: 0.9074074074074074, f_beta: 0.8596491228070176\n",
            "train: step: 388, loss: 0.33215945959091187, acc: 0.921875, recall: 0.9137931034482759, precision: 0.9137931034482759, f_beta: 0.9137931034482759\n",
            "train: step: 389, loss: 0.5508973002433777, acc: 0.8828125, recall: 0.8955223880597015, precision: 0.8823529411764706, f_beta: 0.888888888888889\n",
            "train: step: 390, loss: 0.4950246214866638, acc: 0.890625, recall: 0.8656716417910447, precision: 0.9206349206349206, f_beta: 0.8923076923076922\n",
            "train: step: 391, loss: 0.6685692667961121, acc: 0.828125, recall: 0.8923076923076924, precision: 0.7945205479452054, f_beta: 0.8405797101449275\n",
            "train: step: 392, loss: 0.5330793857574463, acc: 0.8828125, recall: 0.953125, precision: 0.8356164383561644, f_beta: 0.8905109489051095\n",
            "train: step: 393, loss: 0.3218475580215454, acc: 0.953125, recall: 0.9444444444444444, precision: 0.9714285714285714, f_beta: 0.9577464788732395\n",
            "train: step: 394, loss: 0.5229647755622864, acc: 0.8828125, recall: 0.9166666666666666, precision: 0.8461538461538461, f_beta: 0.8799999999999999\n",
            "train: step: 395, loss: 0.43583035469055176, acc: 0.8828125, recall: 0.8714285714285714, precision: 0.9104477611940298, f_beta: 0.8905109489051095\n",
            "train: step: 396, loss: 0.49362000823020935, acc: 0.875, recall: 0.9047619047619048, precision: 0.8507462686567164, f_beta: 0.8769230769230769\n",
            "train: step: 397, loss: 0.5665280222892761, acc: 0.8828125, recall: 0.8484848484848485, precision: 0.9180327868852459, f_beta: 0.8818897637795275\n",
            "train: step: 398, loss: 0.5108386278152466, acc: 0.8984375, recall: 0.9206349206349206, precision: 0.8787878787878788, f_beta: 0.8992248062015504\n",
            "train: step: 399, loss: 0.5980221033096313, acc: 0.890625, recall: 0.8709677419354839, precision: 0.9, f_beta: 0.8852459016393444\n",
            "train: step: 400, loss: 0.5285334587097168, acc: 0.8984375, recall: 0.8529411764705882, precision: 0.9508196721311475, f_beta: 0.8992248062015503\n",
            "\n",
            "Evaluation:\n",
            "2019-10-16T03:40:14.047889, step: 400, loss: 0.6040392044263009, acc: 0.8778044871794872,precision: 0.8950235007435635, recall: 0.8607874269747731, f_beta: 0.8765674878721074\n",
            "Saved model checkpoint to ../model/adversarialLSTM/model/my-model-400\n",
            "\n",
            "train: step: 401, loss: 0.5567096471786499, acc: 0.8984375, recall: 0.8833333333333333, precision: 0.8983050847457628, f_beta: 0.8907563025210085\n",
            "train: step: 402, loss: 0.48146945238113403, acc: 0.90625, recall: 0.9193548387096774, precision: 0.890625, f_beta: 0.9047619047619047\n",
            "train: step: 403, loss: 0.46069836616516113, acc: 0.8984375, recall: 0.9016393442622951, precision: 0.8870967741935484, f_beta: 0.8943089430894309\n",
            "train: step: 404, loss: 0.5575197339057922, acc: 0.90625, recall: 0.9076923076923077, precision: 0.9076923076923077, f_beta: 0.9076923076923076\n",
            "train: step: 405, loss: 0.5406399369239807, acc: 0.890625, recall: 0.9464285714285714, precision: 0.828125, f_beta: 0.8833333333333333\n",
            "train: step: 406, loss: 0.6046655178070068, acc: 0.8671875, recall: 0.9180327868852459, precision: 0.8235294117647058, f_beta: 0.868217054263566\n",
            "train: step: 407, loss: 0.5718356966972351, acc: 0.8828125, recall: 0.890625, precision: 0.8769230769230769, f_beta: 0.883720930232558\n",
            "train: step: 408, loss: 0.48562872409820557, acc: 0.921875, recall: 0.8970588235294118, precision: 0.953125, f_beta: 0.9242424242424244\n",
            "train: step: 409, loss: 0.425316721200943, acc: 0.9140625, recall: 0.9607843137254902, precision: 0.8448275862068966, f_beta: 0.8990825688073395\n",
            "train: step: 410, loss: 0.6713256239891052, acc: 0.9140625, recall: 0.8923076923076924, precision: 0.9354838709677419, f_beta: 0.9133858267716536\n",
            "train: step: 411, loss: 0.47557857632637024, acc: 0.90625, recall: 0.8947368421052632, precision: 0.8947368421052632, f_beta: 0.8947368421052632\n",
            "train: step: 412, loss: 0.5186673998832703, acc: 0.9140625, recall: 0.8983050847457628, precision: 0.9137931034482759, f_beta: 0.9059829059829059\n",
            "train: step: 413, loss: 0.5574051141738892, acc: 0.8828125, recall: 0.8548387096774194, precision: 0.8983050847457628, f_beta: 0.8760330578512397\n",
            "train: step: 414, loss: 0.42298072576522827, acc: 0.90625, recall: 0.8732394366197183, precision: 0.9538461538461539, f_beta: 0.9117647058823529\n",
            "train: step: 415, loss: 0.5941504240036011, acc: 0.8828125, recall: 0.8714285714285714, precision: 0.9104477611940298, f_beta: 0.8905109489051095\n",
            "train: step: 416, loss: 0.37580129504203796, acc: 0.9453125, recall: 0.9558823529411765, precision: 0.9420289855072463, f_beta: 0.9489051094890512\n",
            "train: step: 417, loss: 0.5990378856658936, acc: 0.84375, recall: 0.90625, precision: 0.8055555555555556, f_beta: 0.8529411764705882\n",
            "train: step: 418, loss: 0.49153441190719604, acc: 0.890625, recall: 0.953125, precision: 0.8472222222222222, f_beta: 0.8970588235294118\n",
            "train: step: 419, loss: 0.5358128547668457, acc: 0.8828125, recall: 0.9076923076923077, precision: 0.8676470588235294, f_beta: 0.887218045112782\n",
            "train: step: 420, loss: 0.5696269273757935, acc: 0.859375, recall: 0.9056603773584906, precision: 0.7868852459016393, f_beta: 0.8421052631578947\n",
            "train: step: 421, loss: 0.49883103370666504, acc: 0.8828125, recall: 0.9090909090909091, precision: 0.8695652173913043, f_beta: 0.888888888888889\n",
            "train: step: 422, loss: 0.4945574402809143, acc: 0.8984375, recall: 0.967741935483871, precision: 0.8450704225352113, f_beta: 0.9022556390977443\n",
            "train: step: 423, loss: 0.8366615176200867, acc: 0.796875, recall: 0.6984126984126984, precision: 0.8627450980392157, f_beta: 0.7719298245614035\n",
            "train: step: 424, loss: 0.5480291843414307, acc: 0.921875, recall: 0.9, precision: 0.9545454545454546, f_beta: 0.9264705882352942\n",
            "train: step: 425, loss: 0.41541147232055664, acc: 0.9375, recall: 0.9, precision: 0.984375, f_beta: 0.9402985074626866\n",
            "train: step: 426, loss: 0.6344153881072998, acc: 0.875, recall: 0.8955223880597015, precision: 0.8695652173913043, f_beta: 0.8823529411764706\n",
            "train: step: 427, loss: 0.4345897436141968, acc: 0.90625, recall: 0.88, precision: 0.88, f_beta: 0.88\n",
            "train: step: 428, loss: 0.6427687406539917, acc: 0.8828125, recall: 0.8571428571428571, precision: 0.9, f_beta: 0.8780487804878048\n",
            "train: step: 429, loss: 0.5394373536109924, acc: 0.9140625, recall: 0.8955223880597015, precision: 0.9375, f_beta: 0.9160305343511451\n",
            "train: step: 430, loss: 0.36074310541152954, acc: 0.9140625, recall: 0.9384615384615385, precision: 0.8970588235294118, f_beta: 0.9172932330827067\n",
            "train: step: 431, loss: 0.34623605012893677, acc: 0.921875, recall: 0.9076923076923077, precision: 0.9365079365079365, f_beta: 0.9218749999999999\n",
            "train: step: 432, loss: 0.5652996301651001, acc: 0.8828125, recall: 0.9047619047619048, precision: 0.8636363636363636, f_beta: 0.8837209302325582\n",
            "train: step: 433, loss: 0.5520709753036499, acc: 0.8671875, recall: 0.9137931034482759, precision: 0.8153846153846154, f_beta: 0.8617886178861788\n",
            "train: step: 434, loss: 0.6056376695632935, acc: 0.8984375, recall: 0.9402985074626866, precision: 0.875, f_beta: 0.9064748201438849\n",
            "train: step: 435, loss: 0.4605405330657959, acc: 0.9140625, recall: 0.9571428571428572, precision: 0.8933333333333333, f_beta: 0.9241379310344828\n",
            "train: step: 436, loss: 0.6482309103012085, acc: 0.875, recall: 0.9444444444444444, precision: 0.85, f_beta: 0.8947368421052632\n",
            "train: step: 437, loss: 0.44882142543792725, acc: 0.890625, recall: 0.9047619047619048, precision: 0.8769230769230769, f_beta: 0.890625\n",
            "train: step: 438, loss: 0.5626745820045471, acc: 0.9140625, recall: 0.8793103448275862, precision: 0.9272727272727272, f_beta: 0.902654867256637\n",
            "train: step: 439, loss: 0.5590983629226685, acc: 0.8984375, recall: 0.8461538461538461, precision: 0.9482758620689655, f_beta: 0.894308943089431\n",
            "train: step: 440, loss: 0.6306760311126709, acc: 0.875, recall: 0.7857142857142857, precision: 0.9166666666666666, f_beta: 0.8461538461538461\n",
            "train: step: 441, loss: 0.40665826201438904, acc: 0.921875, recall: 0.9122807017543859, precision: 0.9122807017543859, f_beta: 0.9122807017543859\n",
            "train: step: 442, loss: 0.47884058952331543, acc: 0.9296875, recall: 0.9508196721311475, precision: 0.90625, f_beta: 0.9279999999999999\n",
            "train: step: 443, loss: 0.5102454423904419, acc: 0.90625, recall: 0.9491525423728814, precision: 0.8615384615384616, f_beta: 0.903225806451613\n",
            "train: step: 444, loss: 0.4827457666397095, acc: 0.8984375, recall: 0.8769230769230769, precision: 0.9193548387096774, f_beta: 0.8976377952755904\n",
            "train: step: 445, loss: 0.4310123324394226, acc: 0.8984375, recall: 0.8703703703703703, precision: 0.8867924528301887, f_beta: 0.8785046728971964\n",
            "train: step: 446, loss: 0.5703284740447998, acc: 0.8984375, recall: 0.8461538461538461, precision: 0.9482758620689655, f_beta: 0.894308943089431\n",
            "train: step: 447, loss: 0.40260565280914307, acc: 0.921875, recall: 0.9, precision: 0.9310344827586207, f_beta: 0.9152542372881356\n",
            "train: step: 448, loss: 0.5598973035812378, acc: 0.84375, recall: 0.8805970149253731, precision: 0.8309859154929577, f_beta: 0.8550724637681161\n",
            "train: step: 449, loss: 0.6886666417121887, acc: 0.8515625, recall: 0.859375, precision: 0.8461538461538461, f_beta: 0.8527131782945736\n",
            "train: step: 450, loss: 0.37485066056251526, acc: 0.921875, recall: 0.9142857142857143, precision: 0.9411764705882353, f_beta: 0.9275362318840579\n",
            "train: step: 451, loss: 0.5279708504676819, acc: 0.8984375, recall: 0.9682539682539683, precision: 0.8472222222222222, f_beta: 0.9037037037037037\n",
            "train: step: 452, loss: 0.47980988025665283, acc: 0.890625, recall: 0.873015873015873, precision: 0.9016393442622951, f_beta: 0.8870967741935485\n",
            "train: step: 453, loss: 0.4516667425632477, acc: 0.9140625, recall: 0.9384615384615385, precision: 0.8970588235294118, f_beta: 0.9172932330827067\n",
            "train: step: 454, loss: 0.47659921646118164, acc: 0.890625, recall: 0.9107142857142857, precision: 0.85, f_beta: 0.8793103448275861\n",
            "train: step: 455, loss: 0.5442764759063721, acc: 0.8984375, recall: 0.9428571428571428, precision: 0.88, f_beta: 0.9103448275862068\n",
            "train: step: 456, loss: 0.5862770080566406, acc: 0.875, recall: 0.8955223880597015, precision: 0.8695652173913043, f_beta: 0.8823529411764706\n",
            "train: step: 457, loss: 0.5787385702133179, acc: 0.890625, recall: 0.8524590163934426, precision: 0.9122807017543859, f_beta: 0.8813559322033898\n",
            "train: step: 458, loss: 0.5579806566238403, acc: 0.890625, recall: 0.9056603773584906, precision: 0.8421052631578947, f_beta: 0.8727272727272727\n",
            "train: step: 459, loss: 0.5300865769386292, acc: 0.8828125, recall: 0.84375, precision: 0.9152542372881356, f_beta: 0.8780487804878049\n",
            "train: step: 460, loss: 0.44731825590133667, acc: 0.90625, recall: 0.8983050847457628, precision: 0.8983050847457628, f_beta: 0.8983050847457628\n",
            "train: step: 461, loss: 0.6221749782562256, acc: 0.875, recall: 0.8333333333333334, precision: 0.9166666666666666, f_beta: 0.8730158730158729\n",
            "train: step: 462, loss: 0.45141899585723877, acc: 0.90625, recall: 0.896551724137931, precision: 0.896551724137931, f_beta: 0.896551724137931\n",
            "train: step: 463, loss: 0.7544785737991333, acc: 0.84375, recall: 0.8529411764705882, precision: 0.8529411764705882, f_beta: 0.8529411764705882\n",
            "train: step: 464, loss: 0.43632182478904724, acc: 0.9140625, recall: 0.890625, precision: 0.9344262295081968, f_beta: 0.9120000000000001\n",
            "train: step: 465, loss: 0.5039753913879395, acc: 0.8984375, recall: 0.9166666666666666, precision: 0.873015873015873, f_beta: 0.894308943089431\n",
            "train: step: 466, loss: 0.48098087310791016, acc: 0.8984375, recall: 0.9027777777777778, precision: 0.9154929577464789, f_beta: 0.9090909090909091\n",
            "train: step: 467, loss: 0.5149118900299072, acc: 0.890625, recall: 0.855072463768116, precision: 0.9365079365079365, f_beta: 0.8939393939393939\n",
            "train: step: 468, loss: 0.42300185561180115, acc: 0.9296875, recall: 0.9354838709677419, precision: 0.9206349206349206, f_beta: 0.9279999999999999\n",
            "start training model\n",
            "train: step: 469, loss: 0.43587231636047363, acc: 0.921875, recall: 0.9516129032258065, precision: 0.8939393939393939, f_beta: 0.921875\n",
            "train: step: 470, loss: 0.46554750204086304, acc: 0.9375, recall: 0.96875, precision: 0.9117647058823529, f_beta: 0.9393939393939394\n",
            "train: step: 471, loss: 0.48688051104545593, acc: 0.8984375, recall: 0.8888888888888888, precision: 0.9032258064516129, f_beta: 0.8959999999999999\n",
            "train: step: 472, loss: 0.40721869468688965, acc: 0.921875, recall: 0.9324324324324325, precision: 0.9324324324324325, f_beta: 0.9324324324324325\n",
            "train: step: 473, loss: 0.38584384322166443, acc: 0.9453125, recall: 0.9452054794520548, precision: 0.9583333333333334, f_beta: 0.9517241379310345\n",
            "train: step: 474, loss: 0.3442862629890442, acc: 0.9296875, recall: 0.9076923076923077, precision: 0.9516129032258065, f_beta: 0.9291338582677167\n",
            "train: step: 475, loss: 0.31651175022125244, acc: 0.96875, recall: 0.9583333333333334, precision: 0.9857142857142858, f_beta: 0.971830985915493\n",
            "train: step: 476, loss: 0.455520361661911, acc: 0.90625, recall: 0.9054054054054054, precision: 0.9305555555555556, f_beta: 0.9178082191780821\n",
            "train: step: 477, loss: 0.5371801257133484, acc: 0.890625, recall: 0.9538461538461539, precision: 0.8493150684931506, f_beta: 0.8985507246376813\n",
            "train: step: 478, loss: 0.400018572807312, acc: 0.9609375, recall: 0.9746835443037974, precision: 0.9625, f_beta: 0.9685534591194969\n",
            "train: step: 479, loss: 0.4720894992351532, acc: 0.90625, recall: 0.9436619718309859, precision: 0.8933333333333333, f_beta: 0.9178082191780823\n",
            "train: step: 480, loss: 0.3571443259716034, acc: 0.9140625, recall: 0.953125, precision: 0.8840579710144928, f_beta: 0.9172932330827068\n",
            "train: step: 481, loss: 0.3882908821105957, acc: 0.90625, recall: 0.9259259259259259, precision: 0.8620689655172413, f_beta: 0.8928571428571429\n",
            "train: step: 482, loss: 0.32670918107032776, acc: 0.9609375, recall: 1.0, precision: 0.9206349206349206, f_beta: 0.9586776859504132\n",
            "train: step: 483, loss: 0.4031275510787964, acc: 0.921875, recall: 0.8857142857142857, precision: 0.96875, f_beta: 0.9253731343283582\n",
            "train: step: 484, loss: 0.4743518829345703, acc: 0.8984375, recall: 0.8787878787878788, precision: 0.9206349206349206, f_beta: 0.8992248062015504\n",
            "train: step: 485, loss: 0.48129186034202576, acc: 0.875, recall: 0.7941176470588235, precision: 0.9642857142857143, f_beta: 0.8709677419354839\n",
            "train: step: 486, loss: 0.5494899153709412, acc: 0.890625, recall: 0.8571428571428571, precision: 0.8888888888888888, f_beta: 0.8727272727272727\n",
            "train: step: 487, loss: 0.43621715903282166, acc: 0.90625, recall: 0.8870967741935484, precision: 0.9166666666666666, f_beta: 0.9016393442622951\n",
            "train: step: 488, loss: 0.5485790967941284, acc: 0.8828125, recall: 0.896551724137931, precision: 0.8524590163934426, f_beta: 0.8739495798319327\n",
            "train: step: 489, loss: 0.30415624380111694, acc: 0.9609375, recall: 0.9850746268656716, precision: 0.9428571428571428, f_beta: 0.9635036496350364\n",
            "train: step: 490, loss: 0.27829375863075256, acc: 0.9609375, recall: 0.9859154929577465, precision: 0.9459459459459459, f_beta: 0.9655172413793103\n",
            "train: step: 491, loss: 0.38654470443725586, acc: 0.9375, recall: 0.9848484848484849, precision: 0.9027777777777778, f_beta: 0.9420289855072465\n",
            "train: step: 492, loss: 0.40517550706863403, acc: 0.9140625, recall: 0.9565217391304348, precision: 0.8918918918918919, f_beta: 0.9230769230769231\n",
            "train: step: 493, loss: 0.45331430435180664, acc: 0.921875, recall: 0.9090909090909091, precision: 0.9090909090909091, f_beta: 0.9090909090909091\n",
            "train: step: 494, loss: 0.36961090564727783, acc: 0.9375, recall: 0.9607843137254902, precision: 0.8909090909090909, f_beta: 0.9245283018867925\n",
            "train: step: 495, loss: 0.49003171920776367, acc: 0.8984375, recall: 0.8955223880597015, precision: 0.9090909090909091, f_beta: 0.9022556390977443\n",
            "train: step: 496, loss: 0.5550518035888672, acc: 0.8828125, recall: 0.7931034482758621, precision: 0.9387755102040817, f_beta: 0.8598130841121495\n",
            "train: step: 497, loss: 0.6779500842094421, acc: 0.875, recall: 0.8194444444444444, precision: 0.9516129032258065, f_beta: 0.8805970149253732\n",
            "train: step: 498, loss: 0.6217060089111328, acc: 0.890625, recall: 0.9113924050632911, precision: 0.9113924050632911, f_beta: 0.9113924050632911\n",
            "train: step: 499, loss: 0.4227309823036194, acc: 0.9140625, recall: 0.9253731343283582, precision: 0.9117647058823529, f_beta: 0.9185185185185185\n",
            "train: step: 500, loss: 0.4702806770801544, acc: 0.890625, recall: 0.9142857142857143, precision: 0.8888888888888888, f_beta: 0.9014084507042254\n",
            "\n",
            "Evaluation:\n",
            "2019-10-16T03:48:29.034937, step: 500, loss: 0.6198946359830025, acc: 0.8711939102564102,precision: 0.8497169550606458, recall: 0.9046688245965021, f_beta: 0.8757501229810174\n",
            "Saved model checkpoint to ../model/adversarialLSTM/model/my-model-500\n",
            "\n",
            "train: step: 501, loss: 0.47689077258110046, acc: 0.8984375, recall: 0.953125, precision: 0.8591549295774648, f_beta: 0.9037037037037037\n",
            "train: step: 502, loss: 0.3668471574783325, acc: 0.9296875, recall: 0.9253731343283582, precision: 0.9393939393939394, f_beta: 0.9323308270676692\n",
            "train: step: 503, loss: 0.6300621628761292, acc: 0.84375, recall: 0.8888888888888888, precision: 0.8115942028985508, f_beta: 0.8484848484848485\n",
            "train: step: 504, loss: 0.4404867887496948, acc: 0.921875, recall: 0.9705882352941176, precision: 0.8918918918918919, f_beta: 0.9295774647887325\n",
            "train: step: 505, loss: 0.36256861686706543, acc: 0.9296875, recall: 0.921875, precision: 0.9365079365079365, f_beta: 0.9291338582677166\n",
            "train: step: 506, loss: 0.45723798871040344, acc: 0.9140625, recall: 0.8985507246376812, precision: 0.9393939393939394, f_beta: 0.9185185185185185\n",
            "train: step: 507, loss: 0.40381547808647156, acc: 0.8984375, recall: 0.9193548387096774, precision: 0.8769230769230769, f_beta: 0.8976377952755904\n",
            "train: step: 508, loss: 0.518468976020813, acc: 0.890625, recall: 0.8461538461538461, precision: 0.9322033898305084, f_beta: 0.8870967741935484\n",
            "train: step: 509, loss: 0.49995094537734985, acc: 0.8984375, recall: 0.8333333333333334, precision: 0.9183673469387755, f_beta: 0.8737864077669903\n",
            "train: step: 510, loss: 0.40548527240753174, acc: 0.90625, recall: 0.9230769230769231, precision: 0.8955223880597015, f_beta: 0.9090909090909091\n",
            "train: step: 511, loss: 0.41959893703460693, acc: 0.9453125, recall: 0.9285714285714286, precision: 0.9454545454545454, f_beta: 0.9369369369369368\n",
            "train: step: 512, loss: 0.38155609369277954, acc: 0.9140625, recall: 0.9032258064516129, precision: 0.9180327868852459, f_beta: 0.9105691056910569\n",
            "train: step: 513, loss: 0.397914320230484, acc: 0.9140625, recall: 0.9452054794520548, precision: 0.9078947368421053, f_beta: 0.9261744966442953\n",
            "train: step: 514, loss: 0.5402970314025879, acc: 0.8828125, recall: 0.8, precision: 0.9166666666666666, f_beta: 0.854368932038835\n",
            "train: step: 515, loss: 0.7598250508308411, acc: 0.859375, recall: 0.9032258064516129, precision: 0.8235294117647058, f_beta: 0.8615384615384616\n",
            "train: step: 516, loss: 0.32218021154403687, acc: 0.9296875, recall: 0.9206349206349206, precision: 0.9354838709677419, f_beta: 0.9279999999999999\n",
            "train: step: 517, loss: 0.33756163716316223, acc: 0.9453125, recall: 0.9552238805970149, precision: 0.9411764705882353, f_beta: 0.9481481481481482\n",
            "train: step: 518, loss: 0.6213480234146118, acc: 0.8828125, recall: 0.8490566037735849, precision: 0.8653846153846154, f_beta: 0.8571428571428571\n",
            "train: step: 519, loss: 0.5453771352767944, acc: 0.8984375, recall: 0.8939393939393939, precision: 0.9076923076923077, f_beta: 0.900763358778626\n",
            "train: step: 520, loss: 0.4679213762283325, acc: 0.8828125, recall: 0.8035714285714286, precision: 0.9183673469387755, f_beta: 0.8571428571428571\n",
            "train: step: 521, loss: 0.4805014133453369, acc: 0.890625, recall: 0.9074074074074074, precision: 0.8448275862068966, f_beta: 0.875\n",
            "train: step: 522, loss: 0.49337196350097656, acc: 0.90625, recall: 0.9016393442622951, precision: 0.9016393442622951, f_beta: 0.9016393442622952\n",
            "train: step: 523, loss: 0.4989643692970276, acc: 0.8984375, recall: 0.9, precision: 0.9130434782608695, f_beta: 0.9064748201438848\n",
            "train: step: 524, loss: 0.3911018669605255, acc: 0.90625, recall: 0.9027777777777778, precision: 0.9285714285714286, f_beta: 0.9154929577464788\n",
            "train: step: 525, loss: 0.4053173065185547, acc: 0.921875, recall: 0.890625, precision: 0.95, f_beta: 0.9193548387096774\n",
            "train: step: 526, loss: 0.6248122453689575, acc: 0.8515625, recall: 0.8059701492537313, precision: 0.9, f_beta: 0.8503937007874016\n",
            "train: step: 527, loss: 0.3213081359863281, acc: 0.953125, recall: 0.9714285714285714, precision: 0.9444444444444444, f_beta: 0.9577464788732395\n",
            "train: step: 528, loss: 0.4102898836135864, acc: 0.9375, recall: 0.9655172413793104, precision: 0.9032258064516129, f_beta: 0.9333333333333333\n",
            "train: step: 529, loss: 0.7698899507522583, acc: 0.8671875, recall: 0.9117647058823529, precision: 0.8493150684931506, f_beta: 0.8794326241134752\n",
            "train: step: 530, loss: 0.48944786190986633, acc: 0.890625, recall: 0.9206349206349206, precision: 0.8656716417910447, f_beta: 0.8923076923076922\n",
            "train: step: 531, loss: 0.3354479670524597, acc: 0.921875, recall: 0.9180327868852459, precision: 0.9180327868852459, f_beta: 0.9180327868852459\n",
            "train: step: 532, loss: 0.27824026346206665, acc: 0.9453125, recall: 0.95, precision: 0.9344262295081968, f_beta: 0.9421487603305784\n",
            "train: step: 533, loss: 0.2662140727043152, acc: 0.9609375, recall: 0.9642857142857143, precision: 0.9473684210526315, f_beta: 0.9557522123893805\n",
            "train: step: 534, loss: 0.3791130781173706, acc: 0.921875, recall: 0.8548387096774194, precision: 0.9814814814814815, f_beta: 0.9137931034482759\n",
            "train: step: 535, loss: 0.5299808382987976, acc: 0.890625, recall: 0.8305084745762712, precision: 0.9245283018867925, f_beta: 0.875\n",
            "train: step: 536, loss: 0.39083972573280334, acc: 0.921875, recall: 0.9, precision: 0.9310344827586207, f_beta: 0.9152542372881356\n",
            "train: step: 537, loss: 0.4124654531478882, acc: 0.9296875, recall: 0.9322033898305084, precision: 0.9166666666666666, f_beta: 0.9243697478991596\n",
            "train: step: 538, loss: 0.49223193526268005, acc: 0.9140625, recall: 0.8823529411764706, precision: 0.9523809523809523, f_beta: 0.916030534351145\n",
            "train: step: 539, loss: 0.2853664457798004, acc: 0.953125, recall: 0.9830508474576272, precision: 0.9206349206349206, f_beta: 0.9508196721311476\n",
            "train: step: 540, loss: 0.48853105306625366, acc: 0.90625, recall: 0.9104477611940298, precision: 0.9104477611940298, f_beta: 0.9104477611940298\n",
            "train: step: 541, loss: 0.44557109475135803, acc: 0.9140625, recall: 0.95, precision: 0.8769230769230769, f_beta: 0.912\n",
            "train: step: 542, loss: 0.44289207458496094, acc: 0.9140625, recall: 0.9264705882352942, precision: 0.9130434782608695, f_beta: 0.9197080291970804\n",
            "train: step: 543, loss: 0.39707350730895996, acc: 0.9140625, recall: 0.9838709677419355, precision: 0.8591549295774648, f_beta: 0.9172932330827067\n",
            "train: step: 544, loss: 0.44949963688850403, acc: 0.9140625, recall: 0.9206349206349206, precision: 0.90625, f_beta: 0.9133858267716536\n",
            "train: step: 545, loss: 0.5135793685913086, acc: 0.890625, recall: 0.9393939393939394, precision: 0.8611111111111112, f_beta: 0.8985507246376813\n",
            "train: step: 546, loss: 0.4752016067504883, acc: 0.9375, recall: 0.96875, precision: 0.9117647058823529, f_beta: 0.9393939393939394\n",
            "train: step: 547, loss: 0.3625096082687378, acc: 0.921875, recall: 0.9154929577464789, precision: 0.9420289855072463, f_beta: 0.9285714285714286\n",
            "train: step: 548, loss: 0.47261977195739746, acc: 0.8984375, recall: 0.875, precision: 0.9180327868852459, f_beta: 0.8959999999999999\n",
            "train: step: 549, loss: 0.5734931230545044, acc: 0.8671875, recall: 0.8153846153846154, precision: 0.9137931034482759, f_beta: 0.8617886178861788\n",
            "train: step: 550, loss: 0.5097445845603943, acc: 0.8828125, recall: 0.7794117647058824, precision: 1.0, f_beta: 0.8760330578512397\n",
            "train: step: 551, loss: 0.4125666618347168, acc: 0.890625, recall: 0.890625, precision: 0.890625, f_beta: 0.890625\n",
            "train: step: 552, loss: 0.4369417428970337, acc: 0.9140625, recall: 0.9375, precision: 0.8955223880597015, f_beta: 0.9160305343511451\n",
            "train: step: 553, loss: 0.4294445514678955, acc: 0.890625, recall: 0.9402985074626866, precision: 0.863013698630137, f_beta: 0.9\n",
            "train: step: 554, loss: 0.2783138155937195, acc: 0.9609375, recall: 1.0, precision: 0.9206349206349206, f_beta: 0.9586776859504132\n",
            "train: step: 555, loss: 0.46119916439056396, acc: 0.90625, recall: 0.9516129032258065, precision: 0.8676470588235294, f_beta: 0.9076923076923077\n",
            "train: step: 556, loss: 0.5581656098365784, acc: 0.875, recall: 0.9615384615384616, precision: 0.78125, f_beta: 0.8620689655172413\n",
            "train: step: 557, loss: 0.378760427236557, acc: 0.8984375, recall: 0.9206349206349206, precision: 0.8787878787878788, f_beta: 0.8992248062015504\n",
            "train: step: 558, loss: 0.4189985394477844, acc: 0.90625, recall: 0.9344262295081968, precision: 0.8769230769230769, f_beta: 0.9047619047619049\n",
            "train: step: 559, loss: 0.5380343198776245, acc: 0.90625, recall: 0.8787878787878788, precision: 0.9354838709677419, f_beta: 0.90625\n",
            "train: step: 560, loss: 0.32868528366088867, acc: 0.9375, recall: 0.9210526315789473, precision: 0.9722222222222222, f_beta: 0.9459459459459458\n",
            "train: step: 561, loss: 0.5592033863067627, acc: 0.90625, recall: 0.9193548387096774, precision: 0.890625, f_beta: 0.9047619047619047\n",
            "train: step: 562, loss: 0.47008082270622253, acc: 0.890625, recall: 0.8548387096774194, precision: 0.9137931034482759, f_beta: 0.8833333333333333\n",
            "train: step: 563, loss: 0.41615229845046997, acc: 0.90625, recall: 0.9365079365079365, precision: 0.8805970149253731, f_beta: 0.9076923076923077\n",
            "train: step: 564, loss: 0.5802600383758545, acc: 0.890625, recall: 0.8656716417910447, precision: 0.9206349206349206, f_beta: 0.8923076923076922\n",
            "train: step: 565, loss: 0.5297306776046753, acc: 0.890625, recall: 0.8620689655172413, precision: 0.8928571428571429, f_beta: 0.8771929824561403\n",
            "train: step: 566, loss: 0.3872741460800171, acc: 0.9453125, recall: 0.9206349206349206, precision: 0.9666666666666667, f_beta: 0.943089430894309\n",
            "train: step: 567, loss: 0.4382224678993225, acc: 0.8984375, recall: 0.9464285714285714, precision: 0.8412698412698413, f_beta: 0.8907563025210083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx3mXzkuXWhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = \"this movie is full of references like mad max ii the wild one and many others the ladybug´s face it´s a clear reference or tribute to peter lorre this movie is a masterpiece we´ll talk much more about in the future\"\n",
        "\n",
        "# 注：下面两个词典要保证和当前加载的模型对应的词典是一致的\n",
        "with open(mypath/\"data/wordJson/word2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    word2idx = json.load(f)\n",
        "        \n",
        "with open(mypath/\"data/wordJson/label2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    label2idx = json.load(f)\n",
        "idx2label = {value: key for key, value in label2idx.items()}\n",
        "    \n",
        "xIds = [word2idx.get(item, word2idx[\"UNK\"]) for item in x.split(\" \")]\n",
        "if len(xIds) >= config.sequenceLength:\n",
        "    xIds = xIds[:config.sequenceLength]\n",
        "else:\n",
        "    xIds = xIds + [word2idx[\"PAD\"]] * (config.sequenceLength - len(xIds))\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
        "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False, gpu_options=gpu_options)\n",
        "    sess = tf.Session(config=session_conf)\n",
        "\n",
        "    with sess.as_default():\n",
        "        checkpoint_file = tf.train.latest_checkpoint(\"../model/adversarialLSTM/model/\")\n",
        "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
        "        saver.restore(sess, checkpoint_file)\n",
        "\n",
        "        # 获得需要喂给模型的参数，输出的结果依赖的输入值\n",
        "        inputX = graph.get_operation_by_name(\"inputX\").outputs[0]\n",
        "        dropoutKeepProb = graph.get_operation_by_name(\"dropoutKeepProb\").outputs[0]\n",
        "\n",
        "        # 获得输出的结果\n",
        "        predictions = graph.get_tensor_by_name(\"output/predictions:0\")\n",
        "\n",
        "        pred = sess.run(predictions, feed_dict={inputX: [xIds], dropoutKeepProb: 1.0})[0]\n",
        "        \n",
        "pred = [idx2label[item] for item in pred]     \n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}