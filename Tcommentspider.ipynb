{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "commentspider.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljyslyc/comment-analysis/blob/master/Tcommentspider.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNS4oBrX1MrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import json\n",
        "from lxml import etree\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "\n",
        "##调用getMerchantComment接口获取评论json\n",
        "def getShopPage(shopID,offSet):\n",
        "    simulateChromeBrowserData = {\n",
        "        'Accept':'*/*',\n",
        "        'Accept-Encoding':'gzip, deflate',\n",
        "        'Accept-Language':'zh-CN,zh;q=0.8',\n",
        "        'Connection':'keep-alive',\n",
        "        'Host':'sz.meituan.com',\n",
        "        'Referer':'http://wh.meituan.com/meishi/',\n",
        "        # 'Cookie': '_lxsdk_cuid=16212a00d8ec8-07cdb6596bad8e-178123e-1fa400-16212a00d8fc8; lat=22.780886; lng=113.906362; client-id=34908f62-ea11-4211-b60a-f62c32288b2e; uuid=9be4f96971ac4c9cab4c.1520730903.1.0.0; webloc_geo=22.527181%2C113.938582%2Cwgs84; ci=30; _lxsdk=16212a1cddfc8-011369480302e7-178123e-1fa400-16212a1cddfc8; __mta=247430459.1520730902128.1520731016684.1520731025187.5; _lxsdk_s=16212a00d8f-c83-6e-376%7C%7C9',\n",
        "        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'\n",
        "    }\n",
        "    purl='https://www.meituan.com/meishi/api/poi/getMerchantComment'\n",
        "    params = {\n",
        "        'id' : shopID,\n",
        "        'userId' : 0,\n",
        "        'offset' : offSet*10,#偏移量，也是页码信息\n",
        "        'pageSize' : 10,\n",
        "        'sortType' : 1\n",
        "    }\n",
        "    page = requests.get(purl,params=params,headers=simulateChromeBrowserData)\n",
        "    return page\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF2HtBm31MrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataClean(comment):  #只保留中文\n",
        "    pattern = re.compile(r'[^\\u4e00-\\u9fa5]')\n",
        "    commentc = re.sub(pattern, '', comment)\n",
        "    return commentc\n",
        "\n",
        "def parse_POI_Shop_Json(shop_json):\n",
        "    one_comment_data = pd.DataFrame()\n",
        "    if 'data'in shop_json:\n",
        "        for eachShopInfo in shop_json.get('data').get('comments'):\n",
        "            Comment = dataClean(eachShopInfo.get('comment'))\n",
        "            if not Comment:\n",
        "                continue\n",
        "            IsGreet = 1 if eachShopInfo.get('star')>= 30 else 0\n",
        "            one_comment_data = one_comment_data.append(pd.DataFrame({'Comment':[Comment],'IsGreet':[IsGreet]}))\n",
        "    return one_comment_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4BtlMLYy1MrS",
        "colab_type": "code",
        "outputId": "576b6b14-23ab-4ae4-d3dd-31e84d890690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Comment_datas = pd.DataFrame();\n",
        "# allCommentNum商家的最大评论数\n",
        "allCommentNum=150\n",
        "for i in range(allCommentNum//10):\n",
        "    page = getShopPage(74779,i)\n",
        "    #print(page.url)\n",
        "    # print(page.json())\n",
        "    Comment_data = parse_POI_Shop_Json(page.json())\n",
        "    Comment_datas = Comment_datas.append(Comment_data)\n",
        "print(Comment_datas)\n",
        "Comment_datas.to_csv('Comment_datas.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              Comment  IsGreet\n",
            "0                                个人吃加了个羊腿不错的一家西餐厅还会光顾        1\n",
            "0   我和男朋友是秀玉的老顾客了第一次认识就是在秀玉吃的饭他们家的环境优雅菜品味道也挺不错今天和男...        1\n",
            "0   每次来秀玉觉得心情很好因为环境不错东西又很好吃因为没有拍照的习惯只能拿朋友做的东西来凑了貌似...        1\n",
            "0   和室友一起来的这里位置很好找整个装饰也很有格调适合人们休息点了香辣咖喱鸡焗饭冰淇淋水果圣代奶...        1\n",
            "0                                  还不错吧环境优雅菜品种类多味道还可以        1\n",
            "0                               不好吃都浪费了真不知道秀玉现在怎么这么差了        0\n",
            "0                                     味道还不错环境比较好就餐很安静        1\n",
            "0                          环境特别好牛排也特别美味金桔茶特别推荐哦就餐特别推荐        1\n",
            "0                                        还可以人不多上菜快服务好        1\n",
            "0               带着弟弟们来吃的位置有点难得找人很多味道不错看来地方不同秀玉的味道差别也大        1\n",
            "0   邓平小姐姐服务态度很好因为是第一次来这里耐心给我们推荐菜品回答我们的问题这里二楼比较安静喜欢...        1\n",
            "0                                    味道好吃可以可以男朋友吃的很满意        1\n",
            "0   环境还不错主要是吃这个衣服上不会有味道饮料也很好喝不是那种用粉冲出来的服务员态度也都很好给我...        1\n",
            "0                                      味道不错套餐实惠小孩很喜欢吃        1\n",
            "0                                           味道好很喜欢吃推荐        1\n",
            "0                                           炒饭和牛肉饼还不错        1\n",
            "0             晚上去的牛排全熟盘子都是温的沙拉也不好吃感觉都是放久了的点了个面包汤真难喝差评        0\n",
            "0                      邓平小姐姐的服务很好小姐姐很有活力哈哈祝该店生意红火服务超赞        1\n",
            "0                                  服务员汪思佳态度不错店里环境好冷气足        1\n",
            "0                                             味道很好敖之放        1\n",
            "0                                               很好余梦月        1\n",
            "0                                   服务员余梦月服务态度非常好五星好评        1\n",
            "0                                牛排要七分给做全熟味道差服务员服务不周到        0\n",
            "0                                              邓平服务很好        1\n",
            "0                                    套餐里的牛柳一点都不好吃面是硬的        0\n",
            "0                                   味道不错经常来吃也会介绍朋友过来的        1\n",
            "0                                            口味环境服务等位        1\n",
            "0   印尼炒饭有一点点小辣但是越吃越来劲奶油意面特别好吃我很爱吃这些腻腻的食物奶油味道很浓吃完了想...        1\n",
            "0                 菜品一菜服务差地很二年前去吃味道好服务地很好现在完全不行了在也不会去了        0\n",
            "0                           美团购买优惠不错口味一如既往的可以上菜稍微慢了一点        1\n",
            "..                                                ...      ...\n",
            "0                                                  陈黎        1\n",
            "0                                          钟志服务很好下次再来        1\n",
            "0                                              吴梦蝶棒棒哒        1\n",
            "0                                             口味环境环境好        1\n",
            "0                                               味道很满意        1\n",
            "0                                     秀玉已经去了好多次了味道还不错        1\n",
            "0                                      王小凤服务态度很好分量足好吃        1\n",
            "0                      服务还不错东西不怎么好吃披萨真的好难吃牛柳刚吃还可以越吃越腻        0\n",
            "0   感觉份量比较少而且要美团付款的时候服务员是个女的那表情像吃了苍蝇一样恶心你们店铺自己做活动折...        0\n",
            "0                        水果沙拉还可以的雪球面包也可以服务态度一般般啦习惯性五星        1\n",
            "0                            服务员王小凤态度可以很热情菜品的话还行比较合胃口        1\n",
            "0                                 菜式很新颖看着就有胃口环境也好五分好评        1\n",
            "0                                      味道一般吧番茄牛肉饭饭量很少        0\n",
            "0                                       王小凤服务很好牛排味道不错        1\n",
            "0                                                  不错        1\n",
            "0                              儿子喜欢吃他家牛排只能妥协希望牛肉更新鲜一些        1\n",
            "0   服务态度不好明明点的牛柳意面换成米饭重点是换成米饭你给我加一个米饭几个意思呢你是需要我说三遍...        0\n",
            "0                                      牛排味道还可以环境优雅服务好        1\n",
            "0                                     味道不错真好吃环境和服务超级赞        1\n",
            "0                                      还不错好吃也不怎么辣下次还来        1\n",
            "0                                                环境很好        1\n",
            "0                       每次跟儿子二个人来吃都要点差不多的东西儿子喜欢吃这里的牛排        1\n",
            "0                                        坑死了没有牛排是汉堡肉沫        1\n",
            "0                                      没有以前做的好了很多都做假了        0\n",
            "0                                         不错味道好服务好分量足        1\n",
            "0                      就那样呗反正经常去买单的时候折时候服务员像是不愿意让人用样的        1\n",
            "0   三个人吃两份牛排都吃完了鸡排还没有上单子上的时间和上菜的时间隔了分钟加上之前等的时候也有半个...        0\n",
            "0                                        一直在他们家吃吃了几年了        1\n",
            "0                 和男票一起去的吃的只是一个安静的环境菜品味道一般般价格不高性价比还算高        1\n",
            "0                 的我在一期消费的点到你们这边的买单了还不给我退款说我消费了我消费的腿的        0\n",
            "\n",
            "[150 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My0So50B1MrU",
        "colab_type": "code",
        "outputId": "fa7ed0f6-d79c-43b9-9b60-3ed67e2d8eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#加在停用词库\n",
        "def get_custom_stopwords(stop_words_file):\n",
        "    with open(stop_words_file,encoding='gbk') as f:\n",
        "        stopwords = f.read()\n",
        "    stopwords_list = stopwords.split('\\n')\n",
        "    custom_stopwords_list = [i for i in stopwords_list]\n",
        "    return custom_stopwords_list\n",
        "stop_words_file = \"中文停用词库.txt\"\n",
        "stopwords = get_custom_stopwords(stop_words_file)\n",
        "print(stopwords)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['$', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', '_', '“', '”', '、', '。', '《', '》', '一', '一些', '一何', '一切', '一则', '一方面', '一旦', '一来', '一样', '一般', '一转眼', '万一', '上', '上下', '下', '不', '不仅', '不但', '不光', '不单', '不只', '不外乎', '不如', '不妨', '不尽', '不尽然', '不得', '不怕', '不惟', '不成', '不拘', '不料', '不是', '不比', '不然', '不特', '不独', '不管', '不至于', '不若', '不论', '不过', '不问', '与', '与其', '与其说', '与否', '与此同时', '且', '且不说', '且说', '两者', '个', '个别', '临', '为', '为了', '为什么', '为何', '为止', '为此', '为着', '乃', '乃至', '乃至于', '么', '之', '之一', '之所以', '之类', '乌乎', '乎', '乘', '也', '也好', '也罢', '了', '二来', '于', '于是', '于是乎', '云云', '云尔', '些', '亦', '人', '人们', '人家', '什么', '什么样', '今', '介于', '仍', '仍旧', '从', '从此', '从而', '他', '他人', '他们', '以', '以上', '以为', '以便', '以免', '以及', '以故', '以期', '以来', '以至', '以至于', '以致', '们', '任', '任何', '任凭', '似的', '但', '但凡', '但是', '何', '何以', '何况', '何处', '何时', '余外', '作为', '你', '你们', '使', '使得', '例如', '依', '依据', '依照', '便于', '俺', '俺们', '倘', '倘使', '倘或', '倘然', '倘若', '借', '假使', '假如', '假若', '傥然', '像', '儿', '先不先', '光是', '全体', '全部', '兮', '关于', '其', '其一', '其中', '其二', '其他', '其余', '其它', '其次', '具体地说', '具体说来', '兼之', '内', '再', '再其次', '再则', '再有', '再者', '再者说', '再说', '冒', '冲', '况且', '几', '几时', '凡', '凡是', '凭', '凭借', '出于', '出来', '分别', '则', '则甚', '别', '别人', '别处', '别是', '别的', '别管', '别说', '到', '前后', '前此', '前者', '加之', '加以', '即', '即令', '即使', '即便', '即如', '即或', '即若', '却', '去', '又', '又及', '及', '及其', '及至', '反之', '反而', '反过来', '反过来说', '受到', '另', '另一方面', '另外', '另悉', '只', '只当', '只怕', '只是', '只有', '只消', '只要', '只限', '叫', '叮咚', '可', '可以', '可是', '可见', '各', '各个', '各位', '各种', '各自', '同', '同时', '后', '后者', '向', '向使', '向着', '吓', '吗', '否则', '吧', '吧哒', '吱', '呀', '呃', '呕', '呗', '呜', '呜呼', '呢', '呵', '呵呵', '呸', '呼哧', '咋', '和', '咚', '咦', '咧', '咱', '咱们', '咳', '哇', '哈', '哈哈', '哉', '哎', '哎呀', '哎哟', '哗', '哟', '哦', '哩', '哪', '哪个', '哪些', '哪儿', '哪天', '哪年', '哪怕', '哪样', '哪边', '哪里', '哼', '哼唷', '唉', '唯有', '啊', '啐', '啥', '啦', '啪达', '啷当', '喂', '喏', '喔唷', '喽', '嗡', '嗡嗡', '嗬', '嗯', '嗳', '嘎', '嘎登', '嘘', '嘛', '嘻', '嘿', '嘿嘿', '因', '因为', '因了', '因此', '因着', '因而', '固然', '在', '在下', '在于', '地', '基于', '处在', '多', '多么', '多少', '大', '大家', '她', '她们', '好', '如', '如上', '如上所述', '如下', '如何', '如其', '如同', '如是', '如果', '如此', '如若', '始而', '孰料', '孰知', '宁', '宁可', '宁愿', '宁肯', '它', '它们', '对', '对于', '对待', '对方', '对比', '将', '小', '尔', '尔后', '尔尔', '尚且', '就', '就是', '就是了', '就是说', '就算', '就要', '尽', '尽管', '尽管如此', '岂但', '己', '已', '已矣', '巴', '巴巴', '并', '并且', '并非', '庶乎', '庶几', '开外', '开始', '归', '归齐', '当', '当地', '当然', '当着', '彼', '彼时', '彼此', '往', '待', '很', '得', '得了', '怎', '怎么', '怎么办', '怎么样', '怎奈', '怎样', '总之', '总的来看', '总的来说', '总的说来', '总而言之', '恰恰相反', '您', '惟其', '慢说', '我', '我们', '或', '或则', '或是', '或曰', '或者', '截至', '所', '所以', '所在', '所幸', '所有', '才', '才能', '打', '打从', '把', '抑或', '拿', '按', '按照', '换句话说', '换言之', '据', '据此', '接着', '故', '故此', '故而', '旁人', '无', '无宁', '无论', '既', '既往', '既是', '既然', '时候', '是', '是以', '是的', '曾', '替', '替代', '最', '有', '有些', '有关', '有及', '有时', '有的', '望', '朝', '朝着', '本', '本人', '本地', '本着', '本身', '来', '来着', '来自', '来说', '极了', '果然', '果真', '某', '某个', '某些', '某某', '根据', '欤', '正值', '正如', '正巧', '正是', '此', '此地', '此处', '此外', '此时', '此次', '此间', '毋宁', '每', '每当', '比', '比及', '比如', '比方', '没奈何', '沿', '沿着', '漫说', '焉', '然则', '然后', '然而', '照', '照着', '犹且', '犹自', '甚且', '甚么', '甚或', '甚而', '甚至', '甚至于', '用', '用来', '由', '由于', '由是', '由此', '由此可见', '的', '的确', '的话', '直到', '相对而言', '省得', '看', '眨眼', '着', '着呢', '矣', '矣乎', '矣哉', '离', '竟而', '第', '等', '等到', '等等', '简言之', '管', '类如', '紧接着', '纵', '纵令', '纵使', '纵然', '经', '经过', '结果', '给', '继之', '继后', '继而', '综上所述', '罢了', '者', '而', '而且', '而况', '而后', '而外', '而已', '而是', '而言', '能', '能否', '腾', '自', '自个儿', '自从', '自各儿', '自后', '自家', '自己', '自打', '自身', '至', '至于', '至今', '至若', '致', '般的', '若', '若夫', '若是', '若果 ', '若非', '莫不然', '莫如', '莫若', '虽', '虽则', '虽然', '虽说', '被', '要', '要不', '要不是', '要不然', '要么', '要是', '譬喻', '譬如', '让', '许多', '论', '设使', '设或', '设若', '诚如', '诚然', '该', '说来', '诸', '诸位', '诸如', '谁', '谁人', '谁料', '谁知', '贼死', '赖以', '赶', '起', '起见', '趁', '趁着', '越是', '距', '跟', '较', '较之', '边', '过', '还', '还是', '还有', '还要', '这', '这一来', '这个', '这么', '这么些', '这么样', '这么点儿', '这些', '这会儿', '这儿', '这就是说', '这时', '这样', '这次', '这般', '这边', '这里', '进而', '连', '连同', '逐步', '通过', '遵循', '遵照', '那', '那个', '那么', '那么些', '那么样', '那些', '那会儿', '那儿', '那时', '那样', '那般', '那边', '那里', '都', '鄙人', '鉴于', '针对', '阿', '除', '除了', '除外', '除开', '除此之外', '除非', '随', '随后', '随时', '随着', '难道说', '非但', '非徒', '非特', '非独', '靠', '顺', '顺着', '首先', '！', '，', '：', '；', '？', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_gFUDkN1MrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pkuseg\n",
        "import numpy as np\n",
        "Commentdata=np.array(Comment_datas.get('Comment'))\n",
        "X_data=pd.DataFrame()\n",
        "Y_data=np.array(Comment_datas.get('IsGreet'))\n",
        "seg = pkuseg.pkuseg()           # 以默认配置加载模型\n",
        "for items in Commentdata:\n",
        "    text = seg.cut(items)  # 进行分词  \n",
        "    X_data=X_data.append(pd.DataFrame({'cytedCom':text}))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgsOnIZY1MrY",
        "colab_type": "code",
        "outputId": "4a79bfe5-7445-4c7a-9359-7542a086dfd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer()\n",
        "term_matrix = pd.DataFrame(vect.fit_transform(X_train.cutted_comment).toarray(), columns=vect.get_feature_names())\n",
        "term_matrix.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2d7a522a2dd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mterm_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcutted_comment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mterm_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2qKkbCU1jtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_df = 0.8 # 在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
        "min_df = 3 # 在低于这一数量的文档中出现的关键词（过于独特），去除掉。\n",
        "vect = CountVectorizer(max_df = max_df,\n",
        "                       min_df = min_df,\n",
        "                       token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b',\n",
        "                       stop_words=frozenset(stopwords))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00yEWTV41wid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "term_matrix = pd.DataFrame(vect.fit_transform(X_train.cutted_comment).toarray(), columns=vect.get_feature_names())\n",
        "term_matrix.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNTBxbJZ1zJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.cross_validation import cross_val_score\n",
        "\n",
        "nb = MultinomialNB()\n",
        "pipe = make_pipeline(vect, nb)\n",
        "pipe.steps\n",
        "cross_val_score(pipe, X_train.cutted_comment, y_train, cv=5, scoring='accuracy').mean()\n",
        "pipe.fit(X_train.cutted_comment, y_train)\n",
        "pipe.predict(X_test.cutted_comment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8BpZsl42FpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "y_pred = pipe.predict(X_test.cutted_comment)\n",
        "metrics.accuracy_score(y_test, y_pred)\n",
        "metrics.confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYscLK-t2OMt",
        "colab_type": "text"
      },
      "source": [
        "神经网络模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMMlL14x1Mrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "import tensorflow as tf\n",
        "model = Sequential()\n",
        "# 搭建模型\n",
        "model.add(Dense(32, activation='relu', input_shape=(768,)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVDQaVKi1Mrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_data=X_data[1400:1485]\n",
        "Y_test_data=Y_data[1400:1485]\n",
        "X_train = []\n",
        "y_train = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "nums = np.arange(1400)\n",
        "# 随机打乱12000个训练数据\n",
        "np.random.shuffle(nums)\n",
        "for i in nums:\n",
        "    X_train.append(X_data[1])\n",
        "    y_train.append(Y_data[1])\n",
        "\n",
        "# 随机打乱1600个测试数据\n",
        "nums_ = np.arange(85)\n",
        "np.random.shuffle(nums_)  # shuffle改变的是自身的内容\n",
        "for i in nums_:\n",
        "    X_test.append(X_test_data [1])\n",
        "    y_test.append(Y_test_data [1])\n",
        "\n",
        "# list转Numpy数组\n",
        "X_train= np.array(X_train)\n",
        "X_test= np.array(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlnuf7e61Mrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=tf.train.AdamOptimizer(),\n",
        "    metrics=['acc']\n",
        ")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NsSUFlZ1Mrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "history_dict = history.history\n",
        "epochs = range(1, len(history_dict['acc']) + 1)\n",
        "# 绘图部分\n",
        "plt.figure()\n",
        "plt.plot(epochs, history_dict['acc'], 'b', label='acc')\n",
        "plt.plot(epochs, history_dict['val_acc'], 'bo', label='val_acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVCxRVIF1Mrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss = model.evaluate(\n",
        "    X_test,\n",
        "    y_test,\n",
        "    batch_size=63,\n",
        "    verbose=1\n",
        ")\n",
        "print(test_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUqZQGLg1Mrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ok6F4Hz1Mrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAE1GUYn1Mrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}